{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Débordement d'égouts\n",
    "## Rapport équipe 9 - A19\n",
    "#### Noboru Yoshida - Mehdi Chaid - Mathieu Giroux-Huppé - Maxime Gosselin\n",
    "#### 20 Décembre 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "1. [Objectif](#objectives)  \n",
    "    1. [Installation des librairies](#librairies) \n",
    "2. [Méthode choisie](#chosen_method)\n",
    "3. [Traitement des données](#data_processing)  \n",
    "    1. [Chargement des données des ouvrages](#data_processing_ouvrages) \n",
    "    2. [Chargement des données de surverse](#data_processing_surverse)  \n",
    "    3. [Chargement des données de précipitation](#data_processing_precipitation)\n",
    "    4. [Jointure des Dataframes](#data_processing_join)\n",
    "    5. [Standardization et OneHot des colonnes](#data_processing_std_oh)\n",
    "    6. [Création de l'ensemble de validation](#data_processing_validation)\n",
    "4. [Sélection de modèles](#model-selection)\n",
    "    1. [Régression logistique](#logistic-reg)\n",
    "    2. [Classification bayésienne naive](#naive-bayes)\n",
    "    3. [Forêt aléatoire](#random-forest)\n",
    "    4. [Séparateur à vaste marge](#svm)\n",
    "5. [Améliorations possibles](#possible-improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"objectives\"></a>\n",
    "\n",
    "## 1. Objectif\n",
    "\n",
    "Ce rapport vise à fournir une prédiction sur les surverses dans plusieurs ouvrages sur l'île de Montréal. Il en existe 170 réparties sur tout le bord de l'île. Dans ce travail, nous nous attarderons à la prédiction de seulement 5 d'entre-eux, soit les suivants :\n",
    "- 3260-01D dans Rivière-des-Prairies\n",
    "- 3350-07D dans Ahunstic\n",
    "- 4240-01D dans Pointe-aux-Trembles\n",
    "- 4350-01D dans le Vieux-Montréal\n",
    "- 4380-01D dans Verdun\n",
    "\n",
    "Nous avons à notre disposition 3 jeux de données qui nous aiderons à trouver une relation entre la quantité de pluie tombé et les surverses de certain ouvrages. Nous devrons alors entraîner un modèle sur les données de 2013 à 2018, puis prédire sur certaines dates de 2019. Il n'y pas de restrictions au niveau des techniques à utilisés pour le type de modèle à utiliser. Nous avons exploré quelques techniques différentes que nous allons détaillé plus en détail dans la [section 4](#model-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"librairies\"></a>\n",
    "\n",
    "### 1.1 Installation des librairies nécessaires\n",
    "\n",
    "Ici nous installerons toutes les librairies nécessaires pour la bonne exécution du reste du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\");\n",
    "Pkg.add(\"Random\");\n",
    "Pkg.add(\"DataStructures\");\n",
    "Pkg.add(\"BenchmarkTools\");\n",
    "Pkg.add(\"DataFrames\");\n",
    "Pkg.add(\"Statistics\");\n",
    "Pkg.add(\"Dates\");\n",
    "Pkg.add(\"Gadfly\");\n",
    "Pkg.add(\"MLBase\");\n",
    "Pkg.add(\"DecisionTree\");\n",
    "Pkg.add(\"IterTools\");\n",
    "\n",
    "using CSV, DataFrames,Distributions,DataStructures,BenchmarkTools,DecisionTree, Statistics, Dates, Gadfly, Random, MLBase, IterTools;\n",
    "include(\"utils/precipitation.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chosen_method\"></a>\n",
    "\n",
    "## 2. Méthode choisie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing\"></a>\n",
    "\n",
    "## 3. Traitement des données\n",
    "\n",
    "Cette section expliquera comment nous allons charger et nettoyer nos données afin de bâtir les DataFrames d'entraînement, de validation et de test. Chaque bloc de code sera commenté, et à l'occasion quelques tableaux et graphiques seront disponibles afin de visualiser les opérations effectuées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_ouvrages\"></a>\n",
    "### 3.1 Chargement des données des ouvrages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous chargeons les données provenant du fichier <i>ouvrages-surverses.csv</i>. Il y a plusieurs colonnes dans cette table, mais nous sommes intéréssé que par 4 de ces colonnes. Pour plus de détails sur ce jeu de donnée : http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse.\n",
    "\n",
    "Les colonnes retenues sont structurées comme ceci : <br>\n",
    "**ID_OUVRAGE** : (String) L'idendifiant unique de chacun des ouvrages <br>\n",
    "**TP_Z** : (Float64) La hauteur du trop plein (m) <br>\n",
    "**TP_LAT** : (Float64) La latitude du trop plein selon le système de coordonné WGS84 <br>\n",
    "**TP_LNG** : (Float64) La longitude du trop plein selon le système de coordonné WGS84 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrages = CSV.read(\"data/ouvrages-surverses.csv\");\n",
    "colnames = [\"N_Env\", \"ID_SOMA\", \"ID_OUVRAGE\", \"NOM\", \"SOMA_SEC\", \"REGION\", \"TP_X\", \"TP_Y\", \"TP_Z\", \"TP_LAT\", \"TP_LNG\", \"EMI_X\", \"EMI_Y\", \"EMI_LNG\", \"EMI_LAT\"];\n",
    "names!(ouvrages, Symbol.(colnames));\n",
    "select!(ouvrages, [:ID_OUVRAGE, :TP_LAT, :TP_LNG, :TP_Z]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputation des hauteurs de Trop-plein manquant par la moyenne des hauteurs des autres trop plein.\n",
    "\n",
    "Nous avons choisi de remplacé les données manquantes des hauteurs des trop-plein par la moyenne de ceux-ci puisque c'est une technique assez populaire dans les sciences des données afin de ne pas trop modifié les statisques de la colonne. Pour donner un contre exemple, nous aurions pu aussi remplacer les données manquantes par 0, c'est aussi une technique assez communes, mais dans le contexte d'une hauteur de trop plein, une hauteur de 0m n'a aucun sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrages.TP_Z = coalesce.(ouvrages.TP_Z, mean(ouvrages[completecases(ouvrages), :].TP_Z));\n",
    "first(shuffleDf(ouvrages), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualtion des données chargées\n",
    "\n",
    "Dans le premier graphique, on peut observer la répartion des hauteurs de trop-plein. On peut remarqué une silhouette d'une double loi normale centré en 13 et 25. Nous pouvons aussi voir nos valeurs de trop-plein manquant que nous avons imputé juste avant (la grande barre en 19). Nous pouvons donc valider que notre choix d'imputation est sensé.\n",
    "\n",
    "Dans le second graphique, on peut observer la position de chacun des trop-pleins tout le tour de l'île de Montréal. Remarquablement, puisque les trop-pleins sont en assez grand nombre et assez bien répartie, nous pouvons très bien distinguer la silhouette de l'île de Montréal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrages, x=:TP_Z, Geom.histogram(bincount=50), Guide.xlabel(\"Height of TropPlein\"),Guide.ylabel(\"Frequency\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrages,x=:TP_LNG, y=:TP_LAT, Geom.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_surverse\"></a>\n",
    "### 3.2 Chargement des données de surverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous chargeons les données du fichier <i>Surverses.csv</i>. Nous utilisons toutes les colonnes de ce taleau. Ce jeu de donnée provient des données ouvertes de Montréal : http://donnees.ville.montreal.qc.ca/dataset/debordement. Le fichier ne corespond pas tout a fait aux données en ligne. Il y a eu des transformations (jointure) approtées par notre professeur. Le fichier est structuré comme ceci : <br>\n",
    "\n",
    "\n",
    "**NO_Ouvrage** : (String) L'idendifiant unique de chacun des ouvrages <br>\n",
    "**Date** : (Date) La date sous le format yyyy-mm-jj de l'observation <br>\n",
    "**Surverse** : (Bool) Si l'ouvrage a surversé <br>\n",
    "**Raison** : (String) Abréviation de la raison de la surverse. Les options sont variables, Ex :<br>\n",
    "- \"U\" : Urgence;\n",
    "- \"TS\" : Déversement par temps sec;\n",
    "- \"P\" : Débordement du à la pluie; \n",
    "- \"N\" : Rejet à la rivière des Prairies;\n",
    "- \"S\" : Rejet au fleuve St-Laurent;\n",
    "\n",
    "Au final, nous gardons que les trois premières colonnes. La colonne NO_Ouvrage sera renommé afin de servir de clé externe pour se joindre à la table des ouvrages créés plus haut. De plus, nous allons gardé que les raisons dû à la pluie, débordement par temps sec et ceux inconnus que nous prendrons pour aquis que ce sont des surverses dû à la pluie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du fichier surverses.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = CSV.read(\"data/surverses.csv\", missingstring=\"-99999\");\n",
    "first(shuffleDf(surverses),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garder les mois qui nous concerne. (mai à octobre inclusivement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = filter(row -> month(row.DATE) > 4, surverses);\n",
    "surverses = filter(row -> month(row.DATE) < 11, surverses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garder les raisons de surverse qui nous intéresse. (Pluie, Inconnue, TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raison = coalesce.(surverses[:,:RAISON],\"Inconnue\");\n",
    "surverses[!,:RAISON] = raison;\n",
    "\n",
    "surverses = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], surverses);\n",
    "select!(surverses, [:NO_OUVRAGE, :DATE, :SURVERSE]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retirer les données manquantes ainsi que renommer la colonne NO_OUVRAGE en ID_OUVRAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = dropmissing(surverses, disallowmissing=true);\n",
    "rename!(surverses, :NO_OUVRAGE => :ID_OUVRAGE);\n",
    "first(shuffleDf(surverses), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_precipitation\"></a>\n",
    "### 3.3 Chargement des données de précipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous chargeons les données du fichier <i> Precipitation.csv </i>. Nous devons effectuer des fonctions d'aggrégation sur ce tableau avant de pouvoir l'intégré dans notre jeu de donnée de nos modèles. En effet, ce tableau contient les données de précipitation par heure pour chacune des cinqs stations météorologiques près de montréal. Cependant, notre jeu de données des features est seulement par jour. Nous allons donc applique 3 fonctions d'aggrégation différente, soit : Somme, Maximum et Maximum3h. \n",
    "\n",
    "Ce jeu de données a été préparé par notre professeur à partir des données météos d'environnement Canada :\n",
    "https://climat.meteo.gc.ca/climate_data/hourly_data_f.html?hlyRange=2008-01-08%7C2019-11-12&dlyRange=2002-12-23%7C2019-11-12&mlyRange=%7C&StationID=30165&Prov=QC&urlExtension=_f.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2019&selRowPerPage=25&Line=17&searchMethod=contains&Month=11&Day=12&txtStationName=montreal&timeframe=1&Year=2019.\n",
    "\n",
    "Ce jeu de données contient la date sous le format yyyy-mm-jj, l'heure, ainsi que la quantité de pluie au dixième de millimètre pour chacune des cinqs stations météo suivante :\n",
    "- McTavish (7024745)\n",
    "- Ste-Anne-de-Bellevue (702FHL8)\n",
    "- Montreal/Pierre Elliott Trudeau Intl (702S006)\n",
    "- Montreal/St-Hubert (7027329)\n",
    "- L’Assomption (7014160)\n",
    "\n",
    "Les dates contenues dans ce dataset comprennent les années 2013 à 2019. Par contre, nous devons entraîner notre modèle sur les années avant 2019, et prédire sur toute l'année 2019 (de mai à octobre). Il est donc important de noter que cette table contient des données de l'ensemble d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\");\n",
    "rename!(precipitations, Symbol(\"St-Hubert\")=>:StHubert);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garder les mois qui nous concerne. (mai à octobre inclusivement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations = filter(row -> month(row.date) > 4, precipitations);\n",
    "precipitations = filter(row -> month(row.date) < 11, precipitations);\n",
    "first(shuffleDf(precipitations),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation des données de précipitation manquante par la moyenne de la journée de la station concernée\n",
    "\n",
    "Nous avons commencé par remplacer les valeurs manquantes des précipitations par 0. 0 ici n'était pas farfelu. Le problème est que la grande majorité des colonnes ont des valeurs de 0. De plus, il arrivait qu'il y avait des valeurs manquantes entre deux valeurs de précipitation non-nulle. Il nous semblait alors plus logique d'y mettre la valeur moyenne journalière que de tout simplement mettre 0. Si par contre il adevenais qu'une journée entière soit manquante, alors 0 sera mis comme valeur manquante sur la journée entière, puisque c'est notre cas de figure le plus fréquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_by_day = by(precipitations, :date,  \n",
    "                            McTavish = :McTavish=>mean_wo_missing, \n",
    "                            Bellevue = :Bellevue=>mean_wo_missing, \n",
    "                            Assomption = :Assomption=>mean_wo_missing,\n",
    "                            Trudeau = :Trudeau=>mean_wo_missing,\n",
    "                            StHubert = :StHubert=>mean_wo_missing)\n",
    "\n",
    "for i=1:size(precipitations,1)\n",
    "    if isequal(precipitations[i, :McTavish], missing)\n",
    "        precipitations[i,:McTavish] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:McTavish][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Bellevue], missing)\n",
    "        precipitations[i,:Bellevue] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Bellevue][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Assomption], missing)\n",
    "        precipitations[i,:Assomption] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Assomption][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Trudeau], missing)\n",
    "        precipitations[i,:Trudeau] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Trudeau][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :StHubert], missing)\n",
    "        precipitations[i,:StHubert] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:StHubert][1]\n",
    "    end\n",
    "end\n",
    "\n",
    "first(shuffleDf(precipitations), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Aggrégation des précipitations\n",
    "\n",
    "Cette sous-section aggrégera les données de précipitation selon 3 fonctions différentes : Somme, Maximum et maximum sur 3 heures consécutives. Une visualiation des transformations accompagnera chacune d'entre-elle. Chaque transformation créera un tableau qui sera reprit dans la [section 3.4](#data_processing_join). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1.1 Somme des précipitations par jour par station météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_sum = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>sum, \n",
    "            Bellevue = :Bellevue=>sum,\n",
    "            Assomption = :Assomption=>sum, \n",
    "            Trudeau = :Trudeau=>sum, \n",
    "            StHubert = :StHubert=>sum);\n",
    "first(shuffleDf(pcp_sum), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation des précipitations par station météo\n",
    "\n",
    "Il y a beaucoup d'élément sur ce graphique. Une particularité de Gadfly est qu'il nous permet de choisir les séries à afficher. On peut cliquer sur les couleurs disirées en légende et réduire l'information affiché ! On peut remarquer sur ce graphique que les stations ont un comportement similaire, mais pas identique. Si nous prennons McTavish et Assomption pour l'année 2016, nous pouvons voir que certain pic de précipitations ne sont pas idendique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = pcp_sum\n",
    "df_for_plot = filter(row -> year(row.date) == 2016, pcp_sum);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Somme des précipitations des stations météo pour 2016\"), Guide.xlabel(\"Date\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1.2 Maximum des précipitations par jour par station météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>maximum,\n",
    "            Bellevue = :Bellevue=>maximum, \n",
    "            Assomption = :Assomption=>maximum,\n",
    "            Trudeau = :Trudeau=>maximum,\n",
    "            StHubert = :StHubert=>maximum)\n",
    "first(shuffleDf(pcp_max),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation des précipitations par station météo\n",
    "\n",
    "On peut voir sur ce graphique qu'il nous apporte une information différente sur les précipitations. Puisque la fonction d'aggrégation est différente, l'allure générale du graphique l'est aussi. On peut remarqué qu'il y a des pics plus prononcé à la station Assomption, tandis qu'à celle de StHubert, celà semble moins commun de tomber énormement de pluie en 1 heure en 2014. De plus, nous pouvons voir que nous n'avons aucune donnée pour mai 2014 à la station de StHubert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = pcp_max\n",
    "df_for_plot = filter(row -> year(row.date) == 2014, pcp_max);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Maximum des précipitations des stations météo pour 2014\"), Guide.xlabel(\"Date\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1.3 Maximum de précipitation durant 3 heures consécutives par heure par station météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max3h = by(precipitations, :date,\n",
    "                McTavish = :McTavish=>maximum3,\n",
    "                Bellevue = :Bellevue=>maximum3,\n",
    "                Assomption = :Assomption=>maximum3,\n",
    "                Trudeau = :Trudeau=>maximum3,\n",
    "                StHubert = :StHubert=>maximum3)\n",
    "first(shuffleDf(pcp_max3h),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation des précipitations par station météo\n",
    "\n",
    "Il n'y a rien de plus à remarquer que sur les deux autres graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = pcp_max3h\n",
    "df_for_plot = filter(row -> year(row.date) == 2018, pcp_max3h);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Maximum3h des précipitations des stations météo pour 2018\"), Guide.xlabel(\"Date\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Visualisation des précipitation aggégrés d'une station pour une année\n",
    "\n",
    "Sur ce graphique, on peut remarquer la corrélation entre les différentes fonctions d'aggrégations. C'était prévisible puisque ce sont des fonctions qui proviennent des mêmes plages de données. De plus, on peut bien voir l'amplitude différentes de chacun de ces features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_pt = 2018;\n",
    "loc = :McTavish\n",
    "mct_sum = pcp_sum[:,[1,2]]\n",
    "rename!(mct_sum, loc => :Sum);\n",
    "mct_sum = filter(row -> year(row.date) == date_to_pt, mct_sum);\n",
    "\n",
    "mct_max = pcp_max[:,[1,2]]\n",
    "rename!(mct_max,loc => :Max);\n",
    "mct_max = filter(row -> year(row.date) == date_to_pt, mct_max);\n",
    "\n",
    "mct_max3h = pcp_max3h[:,[1,2]]\n",
    "rename!(mct_max3h,loc => :Max3h);\n",
    "mct_max3h = filter(row -> year(row.date) == date_to_pt, mct_max3h);\n",
    "\n",
    "df_for_plot = join(mct_sum, mct_max3h, on = :date);\n",
    "df_for_plot = join(df_for_plot, mct_max, on = :date);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "\n",
    "\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_join\"></a>\n",
    "### 3.4 Jointure des Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement de l'ensemble de test et jointure de ouvrages et surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = CSV.read(\"data/test.csv\"); #NO_OUVRAGE, DATE\n",
    "rename!(X_test, :NO_OUVRAGE => :ID_OUVRAGE);\n",
    "\n",
    "X_test = join(ouvrages, X_test, on =:ID_OUVRAGE);\n",
    "features = join(ouvrages, surverses, on =:ID_OUVRAGE);\n",
    "\n",
    "srv = filter(row -> row.SURVERSE == 1, features);\n",
    "first(shuffleDf(srv), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des stations en surverse et en non-surverse à une date donnée\n",
    "\n",
    "Ce graphique est très intéressant, puisqu'il illustre l'état de tous les ouvrages à une date donnée. Il est très intéressant de voir quels ouvrages ont été en surverse et ceux qu'ils ne l'ont pas été. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_geo_plot = filter(row -> row.DATE == Date(2018,7,25), features)\n",
    "df_for_geo_plot = filter(row -> row.DATE == Date(2015, 5, 30), features)\n",
    "# df_for_geo_plot = filter(row -> row.DATE == Date(2017, 10, 9), features)\n",
    "df_for_geo_plot[:SURVERSE] = convert(Array{Bool,1}, df_for_geo_plot[:SURVERSE])\n",
    "plot(df_for_geo_plot, x=:TP_LNG, y=:TP_LAT, Geom.point, color=:SURVERSE, Guide.title(\"2018-07-25, état des surverses\"))\n",
    "#first(df_for_geo_plot,5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation afin d'assigner les données de précipitations aux ouvrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = DataFrame(STATION = String[], LAT = Float64[], LNG = Float64[]);\n",
    "\n",
    "push!(station_df, [\"McTavish\",   45.504742, -73.579167]);\n",
    "push!(station_df, [\"Bellevue\",   45.427222, -73.929167]);\n",
    "push!(station_df, [\"Assomption\", 45.809444, -73.434722]);\n",
    "push!(station_df, [\"Trudeau\",    45.467778, -73.741667]);\n",
    "push!(station_df, [\"StHubert\",   45.517500, -73.416944]);\n",
    "\n",
    "station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout de colonnes vides dans les jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function addColsForPrecipationPerDay(df)\n",
    "    df[!, :PCP_SUM] = zeros(size(df, 1));\n",
    "    df[!, :PCP_MAX] = zeros(size(df, 1));\n",
    "    df[!, :PCP_MAX3] = zeros(size(df, 1));\n",
    "    df[!, :METEO] = fill(\"\", size(df, 1));\n",
    "    return df\n",
    "end\n",
    "\n",
    "X_test = addColsForPrecipationPerDay(X_test)\n",
    "permutecols!(X_test, [:ID_OUVRAGE, :TP_LAT, :TP_LNG, :TP_Z, :DATE, :METEO, :PCP_SUM, :PCP_MAX, :PCP_MAX3]);\n",
    "    \n",
    "features = addColsForPrecipationPerDay(features)\n",
    "permutecols!(features, [:ID_OUVRAGE, :TP_LAT, :TP_LNG, :TP_Z, :DATE, :METEO, :PCP_SUM, :PCP_MAX, :PCP_MAX3, :SURVERSE]);\n",
    "\n",
    "first(shuffleDf(features), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jointure des précipitations aggrégés aux jeux de données basé sur la station météo la plus près de l'ouvrage concerné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fillPrecipitationWithClosestMeteoStation(df)\n",
    "    for i=1:size(df, 1)\n",
    "        id_ouvrage = df[i, 1]; \n",
    "        closest_station = \"McTavish\"; # initial value\n",
    "        shortest_dist = -1;\n",
    "\n",
    "        # Find closest station\n",
    "        for j=1:size(station_df, 1)\n",
    "            dist = findDistance(df[i, :TP_LAT], df[i, :TP_LNG], station_df[j, :LAT], station_df[j, :LNG]);\n",
    "\n",
    "            if shortest_dist == -1 || dist < shortest_dist\n",
    "                shortest_dist = dist;\n",
    "                closest_station = station_df[j, :STATION];\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Augment comb with a weighted p_sum, based on the distance to the station\n",
    "        p_sum = pcp_sum[∈([df[i, :DATE]]).(pcp_sum.date), Symbol(closest_station)];\n",
    "    #     comb[i, :PCP_SUM] = p_sum[1] * (1 - shortest_dist);\n",
    "        df[i, :PCP_SUM] = p_sum[1]; \n",
    "\n",
    "        # Augment comb with a weighted p_max, based on the distance to the station\n",
    "        p_max = pcp_max[∈([df[i, :DATE]]).(pcp_max.date), Symbol(closest_station)]\n",
    "    #     comb[i, :PCP_MAX] = p_max[1] * (1 - shortest_dist);\n",
    "        df[i, :PCP_MAX] = p_max[1];\n",
    "\n",
    "        # Augment comb with a weighted p_max3h, based on the distance to the station\n",
    "        p_max3 = pcp_max3h[∈([df[i, :DATE]]).(pcp_max3h.date), Symbol(closest_station)]\n",
    "    #     comb[i, :PCP_MAX3] = p_max3[1] * (1 - shortest_dist);\n",
    "        df[i, :PCP_MAX3] = p_max3[1]; \n",
    "\n",
    "        df[i, :METEO] = closest_station\n",
    "    end\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = fillPrecipitationWithClosestMeteoStation(X_test)\n",
    "features = fillPrecipitationWithClosestMeteoStation(features)\n",
    "first(shuffleDf(features), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrait des données extrêmes de précipitation. Purement subjectif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test[:PCP_SUM] .> 750, :PCP_SUM] = 750;\n",
    "X_test[X_test[:PCP_MAX] .> 500, :PCP_MAX] = 500;\n",
    "X_test[X_test[:PCP_MAX3] .> 750, :PCP_MAX3] = 750;\n",
    "\n",
    "features[features[:PCP_SUM] .> 750, :PCP_SUM] = 750;\n",
    "features[features[:PCP_MAX] .> 500, :PCP_MAX] = 500;\n",
    "features[features[:PCP_MAX3] .> 750, :PCP_MAX3] = 750;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des stations et leur station météo associée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_geo_plot = filter(row -> row.DATE == Date(2018,7,25), features)\n",
    "df_for_geo_plot[:SURVERSE] = convert(Array{Bool,1}, df_for_geo_plot[:SURVERSE])\n",
    "plot(df_for_geo_plot, x=:TP_LNG, y=:TP_LAT, Geom.point, color=:METEO, Guide.title(\"2018-07-25, Regroupement par station météo\"))\n",
    "#first(df_for_geo_plot,5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ouvrage 3350-07D pour 2018, quand il surverse, basé sur le critère max3h de pluie tombé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3260-01D dans Rivière-des-Prairies\n",
    "# 3350-07D dans Ahunstic\n",
    "# 4240-01D dans Pointe-aux-Trembles\n",
    "# 4350-01D dans le Vieux-Montréal\n",
    "# 4380-01D dans Verdun\n",
    "\n",
    "id_ouvrage_to_show = \"3350-07D\"\n",
    "df_temp = filter(row -> row.ID_OUVRAGE ∈ [id_ouvrage_to_show], features)\n",
    "df_temp = filter(row -> year(row.DATE) == 2018, df_temp);\n",
    "df_temp = df_temp[!,[:ID_OUVRAGE, :DATE, :PCP_MAX3, :SURVERSE]]\n",
    "df_temp[:SURVERSE] = convert(Array{Bool,1}, df_temp[:SURVERSE])\n",
    "\n",
    "\n",
    "\n",
    "plot(df_temp, x=:DATE, y=:PCP_MAX3, Geom.point, color=:SURVERSE,Guide.title(id_ouvrage_to_show))\n",
    "#first(shuffleDf(df_temp), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout de colonnes Mois (MONTH) et Jour (DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function splitDateByMonthAndDay(df)\n",
    "    df[!,:MONTH] = month.(df.DATE);\n",
    "    df[!,:DAY] = day.(df.DATE);\n",
    "    return df\n",
    "end\n",
    "\n",
    "X_test = splitDateByMonthAndDay(X_test)\n",
    "features = splitDateByMonthAndDay(features)\n",
    "first(shuffleDf(features[!, [:DATE, :MONTH, :DAY]]), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Déclaration de fonctions utilitaires\n",
    "\n",
    "C'est fonction seront utiles dans les section [3.5](#data_processing_std_oh) et [3.6](#data_processing_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function partitionTrainTest(data, at = 0.8) # https://discourse.julialang.org/t/simple-tool-for-train-test-split/473/2\n",
    "    n = nrow(data)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    return data[train_idx,:], data[test_idx,:]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function standarizeTrainTestCol(X_train, X_test, col)\n",
    "    mean_train_col = mean(X_train[!, col]);\n",
    "    std_train_col = std(X_train[!, col]);\n",
    "    X_train[!, col] = (X_train[!, col] .- mean_train_col) ./ std_train_col;\n",
    "    X_test[!, col] = (X_test[!, col] .- mean_train_col) ./ std_train_col;\n",
    "    \n",
    "    return X_train, X_test\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function oneHotMeteoStation(df, col)\n",
    "    df[!, :S_1] = zeros(size(df, 1));\n",
    "    df[!, :S_2] = zeros(size(df, 1));\n",
    "    df[!, :S_3] = zeros(size(df, 1));\n",
    "    df[!, :S_4] = zeros(size(df, 1));\n",
    "    df[!, :S_5] = zeros(size(df, 1));\n",
    "    \n",
    "    categories = [\"Assomption\", \"McTavish\", \"Trudeau\", \"Bellevue\", \"StHubert\"]\n",
    "    one_hot_cols = [:S_1, :S_2, :S_3, :S_4, :S_5]\n",
    "    \n",
    "    for i=1:size(df,1)\n",
    "        for j=1:size(categories,1)\n",
    "            if df[i, col] ∈ [ categories[j] ]\n",
    "                df[i, one_hot_cols[j]] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_std_oh\"></a>\n",
    "### 3.5 Standardization et OneHot des colonnes\n",
    "\n",
    "Nous devons effectuer ces opérations puisqu'en règle générale, les algorithmes de machine learing fonctionne mieux avec des données entre 0 et 1. De plus, il est primordiale pour la colonne :METEO d'être encodé sous ce format comme pour les colonnes sous format float, elle serait incompréhensible.\n",
    "\n",
    "**Standardisation** : $ \\bf{X}_{ij} = \\frac{\\bf{X}_{ij}-\\bar{x}_i}{\\sigma^2_{X_i}}$, où i est l'index de la colonne désirée et j l'index de la ligne désirée. Il est a noté que les paramètres $\\bar{X_i}$ et $\\sigma^2_{X_i}$ sont calculés et utilisés sur l'ensemble d'entraînement, mais seulement utilisé sur l'ensemble de test. C'est-à-dire que les données de test ne sont pas pris en compte dans les calcules de ces paramètres. Cette opération recentre la distribution en 0.\n",
    "\n",
    "**OneHot** : Le OneHot est le principe de transformer une variable catégorielle en plusieurs colonnes binaires. Ex : \n",
    "\n",
    "|A|>|A_1|A_2|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|\"a\"|>|1|0|\n",
    "|\"b\"|>|0|1|\n",
    "|\"a\"|>|1|0|\n",
    "\n",
    "Il y a deux écoles de pensé dans cette technique. La première est de représenter la première catégorie avec que des 0. Dans l'exemple ci-haut, \"a\" aurait la valeur A_1=0 et A_2=0. Si nous avons $n$ catégories, nous avons besoin alors de $n-1$ colonnes pour représenter toutes ces catégories. La deuxième école de pensé est la suivante : 1 colonne par catégorie. Si nous avons $n$ catégories, nous avons besoin alors de $n$ colonnes. Nous pouvons donc réserver le 0,...,0 pour des cas inconnus du OneHot. Nous avons opté pour la deuxième école de penser, mais si nous n'exploitons pas l'avantage de celle-ci sur le premier cas énnoncé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features\n",
    "\n",
    "X_train = oneHotMeteoStation(X_train, :METEO)\n",
    "X_test = oneHotMeteoStation(X_test, :METEO)\n",
    "\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :TP_LAT)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :TP_LNG)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :TP_Z)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :PCP_SUM)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :PCP_MAX)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :PCP_MAX3)\n",
    "X_train, X_test = standarizeTrainTestCol(X_train, X_test, :MONTH)\n",
    "X_train_all, X_test = standarizeTrainTestCol(X_train, X_test, :DAY)\n",
    "\n",
    "first(shuffleDf(X_test), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_processing_validation\"></a>\n",
    "### 3.6 Création de l'ensemble de validation\n",
    "\n",
    "Une technique extrêmement commune dans le domaine des sciences des données est de séparer l'ensemble d'entraînement afin de se créer un ensemble de validation. L'intérêt de faire ceci est que l'on sait les réponses sur l'ensemble de validation. Nous pouvons donc être en mesure de faire un évalutaion sommaire d'un modèle sur cette partie du dataset. Il est donc cruciale que cette partie ne soit pas pris en compte dans l'entraînement. Il n'est pas recommandé de valider et de s'entraîner sur les mêmes données. On pourrait courrir le risque d'avoir un phénomène de surapprentissage et biaiser notre évaluation sommaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = partitionTrainTest(X_train_all, 0.8)\n",
    "\n",
    "first(shuffleDf(X_train), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first(shuffleDf(X_val), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first(shuffleDf(X_val[!,[:METEO,:S_1,:S_2,:S_3,:S_4,:S_5]]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section nous avons créé nos trois dataframes important à la construction de nos différents modèles. Nous avons construit ces dataframes grâce à nos trois sources de données sur les ouvrages, les surverses ainsi que les précipitaions par jour. De plus, nous avons appliqué des transformations sur certaines colonnes afin de créer d'autres features pouvant être utilisé par nos modèles par la suite. Il est vrai que nous avons fait une certaine analyse exploratoire au fur et à mesure de cette section, mais il aurait été intéressant d'en faire une plus en profondeur. Quelles ouvrages / regions sont les plus propices à la surverse ? Est-ce qu'il y a un lien avec la hauteur du trop-plein ? Est-ce que avoir une animation de chaque jour et l'état des surverses pourrait nous donner de l'information additionnel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_analysis\"></a>\n",
    "\n",
    "## 4. Analyse exploratoire\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"naive-bayes\"></a>\n",
    "### 4.2 Régression bayésienne naïve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Définition des fonctions d'aide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de trouver la meilleure distribution qui s'ajuste le mieux pour un ensemble de données. Pour faire cela, nous testons plusieurs distributions différentes sur notre ensemble de données. Pour testé chacune de ces distributions, nous faisons appel à la librairie de Julia \"fit\" qui permet d'ajuster une distribution sur un ensemble de données. Par la suite, nous calculons la log vraisemblance de cette distribution. Nous ne conservons seulement que la distribution ayant la logvraisemblance la plus élevée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= fonction qui trouve la distribution qui représente le mieux un ensemble de donnée=#\n",
    "\n",
    "function fitBestLikelihoodDistribution(data::Array, verbose::Bool)\n",
    "    # Définition des distributions à essayer\n",
    "    distributions = [Beta, Binomial, \n",
    "              Categorical, DiscreteUniform, Exponential, \n",
    "              Normal, Gamma, Geometric, Laplace, Pareto, \n",
    "              Poisson, Uniform, Multinomial, MvNormal, Dirichlet, Weibull];\n",
    "    \n",
    "    distributionNames = [\"Beta\", \"Binomial\", \n",
    "                  \"Categorical\", \"DiscreteUniform\", \"Exponential\", \n",
    "                  \"Normal\", \"Gamma\", \"Geometric\", \"Laplace\", \"Pareto\", \n",
    "                  \"Poisson\", \"Uniform\", \"Multinomial\", \"MvNormal\", \"Dirichlet\", \"Weibull\"];\n",
    "    \n",
    "    # Déclaration des variables\n",
    "    maxLikelihood = -Inf;\n",
    "    distributionName = nothing;\n",
    "    finalFitDistribution = nothing;\n",
    "    fitDistribution = nothing;\n",
    "    \n",
    "    for i = 1:length(distributions) # Pour chaque type de distribution\n",
    "        if (verbose)\n",
    "            println(\"Trying model of type: \", distributionNames[i]);\n",
    "        end\n",
    "        try # On essaie de faire fit la distribution sur les données\n",
    "            fitDistribution = fit(distributions[i], data);\n",
    "        catch\n",
    "            if (verbose)\n",
    "                println(\"Invalid\");\n",
    "            end\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        newLikelihood = loglikelihood(fitDistribution, data)\n",
    "        \n",
    "        # Si on trouve une meilleure logvraisemblance que celle qu'on a déjà, \n",
    "        # la distribution courante est le meilleure\n",
    "        if (newLikelihood > maxLikelihood) \n",
    "            maxLikelihood = newLikelihood;\n",
    "            distributionName = distributionNames[i];\n",
    "            finalFitDistribution = fitDistribution;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println(\"The best distribution is of type \", distributionName, \" with a likelihood of \", maxLikelihood)\n",
    "    return finalFitDistribution;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de retrouver les meilleures distributions pour chacunes de nos variables explicatives sélectionnées. Pour chacune d'entre elles, nous devons trouver la meilleure distribution sachant la classe final. Dans notre cas, nous trouvons donc, pour chaque variable explicative, la meilleure distribution pour les exemplaires où il n'y a pas de surverse et une pour les exemplaires où il y a surverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= Cette fonction retourne un tableau des meilleurs distributions selon les variables explicatives données \n",
    "(une distribution lorsqu'il n'y a pas de surverse et \n",
    "une distribution lorsqu'il y a surverse pour chaque variabe explicative)=#\n",
    "\n",
    "function getBestLikelihoodDistributions(train::DataFrame, variable::Symbol)\n",
    "    x_m = [];\n",
    "    for i=0:1\n",
    "        ind = train[:,:SURVERSE] .== i;\n",
    "        x=train[ind,variable];\n",
    "        push!(x_m,fitBestLikelihoodDistribution(x, false));\n",
    "    end\n",
    "    \n",
    "    return x_m;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de récuperer une loi a priori en se basant sur un ensemble de données. Puisque notre prédiction est soit surverse ou non, notre loi a priori est donc une loi de Bernoulli où nous pouvons trouver la probabilité de succès en selon l'ensemble de données passé en paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=fonction permettant de récupérer un a priori en se basant sur la distribution d'un ensemble de données. =#\n",
    "function getPrioris(trainSet::DataFrame)\n",
    "    dAlpha = trainSet[:,:SURVERSE];\n",
    "    n_mode = Float64[];\n",
    "    for i=0:1\n",
    "        push!(n_mode, count(dAlpha .== i));\n",
    "    end\n",
    "    α = n_mode/size(trainSet,1);\n",
    "    \n",
    "    return α;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Prédiction bayesien naïf\n",
    "\n",
    "Ici nous définissons la fonction permettant d'effectuer un prédiction sur les données fournies en paramètres à l'aide des distributions et de la loi a priori aussi fournies en paramètres. Si nous étions capable de trouver une loi a posteriori de façon analytique, nous effecturions la prédiction de la façon suivante:\n",
    "a posteriori = vraisemblance * a priori, ce qui donnerait la formule suivante:\n",
    "\n",
    "$ f_{\\theta | Y}(\\theta) = f_{Y | \\theta}(y) \\dot f_\\theta(\\theta) $\n",
    "\n",
    "Toutefois, puisque nous utilisons une approche numérique, la formule ressemble plutôt à celà: \n",
    "\n",
    "$ p(C|X) = p(C) \\prod_{i=1}^n p(X_i|C)$\n",
    "\n",
    "Les distributions sachant la classe sont données en paramètres et peuvent donc être utilisé ici directement. La loi a priori est aussi fournies en paramètre. Toutefois, si nous voulions utiliser une loi a priori non informative, il sufirait de remplacer la probabilité par 1 comme la ligne commentée ici le suggère. Cela nous donnerait donc une loi non informative Uniforme(0,1). Finalement, pour déterminer la classe de chaque exemplaire, nous prenons la classe ayant obtenu la plus grande probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= fonction permettant de faire une prédiction sur les données à \n",
    "l'aide des distributions et de la loi a priori fourni. =#\n",
    "function predictNaiveBayes(data::DataFrame, likelihoodDistrs::Array, prioris::Array, variables::Array)\n",
    "    n_data = size(data,1);\n",
    "    nb_exp_var = size(variables,1);\n",
    "    y_m = Array{Int64}(undef,n_data);\n",
    "\n",
    "    for i=1:n_data\n",
    "        p = [];\n",
    "        for j=1:2\n",
    "#             prob =1; # à priori non informatif (Uniform(0,1))\n",
    "            prob = prioris[j]\n",
    "            for k=1:nb_exp_var\n",
    "                prob *= pdf.(likelihoodDistrs[k][j], data[i,variables[k]]);\n",
    "            end\n",
    "            push!(p,prob);\n",
    "        end\n",
    "        _, ind = findmax(p);\n",
    "        y_m[i] =ind -1;\n",
    "    end\n",
    "    \n",
    "    return y_m;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Sélection du meilleur modèle\n",
    "\n",
    "Dans cette section, nous explorerons deux approches différentes pour sélectionner le meilleur modèle de classification bayésienne naïve, soient une approche basée sur le BIC, ainsi qu'une approche basée sur le F1-score obtenu sur un ensemble de validation.\n",
    "\n",
    "Sélectionner le meilleur modèle de classification bayésienne naïve revient à trouver les variables explicatives devant être incluses et excluses pour que le modèle soit le plus précis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.1 Sélection du meilleur modèle à l'aide du BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function BIC(distributions::Dict, data::DataFrame)\n",
    "    n = size(data)[1]\n",
    "    k = length(keys(distributions))\n",
    "    totalLogLikelihood = 0\n",
    "    for variable in keys(distributions)\n",
    "        for j=0:1\n",
    "            ind = data[:,:SURVERSE] .== j;\n",
    "            x=data[ind,variable];\n",
    "            totalLogLikelihood += loglikelihood(distributions[variable][j+1], x)\n",
    "        end\n",
    "    end\n",
    "    return (totalLogLikelihood - k*log(n)/2);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function findBestVariablesCombinationBIC(train::DataFrame, likelihoodDistrs::Dict, variables::Array{Symbol})\n",
    "    bics = []\n",
    "    bicsDict = Dict()\n",
    "    combinations = []\n",
    "    for combination in subsets(variables)\n",
    "        push!(combinations, combination)\n",
    "        modelVariables = []\n",
    "        modelLikelihoodDistrs = Dict()\n",
    "        for variable in combination\n",
    "            modelLikelihoodDistrs[variable] = likelihoodDistrs[variable]\n",
    "        end\n",
    "        bic = BIC(modelLikelihoodDistrs, train)\n",
    "        if bic == 0.0\n",
    "            continue\n",
    "        end\n",
    "        push!(bics, bic)\n",
    "    end\n",
    "    _, indexMax = findmax(bics)\n",
    "    println(\"Best combination: \", combinations[indexMax], \" with bic: \", bics[indexMax])\n",
    "    return (combinations[indexMax], bics[indexMax])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
