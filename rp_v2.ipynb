{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport de projet final: Débordement d'égouts\n",
    "## Équipe 9 - A19\n",
    "#### _Noboru Yoshida - Mehdi Chaid - Mathieu Giroux-Huppé - Maxime Gosselin_\n",
    "<br>\n",
    "20 Décembre 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "+ [1. Introduction](#1.)\n",
    "    + [1.1. Objectifs](#1.1.)  \n",
    "    + [1.2. Méthode de travail](#1.2.)  \n",
    "<br>\n",
    "+ [2. Analyse exploratoire](#2.)\n",
    "    + [2.1. Motivation](#2.1.)  \n",
    "    + [2.2. Importation des données](#2.2.)  \n",
    "    + [2.3. Fonctions utilisées](#2.3.)\n",
    "    + [2.4. Analyse des données d'ouvrages](#2.4.)  \n",
    "        + [2.4.1. Données discrètes](#2.4.1.)\n",
    "        + [2.4.2. Données de Trop-Plein](#2.4.2.)\n",
    "        + [2.4.3. Données d'Emissaire](#2.4.3.)\n",
    "    + [2.5. Analyse des données de précipitations](#2.5.)  \n",
    "        + [2.5.1. Données de précipitations](#2.5.1.)\n",
    "        + [2.5.2. Somme des précipitations journalières](#2.5.2.)\n",
    "        + [2.5.3. Taux horaire maximal de précipitations journalières](#2.5.3.)\n",
    "        + [2.5.4. Taux sur trois heures maximal des précipitations journalières](#2.5.4.)\n",
    "    + [2.6. Analyse des données de surverses](#2.6.)\n",
    "        + [2.6.1. Chargement des surverses pour analyse](#2.6.1.)\n",
    "        + [2.6.2. Lien entre ouvrages et surverses](#2.6.2.)\n",
    "        + [2.6.3. Lien entre précipitations de surverses](#2.6.3.)\n",
    "    + [2.7. Isolation des ouvrages d'intérêts](#2.7.)\n",
    "        + [2.7.1. Motivation](#2.7.1.)\n",
    "        + [2.7.2. Chargement des données traitées](#2.7.2.)\n",
    "        + [2.7.3. Station la plus proche](#2.7.3.)\n",
    "        + [2.7.4. Deuxième station la plus proche](#2.7.4.)\n",
    "    + [2.8. Retour sur l'analyse](#2.8.)\n",
    "<br>\n",
    "\n",
    "\n",
    "+ [3. Traitement des données](#3.)\n",
    "    + [3.1. Motivation](#3.1.)  \n",
    "    + [3.2. Processus de traitement](#3.2.)\n",
    "    + [3.3. Fonctions utilisées](#3.3.)\n",
    "    + [3.4. Considérations et étapes retenues](#3.4.)\n",
    "    + [3.5. Isolation des ouvrages](#3.6.)\n",
    "    + [3.6. Rafinements isolés](#3.7.)\n",
    "<br>\n",
    "\n",
    "\n",
    "+ [4. Sélection de modèles](#4.)\n",
    "    + [4.1. Motivation](#4.1)\n",
    "    + [4.2. Choix des modèles](#4.2.)\n",
    "    + [4.3. Arbres de décision et forêt aléatoire](#4.3.)  \n",
    "    + [4.4. Régression logistique](#4.4.)  \n",
    "    + [4.5. Machine à vecteurs de support](#4.5.)  \n",
    "    + [4.6. Classification bayésienne naive](#4.6.)  \n",
    "    + [4.7. Ensemble de modèles](#4.7.)  \n",
    "<br>\n",
    "+ [5. Retour et conclusion](#5.)\n",
    "    + [5.1. Retour sur les résultats](#5.1.)  \n",
    "    + [5.2. Améliorations possibles](#5.2.)\n",
    "    + [5.3. Difficultés rencontrées](#5.3.)\n",
    "        + [5.3.1. Interpretation des données](#5.3.1.)\n",
    "    + [5.4. Conclusion](#5.5.)  \n",
    "<br>\n",
    "\n",
    "+ [6. Références](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.\"></a>\n",
    "\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1.\"></a>\n",
    "### 1.1. Objectifs\n",
    "\n",
    "Ce rapport vise à fournir une prédiction sur les surverses dans plusieurs ouvrages sur l'île de Montréal. Il en existe 170 répartis sur tout le bord de l'île. Dans ce travail, nous nous attarderons sur la prédiction de seulement 5 d'entre-eux, soit les suivants :\n",
    "- 3260-01D dans Rivière-des-Prairies\n",
    "- 3350-07D dans Ahunstic\n",
    "- 4240-01D dans Pointe-aux-Trembles\n",
    "- 4350-01D dans le Vieux-Montréal\n",
    "- 4380-01D dans Verdun\n",
    "\n",
    "Nous avons à notre disposition 3 jeux de données qui nous aiderons à trouver une relation entre la quantité de pluie tombé et les surverses de certain ouvrages. Nous devrons alors entraîner un modèle sur les données de 2013 à 2018, puis prédire sur certaines dates de 2019. Il n'y pas de restrictions au niveau des techniques à utilisés pour le type de modèle à utiliser. Nous avons exploré quelques techniques différentes que nous allons détaillé plus en détail dans la [section 5](#model-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2.\"></a>\n",
    "### 1.2. Déroulement de travail\n",
    "\n",
    "Le projet s'est déroulé de la sorte: \n",
    "\n",
    " 1. Tout d'abord, nous avons effectué une analyse superficielle des données afin de comprendre leur sens et de pouvoir commencer la tâche.\n",
    "\n",
    " 2. Puis, nous avons procédé à une exploration initiale des modèles selon les données que nous avons et en pesant les avantages et inconvénients de chaque modèle afin de déterminer les plus appropriés.\n",
    "\n",
    " 3. Par la suite, nous avons effectué un traitement initiale des données afin de les intégrer aux modèles.\n",
    "\n",
    " 4. À cette étape, nous avons appliqué les multiples modèles choisis à nos données traitées afin de les analyser et d'isoler les plus performants.\n",
    "\n",
    " 5. Nous avons ensuite procédé à une analyse plus poussée afin de trouver des manière d'améliorer nos données.\n",
    "\n",
    " 6. Nous avons alors traité plus en profondeur les données pour les adapter à la situation et rendre nos modèles plus robustes.\n",
    "\n",
    " 7. Finalement, nous avons rafiné les modèles selon ces nouvelles données, menant à nos prédictions finales.\n",
    "\n",
    "Nous aborderons dans ce rapport chacune de ces parties, séparées en 3 groupes: l'analyse exploratoire, le traitement de données et la sélection de modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.\"></a>\n",
    "## 2. Analyse exploratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1.\"></a>\n",
    "### 2.1. Motivation\n",
    "\n",
    "L'analyse exploratoire est une très importante pour tout exercice en sciences de données. Elle permet de déterminer, entres autres:\n",
    "\n",
    "- Quelles variables vont jouer un rôle important pour la prédiction.\n",
    "- À quel point les variables sont corrélées entre elles.\n",
    "- Si une régularisation est nécessaire pour certaines variables.\n",
    "- Comment améliorer le temps d'entrainement et les prédictions.\n",
    "\n",
    "Pour ce faire, nous allons analyser chacune des variables explicatives en utilisant des graphes et des statistiques afin d'appuyer nos résultats et de guider notre prise de décision.\n",
    "\n",
    "Cette section du rapport, qui est assez longue, peut être ignorée si le lecteur est déjà familier avec les jeux de données utilisés. Elle permet néanmoins de mieux comprendre comment nous avons choisi les variables d'intérêts et comment nous sommes arrivés aux conclusions que nous avons tiré lors de la préparation de modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2.\"></a>\n",
    "### 2.2. Importation des données\n",
    "\n",
    "Installons d'abord les libraires utilisées pour l'analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\");\n",
    "Pkg.add(\"Random\");\n",
    "Pkg.add(\"DataStructures\");\n",
    "Pkg.add(\"BenchmarkTools\");\n",
    "Pkg.add(\"DataFrames\");\n",
    "Pkg.add(\"Statistics\");\n",
    "Pkg.add(\"Dates\");\n",
    "Pkg.add(\"Gadfly\");\n",
    "Pkg.add(\"IterTools\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons maintenant ces librairies dans le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading DataFrames support into Gadfly.jl\n",
      "└ @ Gadfly /home/chaime/.julia/packages/Gadfly/09PWZ/src/mapping.jl:228\n"
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames, DataStructures, BenchmarkTools, Statistics, Dates, Gadfly, Random, IterTools;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de l'outil de visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(25cm, 13cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3.\"></a>\n",
    "### 2.3. Fonctions utilisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons ici les fonctions utilisées pour l'analyse exploratoire de données. Ces fonctions sont implémentées dans le fichier suivant afin de ne pas polluer le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"utils/precipitation.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3.1.\"></a>\n",
    "#### 2.3.1. random_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette function est un remplacement à la fonction first qui nous permet d'obtenir un échantillon aléatoire de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_first (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function random_first(df::DataFrame, nb::Int64, fts::Array=[])\n",
    "    if length(fts) == 0\n",
    "        first(df[shuffle(1:size(df, 1)),:], nb)\n",
    "    else\n",
    "        first(df[shuffle(1:size(df, 1)),fts], nb)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3.2.\"></a>\n",
    "#### 2.3.2. maximum_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction permet d'obtenir le taux maximal de précipitations sur 3h pour une journée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maximum3 (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function maximum3(col)\n",
    "    maxP = 0;\n",
    "\n",
    "    for i=1:(size(col, 1) - 2)\n",
    "        p3h = col[i, 1] + col[i + 1, 1] + col[i + 2, 1]\n",
    "\n",
    "        if p3h > maxP\n",
    "            maxP = p3h\n",
    "        end\n",
    "    end\n",
    "    return maxP;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4.\"></a>\n",
    "### 2.4. Analyse des données d'ouvrages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer le fichier de données d'ouvrages et renommons les colonnes pour qu'elles soient plus facile à traiter. Le fichier concerné est _ouvrages-surverses.csv_. Pour plus d'informations sur ce jeu de données: http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `names!(df::AbstractDataFrame, vals::Vector{Symbol}; makeunique::Bool=false)` is deprecated, use `rename!(df, vals, makeunique=makeunique)` instead.\n",
      "│   caller = top-level scope at In[267]:3\n",
      "└ @ Core In[267]:3\n"
     ]
    }
   ],
   "source": [
    "ouvrages = CSV.read(\"data/ouvrages-surverses.csv\");\n",
    "colnames = [\"N_ENV\", \"ID_SOMA\", \"ID_OUVRAGE\", \"NOM\", \"SOMA_SEC\", \"REGION\", \"TP_X\", \"TP_Y\", \"TP_Z\", \"TP_LAT\", \"TP_LNG\", \"EMI_X\", \"EMI_Y\", \"EMI_LNG\", \"EMI_LAT\"];\n",
    "names!(ouvrages, Symbol.(colnames));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4.1.\"></a>\n",
    "#### 2.4.1. Données discrètes\n",
    "\n",
    "Tout d'abord, procédons à une analyse des données discrètes des ouvrages. Les données discrètes de ce jeu de données sont:\n",
    "\n",
    "- N_ENV: Identifiant de l'ouvrage Environnement Canada.\n",
    "- ID_SOMA: Identifiant de l'ouvrage selon SOMAEU (Gouvernement du Québec).\n",
    "- ID_OUVRAGE: Identifiant de l'ouvrage selon la ville.\n",
    "- NOM: Nom de l'ouvrage.\n",
    "- SOMA_SEC: Nom du secteur SOMAEU où se trouve l'ouvrage.\n",
    "- REGION: Nom de la région (arrondissement ou municipalité) où se trouve l'ouvrage.\n",
    "\n",
    "Analysons brièvement chacune de ces variables afin de déterminer si elles pourraient jouer le rôle de variable explicative dans notre situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*N_ENV, ID_SOMA, ID_OUVRAGE, NOM:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_first(ouvrages, 10, [:N_ENV, :ID_SOMA, :ID_OUVRAGE, :NOM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, chaque ouvrage possède une valeur unique pour ces colonnes. Celà fait du sens, compte tenu qu'il s'agit d'un identifiant unique à l'ouvrage, exprimé sous plusieurs normes.\n",
    "<br>\n",
    "\n",
    "En raison du faible pouvoir prédictif d'une telle propriété, dûe à l'unicité de chaque valeur, et de la haute corrélation entre les variables, nous avons décidé de ne pas inclure ces variables dans notre modèle. Nous conservons évidemment l'identifiant de l'ouvrage (_ID_OUVRAGE_) qui va permettre d'identifier chaque ouvrage et de joindre plusieurs jeux de données ensemble, mais les autres ne semblent pas posséder de pouvoir prédictif significatif.\n",
    "<br>\n",
    "\n",
    "Il est à noter que le numéro contenu dans _N_ENV_ et _ID_SOMA_, ainsi que les 3 lettres dans _ID_SOMA_ pourraient tous deux permettrent de grouper certains ouvrages ensembles ou de les localiser dans une carte, mais d'autres variables explicatives abordées à la section [2.4.1. Données discrètes](#2.4.1.) et [2.4.2. Données de Trop-Plein](#2.4.2.) s'occupent déjà de cette localisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_SOMA_SEC, REGION_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons maintenant les effets de ces deux variables discrètes. Tout d'abord, le secteur selon la norme SOMAEU. Visualisons la carte de Montréal selon les positions de Trop-Pleins, identifiant le secteur SOMAEU par couleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrages,x=:TP_LNG, y=:TP_LAT, Geom.point, color= :SOMA_SEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, la majorité des ouvrages sont groupés sous le secteur _OMAEU de Montreal (Station Jean-R.-Marcotte_). Ce problème d'imbalancement de classes pourrait s'avérer problématique. Effectuons maintenant la même tâche pour la séparation de secteurs selon leur arrondissement ou municipalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrages,x=:TP_LNG, y=:TP_LAT, Geom.point, color= :REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les secteurs sont beaucoups mieux balancés. Cette variable explicative permet de grouper ensemble un certain nombres d'ouvrages, ce qui pourrait s'avérer utile pour la prédiction.\n",
    "\n",
    "On remarque cependant que certaines régions ne contiennent que très peu d'ouvrages. Il pourrait être intéressant de grouper ensemble des régions contigues afin d'obtenir un nombre d'ouvrage par région plus balancé encore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4.2.\"></a>\n",
    "#### 2.4.2. Données de trop-plein\n",
    "\n",
    "Passons maintenant à l'analyse des données de trop-plein. Nous avons ici 5 variables explicatives pour le trop-plein.\n",
    "\n",
    "- TP_X: Coordonnée en X selon la norme NAD83 MTM8.\n",
    "- TP_Y: Coordonnée en Y selon la norme NAD83 MTM8\n",
    "- TP_Z: Coordonnée en Z selon la norme NAD83 MTM8.\n",
    "- TP_LNG: Longitude selon la norme WGS84.\n",
    "- TP_LAT: Latitude selon la norme WGS84.\n",
    "\n",
    "Pour ce qui est de l'interprétation des trops-pleins, bien que nous ayons eu certaines difficultés à correctement interpréter le sens de ce terme en comparaison avec les émissaires, comme discuté dans la section [5.3. Difficultés rencontrées](#5.3.), des recherches plus poussées en ligne nous ont guidé vers le sens du trop-plein dans ce contexte: Il s'agit ici de l'ouvrage qui permet aux eaux non dirigées vers la station d'épuration d'être évacuées vers le milieu récepteur naturel<sup>[[1]](#refs)</sup>. \n",
    "\n",
    "Les couples de coordonnées [X, Y] et [LNG, LAT] semblent redondantes, testons cette hypothèse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TP_X, TP_Y, TP_LNG, TP_LAT:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_x_y = plot(ouvrages, x=:TP_X, y=:TP_Y, Geom.point);\n",
    "plt_lng_lat = plot(ouvrages, x=:TP_LNG, y=:TP_LAT, Geom.point);\n",
    "hstack(plt_x_y, plt_lng_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir sur les graphiques, ces données sont en effet plutôt similaires et mappent toutes les deux le contour de l'Ile de Montréal, où sont situés les Trop-Pleins. Nous avons choisi de ne garder pour l'instant que les latitudes et longitudes, ce format étant consistent avec les données présentes dans un autre jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TP_Z:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons maintenant la répartition des hauteurs de trop-plein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrages, x=:TP_Z, Geom.histogram(bincount=50), Guide.xlabel(\"Hauteur de trop-plein\"),Guide.ylabel(\"Fréquence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une analyse sur les données de hauteur nous informent que cette information est manquante pour une partie des ouvrages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(ouvrages[!, :TP_Z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, environ 10% des données sont manquantes. Une solution à ce problème est proposée dans la section [3. Traitement des données](#3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4.3.\"></a>\n",
    "#### 2.4.3. Données d'emissaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluons l'analyse des données d'ouvrages avec les variables concernant l'émissaire. Selon la description des données accompagnant le jeu de données sur le site de Montréal, le positionnement géographique des ouvrages de débordement est associé aux émissaires dans cet ensemble de données<sup>[[2]](#refs)</sup>. \n",
    "\n",
    "Encore une fois, ces coordonnées sont présentes selon les deux normes, avec le couple [X,Y] ou le couple [LNG, LAT]. Nous choisissons le second afin de rester consistent avec nos autres choix jusqu'à présent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces données contiennent des valeurs manquantes, comme l'indique la description des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(ouvrages[!, :EMI_LNG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(ouvrages[!, :EMI_LAT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela correspond aux indications présentes sur le site web. Le traitement des données manquantes sera abordé dans la section [3. Traitement des données](#3.). Pour l'instant, nous allons ignorer les données manquantes afin d'observer le graphique des émissaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emi_lng_no_missing = collect(skipmissing(ouvrages[!, :EMI_LNG]));\n",
    "emi_lat_no_missing = collect(skipmissing(ouvrages[!, :EMI_LAT]));\n",
    "plot(x=emi_lng_no_missing, y=emi_lat_no_missing, Geom.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, la forme ressemble fortement à celle obtenue avec les données de trop-pleins. Stackons les deux pour voir la différence et comparons verticalement le résultat lorsque chacun est par dessus l'autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_first_emi = plot(layer(ouvrages, x=:TP_LNG, y=:TP_LAT, Geom.point, order=1, Theme(default_color=colorant=\"orange\")),\n",
    "     layer(x=emi_lng_no_missing, y=emi_lat_no_missing, Geom.point, order=2, Theme(default_color=colorant=\"blue\")),\n",
    "     Guide.title(\"Émissaire par dessus\"), Guide.xlabel(nothing),Guide.ylabel(nothing));\n",
    "\n",
    "emi_first_tp = plot(layer(ouvrages, x=:TP_LNG, y=:TP_LAT, Geom.point, order=2, Theme(default_color=colorant=\"orange\")),\n",
    "     layer(x=emi_lng_no_missing, y=emi_lat_no_missing, Geom.point, order=1, Theme(default_color=colorant=\"blue\")),\n",
    "     Guide.title(\"Trop-plein par dessus\"), Guide.xlabel(nothing),Guide.ylabel(nothing));\n",
    "\n",
    "hstack(tp_first_emi, emi_first_tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, ces coordonnées sont assez semblables. Utiliser les deux dans notre modèle pourrait donc entrainer des problèmes de corrélation. Nous avons, pour la majorité du temps de travail, utilisé les coordonnées de trop-pleins afin d'effectuer nos prédictions, n'utilisant donc pas les données d'émissaires dans notre modèle. Ce choix provient du fait que les données d'émissaires sont parfois manquantes (10% du temps).\n",
    "<br>\n",
    "\n",
    "Une autre approche utilisée récemment nous permet de ne pas avoir à choisir entre les deux et de ne pas utiliser de données géographiques. Cette approche sera discutée dans la section [2.7. Isolation des ouvrages d'intérêts](#2.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.5.\"></a>\n",
    "### 2.5. Analyse des données de précipitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.5.1.\"></a>\n",
    "#### 2.5.1. Données de précipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à l'analyse des données de précipitations à certaines stations pluviométrique. Nous devons effectuer des fonctions d'aggrégation sur ce tableau avant de pouvoir l'intégré dans notre jeu de donnée de nos modèles. En effet, ce tableau contient les données de précipitation par heure pour chacune des cinqs stations météorologiques près de montréal. Cependant, notre jeu de données des features est seulement par jour. Nous allons donc applique 3 fonctions d'aggrégation différente, soit : Somme, Maximum et Maximum3h. \n",
    "\n",
    "Ce jeu de données a été préparé par notre professeur à partir des données météos d'environnement Canada<sup>[[3]](#refs)</sup> et contient la date sous le format yyyy-mm-jj, l'heure, ainsi que la quantité de pluie au dixième de millimètre pour chacune des cinqs stations pluviométriques suivante :\n",
    "- McTavish (7024745)\n",
    "- Ste-Anne-de-Bellevue (702FHL8)\n",
    "- Montreal/Pierre Elliott Trudeau Intl (702S006)\n",
    "- Montreal/St-Hubert (7027329)\n",
    "- L’Assomption (7014160)\n",
    "\n",
    "Les dates contenues dans ce dataset comprennent les années 2013 à 2019. Par contre, nous devons entraîner notre modèle sur les années avant 2019, et prédire sur toute l'année 2019 (de mai à octobre). Il est donc important de noter que cette table contient des données de l'ensemble d'entraînement et de test.\n",
    "\n",
    "Commençons par importer les données de précipitations et renommer la colonne St-Hubert afin qu'elle soit plus facile à traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\");\n",
    "rename!(precipitations, Symbol(\"St-Hubert\")=>:StHubert);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne sommes intéressés que par les mois de Mai à Octobre, inclusivement. Analysons les valeurs associés à ce lapse de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>date</th><th>heure</th><th>McTavish</th><th>Bellevue</th><th>Assomption</th><th>Trudeau</th><th>StHubert</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>2018-05-01</td><td>21</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>2014-06-04</td><td>23</td><td>missing</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>2013-07-12</td><td>23</td><td>0</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr><tr><th>4</th><td>2016-06-08</td><td>18</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>5</th><td>2019-10-28</td><td>3</td><td>2</td><td>missing</td><td>missing</td><td>0</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& date & heure & McTavish & Bellevue & Assomption & Trudeau & StHubert\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 2018-05-01 & 21 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t2 & 2014-06-04 & 23 &  & 0 & 0 & 0 & 0 \\\\\n",
       "\t3 & 2013-07-12 & 23 & 0 & 0 & 0 & 0 &  \\\\\n",
       "\t4 & 2016-06-08 & 18 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\t5 & 2019-10-28 & 3 & 2 &  &  & 0 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×7 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ date       │ heure │ McTavish │ Bellevue │ Assomption │ Trudeau │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m     │ \u001b[90mInt64⍰\u001b[39m  │\n",
       "├─────┼────────────┼───────┼──────────┼──────────┼────────────┼─────────┤\n",
       "│ 1   │ 2018-05-01 │ 21    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 2   │ 2014-06-04 │ 23    │ \u001b[90mmissing\u001b[39m  │ 0        │ 0          │ 0       │\n",
       "│ 3   │ 2013-07-12 │ 23    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 4   │ 2016-06-08 │ 18    │ 0        │ 0        │ 0          │ 0       │\n",
       "│ 5   │ 2019-10-28 │ 3     │ 2        │ \u001b[90mmissing\u001b[39m  │ \u001b[90mmissing\u001b[39m    │ 0       │"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precipitations = filter(row -> month(row.date) > 4, precipitations);\n",
    "precipitations = filter(row -> month(row.date) < 11, precipitations);\n",
    "random_first(precipitations, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut l'observer ci-dessous, plusieurs de ces données sont manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Union…</th><th>Type</th></tr></thead><tbody><p>7 rows × 8 columns</p><tr><th>1</th><td>date</td><td></td><td>2013-05-01</td><td></td><td>2019-10-31</td><td>1288</td><td></td><td>Date</td></tr><tr><th>2</th><td>heure</td><td>11.5</td><td>0</td><td>11.5</td><td>23</td><td></td><td></td><td>Int64</td></tr><tr><th>3</th><td>McTavish</td><td>1.42821</td><td>0</td><td>0.0</td><td>2082</td><td></td><td>1585</td><td>Union{Missing, Int64}</td></tr><tr><th>4</th><td>Bellevue</td><td>1.16081</td><td>0</td><td>0.0</td><td>295</td><td></td><td>4010</td><td>Union{Missing, Int64}</td></tr><tr><th>5</th><td>Assomption</td><td>1.40768</td><td>0</td><td>0.0</td><td>326</td><td></td><td>1789</td><td>Union{Missing, Int64}</td></tr><tr><th>6</th><td>Trudeau</td><td>1.21048</td><td>0</td><td>0.0</td><td>366</td><td></td><td>182</td><td>Union{Missing, Int64}</td></tr><tr><th>7</th><td>StHubert</td><td>1.22368</td><td>0</td><td>0.0</td><td>307</td><td></td><td>5206</td><td>Union{Missing, Int64}</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Union… & Type\\\\\n",
       "\t\\hline\n",
       "\t1 & date &  & 2013-05-01 &  & 2019-10-31 & 1288 &  & Date \\\\\n",
       "\t2 & heure & 11.5 & 0 & 11.5 & 23 &  &  & Int64 \\\\\n",
       "\t3 & McTavish & 1.42821 & 0 & 0.0 & 2082 &  & 1585 & Union\\{Missing, Int64\\} \\\\\n",
       "\t4 & Bellevue & 1.16081 & 0 & 0.0 & 295 &  & 4010 & Union\\{Missing, Int64\\} \\\\\n",
       "\t5 & Assomption & 1.40768 & 0 & 0.0 & 326 &  & 1789 & Union\\{Missing, Int64\\} \\\\\n",
       "\t6 & Trudeau & 1.21048 & 0 & 0.0 & 366 &  & 182 & Union\\{Missing, Int64\\} \\\\\n",
       "\t7 & StHubert & 1.22368 & 0 & 0.0 & 307 &  & 5206 & Union\\{Missing, Int64\\} \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "7×8 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ variable   │ mean    │ min        │ median │ max        │ nunique │\n",
       "│     │ \u001b[90mSymbol\u001b[39m     │ \u001b[90mUnion…\u001b[39m  │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m  │\n",
       "├─────┼────────────┼─────────┼────────────┼────────┼────────────┼─────────┤\n",
       "│ 1   │ date       │         │ 2013-05-01 │        │ 2019-10-31 │ 1288    │\n",
       "│ 2   │ heure      │ 11.5    │ 0          │ 11.5   │ 23         │         │\n",
       "│ 3   │ McTavish   │ 1.42821 │ 0          │ 0.0    │ 2082       │         │\n",
       "│ 4   │ Bellevue   │ 1.16081 │ 0          │ 0.0    │ 295        │         │\n",
       "│ 5   │ Assomption │ 1.40768 │ 0          │ 0.0    │ 326        │         │\n",
       "│ 6   │ Trudeau    │ 1.21048 │ 0          │ 0.0    │ 366        │         │\n",
       "│ 7   │ StHubert   │ 1.22368 │ 0          │ 0.0    │ 307        │         │"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(precipitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(precipitations[!, :McTavish])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La stratégie de résolution des données manquantes est discutée dans la section [3. Traitement des données](#3.). Pour l'instant, gardons seulement les données complêtes à des fins d'observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropmissing!(precipitations);\n",
    "size(precipitations, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.5.2.\"></a>\n",
    "#### 2.5.2. Somme des précipitations journalières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par obtenir la somme des précipitations dans une journée, par station pluviométrique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_sum = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>sum, \n",
    "            Bellevue = :Bellevue=>sum,\n",
    "            Assomption = :Assomption=>sum, \n",
    "            Trudeau = :Trudeau=>sum, \n",
    "            StHubert = :StHubert=>sum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a beaucoup d'éléments sur ce graphique. Une particularité de Gadfly est qu'il nous permet de choisir les séries à afficher. On peut cliquer donc cliquer sur la station d'intérêt afin de filtrer les résultats. On peut remarquer sur ce graphique que les stations ont des tendances similaires, mais pas totalement identiques. Si nous prennons McTavish et Assomption pour l'année 2016, nous pouvons voir que la majorité des pics se superposent, avec quelques exceptions où des pics plus bas apparaissent seulement pour une station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = filter(row -> year(row.date) == 2016, pcp_sum);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Somme des précipitations des stations météo pour 2016\"), Guide.xlabel(\"Date (j)\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait donc, jusqu'à présent, que McTavish reçoive plus de précipitations en moyenne que les autres stations. Voyons si cette tendance se maintient pour les autres années."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = melt(pcp_sum, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.point, color=:variable, Guide.title(\"Somme des précipitations des stations météo de 2013 à 2018\"), Guide.xlabel(\"Date (j)\"),  Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tendance semble se conserver, mais l'année 2019 est très différente, beaucoup d'informations sont manquantes. Analysons séparément l'année 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = filter(row -> year(row.date) == 2019, pcp_sum);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Somme des précipitations des stations météo pour 2019\"), Guide.xlabel(\"Date (j)\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, nous ne possédons des données de précipitations que pour la majorité du mois de Mai. En réalité, une analyse de la table de précipitation nous informe que nous possédons les informations couvrant la majorité de l'année pour certaines stations, mais d'autres, comme Bellevue, ne sont couvertes que jusqu'en Mai. Une solution à ce problème sera abordé à la section [3. Traitement des données](#3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.5.3.\"></a>\n",
    "#### 2.5.3. Taux horaire maximal des précipitations journalières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons au taux horaire maximal par jour des précipitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>maximum,\n",
    "            Bellevue = :Bellevue=>maximum, \n",
    "            Assomption = :Assomption=>maximum,\n",
    "            Trudeau = :Trudeau=>maximum,\n",
    "            StHubert = :StHubert=>maximum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de ces informations pour l'année 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plot = filter(row -> year(row.date) == 2016, pcp_max);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Taux horaire maximal des précipitations pour 2016\"), Guide.xlabel(\"Date (j)\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique semble suggérer une tendance similaire à celle que la somme des précipitations journalières nous procure. Le reste des informations étant assez similaires entre la somme et le taux horaire maximal, nous ne montrerons pas de graphes additionels dans cette sous-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.5.4.\"></a>\n",
    "#### 2.5.4. Taux sur trois heures maximal des précipitations journalières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, voyons le taux sur trois heures maximal des précipitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max3 = by(precipitations, :date,\n",
    "                McTavish = :McTavish=>maximum3,\n",
    "                Bellevue = :Bellevue=>maximum3,\n",
    "                Assomption = :Assomption=>maximum3,\n",
    "                Trudeau = :Trudeau=>maximum3,\n",
    "                StHubert = :StHubert=>maximum3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de ces informations pour l'année 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_for_plot = filter(row -> year(row.date) == 2016, pcp_max3);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable, Guide.title(\"Taux maximal sur 3 heures pour 2016\"), Guide.xlabel(\"Date (j)\"), Guide.ylabel(\"Précipitation (mm 10^-1)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, nous pouvons voir des positions de pics similaires entre les trois graphiques, avec des hauteurs de pics différentes compte tenu des types de valeurs variables.\n",
    "\n",
    "En effet, la somme des précipitations pour une journée sera toujours supérieure ou égale au taux maximal sur 3h qui lui même sera toujours supérieur ou égal au taux horaire maximal. Démontrons cette hypothèse avec les données pour McTavish en 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_pt = 2018;\n",
    "loc = :McTavish\n",
    "mct_sum = pcp_sum[:,[1,2]]\n",
    "rename!(mct_sum, loc => :Sum);\n",
    "mct_sum = filter(row -> year(row.date) == date_to_pt, mct_sum);\n",
    "\n",
    "mct_max = pcp_max[:,[1,2]]\n",
    "rename!(mct_max,loc => :Max);\n",
    "mct_max = filter(row -> year(row.date) == date_to_pt, mct_max);\n",
    "\n",
    "mct_max3 = pcp_max3[:,[1,2]]\n",
    "rename!(mct_max3,loc => :Max3);\n",
    "mct_max3 = filter(row -> year(row.date) == date_to_pt, mct_max3);\n",
    "\n",
    "df_for_plot = join(mct_sum, mct_max3, on = :date);\n",
    "df_for_plot = join(df_for_plot, mct_max, on = :date);\n",
    "df_for_plot = melt(df_for_plot, :date)\n",
    "\n",
    "\n",
    "plot(df_for_plot, x=:date, y=:value, Geom.line, color=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.6.\"></a>\n",
    "### 2.6. Analyse des données de surverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.6.1\"></a>\n",
    "#### 2.6.1. Chargement des surverses pour analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous chargeons les données du fichier <i>Surverses.csv</i>. Ce jeu de donnée provient des données ouvertes de Montréal : http://donnees.ville.montreal.qc.ca/dataset/debordement. Le fichier ne correspond pas tout à fait aux données en ligne. Il y a eu des transformations (jointures) apportées par notre professeur. Le fichier est structuré de la sorte:\n",
    "<br>\n",
    "\n",
    "**NO_Ouvrage** : (String) L'idendifiant unique de chacun des ouvrages <br>\n",
    "**Date** : (Date) La date sous le format yyyy-mm-jj de l'observation <br>\n",
    "**Surverse** : (Bool) Si l'ouvrage a surversé à cette date <br>\n",
    "**Raison** : (String) Abréviation de la raison de la surverse. Les options sont variables, Ex:\n",
    "- \"U\" : Urgence;\n",
    "- \"Inconnue\" : La raison est inconnue;\n",
    "- \"TS\" : Déversement par temps sec;\n",
    "- \"P\" : Débordement du à la pluie; \n",
    "- \"N\" : Rejet à la rivière des Prairies;\n",
    "- \"S\" : Rejet au fleuve St-Laurent;\n",
    "\n",
    "Au final, nous ne garderons que les trois premières colonnes. La colonne NO_Ouvrage sera renommé afin d'être consistente avec l'identifiant de l'ouvrage des autres jeux de données. De plus, nous ne garderons que les surverses dûes à la pluie, à un débordement par temps sec ou à une raison inconnue que nous considérerons comme étant dûe à la pluie.\n",
    "\n",
    "Chargeons les données de surverses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = CSV.read(\"data/surverses.csv\", missingstring=\"-99999\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne gardons que les mois de Mai à Octobre inclusivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = filter(row -> month(row.DATE) > 4, surverses);\n",
    "surverses = filter(row -> month(row.DATE) < 11, surverses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conservons les surverses dûes à la pluie, au temps sec ou de raison inconnue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "raison = coalesce.(surverses[:,:RAISON],\"Inconnue\");\n",
    "surverses[!,:RAISON] = raison;\n",
    "\n",
    "surverses = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], surverses);\n",
    "select!(surverses, [:NO_OUVRAGE, :DATE, :SURVERSE]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de surverses manquantes sont retirées: On ne peut rien analyser avec celles ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = dropmissing(surverses, disallowmissing=true);\n",
    "rename!(surverses, :NO_OUVRAGE => :ID_OUVRAGE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.6.2.\"></a>\n",
    "#### 2.6.2. Lien entre ouvrages et surverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe t'il des ouvrages qui sont plus susceptibles à des surverses que d'autres? C'est ce que nous allons chercher à déterminer dans cette sous-section. Commençons par joindre ces deux jeux de données ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrages_surverses = join(ouvrages, surverses, on =:ID_OUVRAGE);\n",
    "select!(ouvrages_surverses, [:ID_OUVRAGE, :SURVERSE, :TP_LNG, :TP_LAT, :TP_Z]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons si la position des ouvrages influence le nombre de surverses reçues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_surverses_by_ouvrage = by(ouvrages_surverses, :ID_OUVRAGE, \n",
    "    SURVERSE = :SURVERSE => sum,\n",
    "    TP_LNG = :TP_LNG => mean, \n",
    "    TP_LAT = :TP_LAT => mean,\n",
    "    TP_Z = :TP_Z => mean);\n",
    "plot(n_surverses_by_ouvrage, x=:TP_LNG, y=:TP_LAT, Geom.point, color=:SURVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À première vue, il seble que la moitié droite de l'Île de Montréal soit plus touchée par des surverses que la moitié gauche. On observe aussi le plus de surverses au croisement milieu de l'Île. La position semble donc influencer la quantité de surverses qu'un ouvrage subit durant l'année. On se rend compte en revanche que c'est principalement la longitude qui joue en rôle dans la quantité de surverses plutôt que la longitude.\n",
    "\n",
    "Ceci n'est évidemment qu'une observation qui semble indiquer une corrélation, mais ne constitue pas une preuve de corrélation. Nous tiendrons néanmoins compte de ce résultat dans notre traitement de données puis dans notre sélection de modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand est il de la hauteur du trop plein? Observons son influence sur la quantité de surverses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropmissing!(n_surverses_by_ouvrage, disallowmissing=true);\n",
    "plot(n_surverses_by_ouvrage, x=:TP_Z, y=:SURVERSE, Geom.hair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrairement à notre intuition, la hauteur ne semble pas jouer un rôle si important sur la quantité de surverses d'un trop-plein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.6.3.\"></a>\n",
    "#### 2.6.3. Lien entre précipitations et surverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouvons maintenant le lien entre le taux de surverses et la quantité de précipitation reçus par les ouvrages. Le facteur commun direct entre les deux est la date, groupons donc ces données par dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par renommer la colonne :date des précipitations afin de pouvoir effectuer la jointure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename!(pcp_sum, :date => :DATE)\n",
    "rename!(pcp_max, :date => :DATE)\n",
    "rename!(pcp_max3, :date => :DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la jointure et on groupe par date afin de visualiser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_sum_surverses = join(pcp_sum, surverses, on=:DATE)\n",
    "select!(pcp_sum_surverses, Not([:ID_OUVRAGE]))\n",
    "n_surverses_by_date = by(pcp_sum_surverses, :DATE, \n",
    "                            McTavish = :McTavish => first,\n",
    "                            Bellevue = :Bellevue => first,\n",
    "                            Assomption = :Assomption => first,\n",
    "                            Trudeau = :Trudeau => first,\n",
    "                            StHubert = :StHubert => first,\n",
    "                            SURVERSE = :SURVERSE => sum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons l'effet de la quantité de pluie reçus dans une journée à McTavish sur le taux de précipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(n_surverses_by_date, x=:McTavish, y=:SURVERSE, Geom.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le remarquer, il existe une relation bien définie entre la quantité de pluie reçue et le nombre de surverses dans une journée.\n",
    "\n",
    "Voyons si ce phénomène est présent pour les autres stations pluviométriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bellevue_plot = plot(n_surverses_by_date, x=:Bellevue, y=:SURVERSE, Geom.point);\n",
    "assomption_plot = plot(n_surverses_by_date, x=:Assomption, y=:SURVERSE, Geom.point);\n",
    "trudeau_plot = plot(n_surverses_by_date, x=:Trudeau, y=:SURVERSE, Geom.point);\n",
    "st_hubert_plot = plot(n_surverses_by_date, x=:StHubert, y=:SURVERSE, Geom.point);\n",
    "gridstack([bellevue_plot assomption_plot; trudeau_plot st_hubert_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que la relation ne soit pas aussi clairement définie partout, il existe toutefois un lien entre la quantité reçue par une station et le nombre de surverses sur l'Île de Montréal. Nous tiendrons compte de cette information lors du traitement des données et de l'entrainement des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce même évènement devrait se produire pour pcp_max et pcp_max3, étant donné qu'ils sont très corrélés avec pcp_sum. Assurons nous que ce soit le cas avec pcp_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max_surverses = join(pcp_max, surverses, on=:DATE)\n",
    "select!(pcp_max_surverses, Not([:ID_OUVRAGE]))\n",
    "n_surverses_by_date = by(pcp_max_surverses, :DATE, \n",
    "                            McTavish = :McTavish => first,\n",
    "                            Bellevue = :Bellevue => first,\n",
    "                            Assomption = :Assomption => first,\n",
    "                            Trudeau = :Trudeau => first,\n",
    "                            StHubert = :StHubert => first,\n",
    "                            SURVERSE = :SURVERSE => sum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons la station McTavish pour analyser le phénomène."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(n_surverses_by_date, x=:McTavish, y=:SURVERSE, Geom.point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on pouvait l'imaginer, le graphe est très similaire à celui du pcp_sum. \n",
    "Afin de garder le rapport concis, nous ne démontrerons pas ce résultat pour les autres stations et pour pcp_max3 ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.7.\"></a>\n",
    "### 2.7. Isolation des ouvrages d'intérêts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.7.1.\"></a>\n",
    "#### 2.7.1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche qui a été prise plus tard dans le processus de développement de notre modèle a été d'isoler nos ouvrages. En effet, nous savons déjà quels ouvrages seront utilisés pour la prédiction finale. Il conviendrait alors de ne se concentrer que sur ces ouvrages dans la création de nos modèles. \n",
    "\n",
    "Ce sujet a été abordé par l'équipe dès la première journée du projet, et rapidement disciminé en faveur d'un entrainement sur toutes les données. Il a finalement été revu et mis en oeuvre dans les derniers jours de rafinement du modèle afin d'améliorer encore plus les résultats. \n",
    "\n",
    "Voici les arguments en faveur de cette approche:\n",
    "- Diminution du nombre de données d'entrées, donc permet un entrainement de modèle plus rapide.\n",
    "- Beaucoup de données d'entrées sont invariables aux ouvrages, ils n'influencent pas les chances de surverses au niveau de l'ouvrage en tel quel. \n",
    "- Permet d'avoir un modèle spécialisé par ouvrage, qui sera probablement plus précis qu'un modèle général.\n",
    "\n",
    "Voici les arguments contre cette approche:\n",
    "- Diminution du nombre de données d'entrées, donc moins d'informations pour rendre notre modèle plus robuste.\n",
    "- Ne serait pas capable de prédire les surverses pour un ouvrage autre que ceux entrainés.\n",
    "\n",
    "À la lumière de ces arguments, nous avons finalement choisi d'isoler nos ouvrages et d'entrainer un modèle par ouvrage. En effet, nous savons quels ouvrages vont être utilisés et ne cherchons pas à prédire les surverses pour d'autres ouvrages dans le cadre de ce devoir. \n",
    "\n",
    "De plus, nous éliminons ainsi un bon nombre de variables explicatives qui ne joue pas un rôle direct dans la prédiction du taux de surverses par ouvrage. Par exemple, la position d'un ouvrage ne change pas et donc cette variable n'explique plus la fluctuation des surverses pour cet ouvrage. Nous pouvons alors nous concentrer sur les seuls éléments qui changent: les données de précipitations.\n",
    "\n",
    "Ce faisant, nous éliminons donc une grande partie de l'effort que nous avons mis dans la conception d'un modèle général, y compris le traitement de certaines qui ne seront alors plus utilisés. Nous expliquerons ce que nous ferons à ce sujet dans la section [3. Traitement des données](#3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.7.2.\"></a>\n",
    "#### 2.7.2. Chargement des données traitées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le but de pouvoir analyser les données de chaque ouvrage d'intérêt séparément, ils sont passés par un processus rafinement qui sera couvert dans la section [3. Traitement des données](#3.). Afin de ne pas polluer la partie analyse du rapport, nous allons directement charger les données qui ont été générées plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons l'ouvrage 3260-01D pour commencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrage_3260 = CSV.read(\"data/parsed/ouvrage_3260.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données contient les données de précipitation pour les deux stations les plus proches de l'ouvrage, dont la somme et le maximum sur 3h des précipitations, pour chaque jour. \n",
    "\n",
    "De plus, il contient aussi l'information de surverse pour chaque date enregistrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_first(ouvrage_3260, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, FS correspond à _First Station_ et SS correspond à _Second Station_. Les colonnes _dist_ correspond à la distance entre l'ouvrage et la station correspondante. Pour ce qui des données de _sum_ , _max_ et _max3_ , elles sont standardisées avec les données complètes et traitées de toutes les stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.7.3.\"></a>\n",
    "#### 2.7.3. Station la plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant s'il y'a une relation entre les données de précipitation de la station la plus proche et le taux de surverses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_plot = plot(ouvrage_3260, x=:SURVERSE, y=:FS_sum, Geom.boxplot);\n",
    "max_plot = plot(ouvrage_3260, x=:SURVERSE, y=:FS_max, Geom.boxplot);\n",
    "max3_plot = plot(ouvrage_3260, x=:SURVERSE, y=:FS_max3, Geom.boxplot);\n",
    "hstack([sum_plot, max_plot, max3_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait d'après ce graphe en boites que la somme des précipitations est le indice de surverses. Observons le lien entre la somme et le taux maximum pour cet ouvrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrage_3260, x=:FS_sum, y=:FS_max, Geom.point, color=:SURVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on pouvait l'imaginer, ces deux variables sont très corrélées. Certains modèles ne sont pas affectés par une telle correlation mais d'autres, tel que la régression logistique, peut avoir des difficultés avec une telle corrélation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.7.4.\"></a>\n",
    "#### 2.7.4. Deuxième station la plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La question à se poser maintenant est la suivante: Existe t'il un lien entre la deuxième station la plus proche et le taux de précipitations?\n",
    "\n",
    "En effet, nous avons tout d'abord assumé que seule la station la plus proche d'un ouvrage influençait son taux de précipitation. Cette idée est fausse, car si un ouvrage se trouve à l'intersection de deux stations, et que des précipitations ne touchent qu'une des deux stations, ou en touche une plus que l'autre, l'ouvrage pourrait être affecté.\n",
    "\n",
    "En réalité, un ouvrage pourrait être à l'intersection de plus que deux stations, mais les chances sont très faibles et l'impact sur la prédiction, limité. Nous avons donc décidé de nous limiter aux deux stations les plus proches.\n",
    "\n",
    "Maintenant, il faut tenir compte du fait que ces deux stations n'ont pas le même effet sur l'ouvrage. La station la plus proche est, dans la majorité des cas, celle qui va expliquer le mieux surverses d'un ouvrage. Ainsi, nous diminuons l'impact de la deuxième station par un facteur qui dépends de distance avec l'ouvrage. Plus de détails sur cette modification à la section [3. Traitement des données](#3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons l'impact de la deuxième station par rapport à la première station pour les précipitations de cet ouvrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(ouvrage_3260, x=:FS_sum, y=:SS_sum, Geom.point, color=:SURVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce graphique montre une majorité de surverses à l'intersection de ces deux stations, mais aussi lorsque la deuxième station reçoit plus de précipitations que la station la plus proche. Ceci valide notre hypothèse concernant l'influance des autres stations sur le taux de surverses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir la distance entre l'ouvrage et les stations ainsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrage_3260[1, [:FS_dist, :SS_dist]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième station est donc $ \\frac{0.210769}{0.145981} = 1.44$, soit 44% plus loin de l'ouvrage que la station la plus proche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, afin de ne pas rendre cette section trop longue, nous ne présenterons pas les autres ouvrages ici. Les détails spécifiques à chaque ouvrage seront présentés à la section [3. Traitement des données](#3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.8.\"></a>\n",
    "### 2.8. Retour sur l'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusion, cette analyse nous a permis de repérer les informations pertinentes à notre projet et d'isoler les variables d'intérêt. Elle nous a aussi permis de détecter les possibles causes d'erreurs et a joué un rôle important pour la tâche subséquente, soit le traitement de ces données. Elle a enfin permis de renforcer ou discréditer certaines hypothèses _a priori_ que nous avions sur les données afin de les incorporer ou les retirer de nos modèles. \n",
    "\n",
    "Cette étape a ainsi constitué la partie la plus longue et fastidieuse de notre projet, mais a grandement servi à l'amélioration de nos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.\"></a>\n",
    "## 3. Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1.\"></a>\n",
    "### 3.1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section aborde une étape cruciale lors de la mise en oeuvre de notre solution. C'est en effet ici que les données sont modifiées après l'analyse de l'étape précédente afin de servir de base à nos modèles.\n",
    "\n",
    "Le choix adéquat des variables explicatives ainsi que leur rafinement jouent un rôle tout aussi important que le choix du modèle, et va grandement influencer les performances de celui ci. C'est pourquoi nous avons consacré une grande partie de notre temps à traiter et peaufiner les données pour ce projet.\n",
    "\n",
    "De plus, certains modèles ne fonctionnent bien qu'en présence de données traitées d'une certaine manière. Un tel modèle serait, par exemple, la régression logistique, qui fonctionne de manière bien plus efficace lorsque toutes les variables explicatives sont à la même échelle. La régression logisitique, ainsi que la classification bayésienne naive, marchent aussi mieux lorsque les données ne sont pas trop corrélées entre elles. C'est pourquoi il est important de s'assurer ici de bien choisir et structurer nos données avant de les offrir à nos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2.\"></a>\n",
    "### 3.2. Processus de traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons présenter dans cette section les étapes et choix faits pour la préparation et traitements des données d'entrées à nos modèles. Le processus de traitement de données a été le suivant:\n",
    "\n",
    "1. Sélection des données à utiliser\n",
    "2. Modification des données afin d'entrainer les modèles\n",
    "3. Entrainement des modèles et validation\n",
    "4. Détection des problèmes liés aux données\n",
    "5. Correction et amélioration des données\n",
    "\n",
    "Ces étapes ont été répétés en boucle tout au long du projet, en raison du nombre important de variations des modèles sélectionnés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.\"></a>\n",
    "### 3.3. Considérations et étapes retenues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette sous-section, nous présenterons chacune des considérations de traitements de données ainsi que leurs motivations. Pour les considérations qui ont été retenues, nous irons en détails sur leur implémentation pour le modèle final. \n",
    "\n",
    "En effet, bien que chacune de ces considérations ait été implémentée et testée pour entrainer des modèles et effectuer des prédictions, seulement une partie a été utile et donc gardée pour notre modèle final. \n",
    "\n",
    "Afin de ne pas rendre ce rapport plus long que nécessaire, tout en gardant un historique concis du travail effectué, nous expliquerons ici les options de traitements de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.1.\"></a>\n",
    "#### 3.3.1. Normalisation de chacune des colonnes entre 0 et 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première considération a été de normaliser chacune des colonnes entre 0 et 1. Cette modification a été implémentée afin d'uniformiser les données pour la régression logistique car ce modèle est très sensible aux différences d'ordre de grandeur entre les données. Les autres modèles sont beaucoups moins, voir pas du tout, affecté par ces différences.\n",
    "\n",
    "En effet, nos premiers modèles utilisants les informations de positions ( _TP_LNG, TP_LAT_ ), de hauteur ( _TP_Z_ ) et de précipitations ( _PCP_SUM, PCP_MAX, PCP_MAX3_ ) avaient affaire à des ordres de grandeurs très différents.\n",
    "\n",
    "Par exemple, les données de géolocalisations pour la longitude se trouvaient toutes autour de 45.5 $\\pm$ 0.5 tandis que la lattitude tournait autour de -73.5 $\\pm$ 0.5. Les données de précipitations allaient de 0 à 2500, avant traitement, et cette échelle variait d'une colonne de précipitation à une autre. Les hauteurs de trop-plein quant à elles variaient de 9 à 35 mètres en général.\n",
    "\n",
    "L'approche utilisée pour normaliser ces données a été de diviser chaque rangée de chaque colonne par la valeur maximale de cette colonne.\n",
    "\n",
    "Nous avons cependant remplacé par la suite cette approche en faveur de la standardisation des colonnes, qui sera expliquée dans la partie [3.4.3. Standardisation de chacune des colonnes](#3.4.3.). Ainsi, nous ne détaillerons pas les détails d'implémentations de la normalisation des colonnes dans cette sous-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.2.\"></a>\n",
    "#### 3.3.2. Traitement des données de hauteur manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos premiers modèles utilisant l'information de hauteur des trop-pleins, il était important de ne pas avoir de valeurs manquantes qui pourraient fausser nos résultats. L'approche initiale a été d'assigner la valeur de 0 aux données manquantes, mais a rapidement été remplacé par la moyenne des hauteurs de trop-plein. \n",
    "\n",
    "En effet, il ne fait pas de sens pour un trop-plein d'avoir une hauteur de 0, sachant que la plupart des données se trouvaient autour de 10 à 30m et que les valeurs minimales avant traitement étaient de 9. La moyenne de 19.32 permettait de ne pas trop diverger de cette zone lors de nos prédictions.\n",
    "\n",
    "En revanche, le modèle retenu étant celui des ouvrages isolés, les données de trop-plein qui sont invariables à chaque ouvrage n'étaient plus pertinentes. Ainsi, ce traitement n'a pas été retenu pour notre modèle final et son implémentation ne sera pas détaillé ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.3.\"></a>\n",
    "#### 3.3.3. Standardisation de chacune des colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La standardisation des colonnes a été utilisée à la place de la normalisation de celles ci car elle est plus facile à analyser que cette dernière. \n",
    "\n",
    "Cette standardisation s'est avérée très utile lors de la réalisation de nos premières prédictions qui utilisaient des modèles entrainés sur l'ensemble des données. En revanche, nos prédictions subséquentes ont été réalisées avec des modèles spécialisés par ouvrage qui utilisaient des données relativement uniformes en ordre de grandeur. \n",
    "\n",
    "L'intérêt de la standardisation s'est donc vu grandement atténué. Il n'en reste qu'aucun des modèles utilisés ne souffre d'une telle standardisation et que celle ci pourrait s'avérer à nouveau utile si nous nous étions retrouvés à ajouter des données d'échelles différentes à nos modèles. De plus, le coût de standardisation était faible et celui ci n'est ajouté que lorsque des changements au traitement de données sont effectués. Nous avons donc décidé de conserver cette étape. \n",
    "\n",
    "Pour chacune des colonnes contenant des valeurs continues dans nos données de prédiction, nous effectuons la modification suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "standardize_col (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function standardize_col(col)\n",
    "    mean_col = mean(col);\n",
    "    std_col = std(col);\n",
    "    \n",
    "    res = (col .- mean_col) ./ std_col;\n",
    "    \n",
    "    return res;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.4.\"></a>\n",
    "#### 3.3.4. Filtrage des mois de précipitations et surverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme abordé puis implémenté dans la section [2. Analyse exploratoire](#2.), nous ne conservons que les données de précipitations et de surverses entre Mai et Octobre. \n",
    "\n",
    "En effet, la très grande majorité des précipitations se produisent durant cette période et c'est aussi à ce moment que la majorité des surverses sont causés par la pluie. En dehors de cette période, la neige constituerait le facteur le plus probable pour une surverse, dans le cas où les données sont manquantes. De plus, nos prédictions s'effectuent sur des données situés entre Mai et Octobre, nous n'avons pas à tenir compte des tendances des autres périodes, qui représentent une incertitude considérable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechargeons les données de précipitations pour leur traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\");\n",
    "rename!(precipitations, Symbol(\"St-Hubert\")=>:StHubert);\n",
    "precipitations = filter(row -> month(row.date) > 4, precipitations);\n",
    "precipitations = filter(row -> month(row.date) < 11, precipitations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.5.\"></a>\n",
    "#### 3.3.5. Traitement des données de précipitations abérantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La description des données de précipitations semble indiquer des valeurs maximales assez élevées par rapport à la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Union…</th><th>Type</th></tr></thead><tbody><p>7 rows × 8 columns</p><tr><th>1</th><td>date</td><td></td><td>2013-05-01</td><td></td><td>2019-10-31</td><td>1288</td><td></td><td>Date</td></tr><tr><th>2</th><td>heure</td><td>11.5</td><td>0</td><td>11.5</td><td>23</td><td></td><td></td><td>Int64</td></tr><tr><th>3</th><td>McTavish</td><td>1.42821</td><td>0</td><td>0.0</td><td>2082</td><td></td><td>1585</td><td>Union{Missing, Int64}</td></tr><tr><th>4</th><td>Bellevue</td><td>1.16081</td><td>0</td><td>0.0</td><td>295</td><td></td><td>4010</td><td>Union{Missing, Int64}</td></tr><tr><th>5</th><td>Assomption</td><td>1.40768</td><td>0</td><td>0.0</td><td>326</td><td></td><td>1789</td><td>Union{Missing, Int64}</td></tr><tr><th>6</th><td>Trudeau</td><td>1.21048</td><td>0</td><td>0.0</td><td>366</td><td></td><td>182</td><td>Union{Missing, Int64}</td></tr><tr><th>7</th><td>StHubert</td><td>1.22368</td><td>0</td><td>0.0</td><td>307</td><td></td><td>5206</td><td>Union{Missing, Int64}</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Union… & Type\\\\\n",
       "\t\\hline\n",
       "\t1 & date &  & 2013-05-01 &  & 2019-10-31 & 1288 &  & Date \\\\\n",
       "\t2 & heure & 11.5 & 0 & 11.5 & 23 &  &  & Int64 \\\\\n",
       "\t3 & McTavish & 1.42821 & 0 & 0.0 & 2082 &  & 1585 & Union\\{Missing, Int64\\} \\\\\n",
       "\t4 & Bellevue & 1.16081 & 0 & 0.0 & 295 &  & 4010 & Union\\{Missing, Int64\\} \\\\\n",
       "\t5 & Assomption & 1.40768 & 0 & 0.0 & 326 &  & 1789 & Union\\{Missing, Int64\\} \\\\\n",
       "\t6 & Trudeau & 1.21048 & 0 & 0.0 & 366 &  & 182 & Union\\{Missing, Int64\\} \\\\\n",
       "\t7 & StHubert & 1.22368 & 0 & 0.0 & 307 &  & 5206 & Union\\{Missing, Int64\\} \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "7×8 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ variable   │ mean    │ min        │ median │ max        │ nunique │\n",
       "│     │ \u001b[90mSymbol\u001b[39m     │ \u001b[90mUnion…\u001b[39m  │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m  │\n",
       "├─────┼────────────┼─────────┼────────────┼────────┼────────────┼─────────┤\n",
       "│ 1   │ date       │         │ 2013-05-01 │        │ 2019-10-31 │ 1288    │\n",
       "│ 2   │ heure      │ 11.5    │ 0          │ 11.5   │ 23         │         │\n",
       "│ 3   │ McTavish   │ 1.42821 │ 0          │ 0.0    │ 2082       │         │\n",
       "│ 4   │ Bellevue   │ 1.16081 │ 0          │ 0.0    │ 295        │         │\n",
       "│ 5   │ Assomption │ 1.40768 │ 0          │ 0.0    │ 326        │         │\n",
       "│ 6   │ Trudeau    │ 1.21048 │ 0          │ 0.0    │ 366        │         │\n",
       "│ 7   │ StHubert   │ 1.22368 │ 0          │ 0.0    │ 307        │         │"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(precipitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction permet de traiter les valeurs manquantes pour la somme des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_with_missing (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum_with_missing(col)\n",
    "    somme = 0;\n",
    "    for i=1:size(col, 1)\n",
    "        if !isequal(col[i], missing)\n",
    "            somme += col[i];\n",
    "        end\n",
    "    end\n",
    "    return somme;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphe suivant permet d'observer ce phénomène, avec la station McTavish comme example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations_per_day = by(precipitations, :date,\n",
    "                            McTavish= :McTavish => sum_with_missing,\n",
    "                            Bellevue= :Bellevue => sum_with_missing,\n",
    "                            Assomption= :Assomption => sum_with_missing,\n",
    "                            Trudeau= :Trudeau => sum_with_missing,\n",
    "                            StHubert= :StHubert => sum_with_missing);\n",
    "precipitations_to_plot = melt(precipitations_per_day, :date)\n",
    "plot(precipitations_to_plot, x=:date, y=:value, color=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, certaines données sont bien au dessus de la moyenne. Prenons les valeurs abérantes au dessus de 2000 pour McTavish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>date</th><th>heure</th><th>McTavish</th><th>Bellevue</th><th>Assomption</th><th>Trudeau</th><th>StHubert</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>1 rows × 7 columns</p><tr><th>1</th><td>2013-10-10</td><td>13</td><td>2082</td><td>0</td><td>0</td><td>0</td><td>missing</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& date & heure & McTavish & Bellevue & Assomption & Trudeau & StHubert\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 2013-10-10 & 13 & 2082 & 0 & 0 & 0 &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "1×7 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ date       │ heure │ McTavish │ Bellevue │ Assomption │ Trudeau │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m     │ \u001b[90mInt64⍰\u001b[39m  │\n",
       "├─────┼────────────┼───────┼──────────┼──────────┼────────────┼─────────┤\n",
       "│ 1   │ 2013-10-10 │ 13    │ 2082     │ 0        │ 0          │ 0       │"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_outliers = precipitations[!, :McTavish] .> 2000;\n",
    "idx_outliers[isequal.(idx_outliers, missing)] .= false;\n",
    "idx_outliers = convert(Array{Bool, 1}, idx_outliers);\n",
    "\n",
    "date_outlier = precipitations[idx_outliers, :date];\n",
    "precipitations[idx_outliers, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une seule date contient la valeur au dessus de 2000, quand est il des autres stations à cette date? Les autres stations ne rapportent pas de précipitations à cette date là. Y'a t'il eu des surverses dans nos ouvrages d'intérets, le 10 Octobre 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>ID_OUVRAGE</th><th>DATE</th><th>SURVERSE</th></tr><tr><th></th><th>String</th><th>Date</th><th>Int64</th></tr></thead><tbody><p>4 rows × 3 columns</p><tr><th>1</th><td>3260-01D</td><td>2013-10-10</td><td>0</td></tr><tr><th>2</th><td>4240-01D</td><td>2013-10-10</td><td>0</td></tr><tr><th>3</th><td>4350-01D</td><td>2013-10-10</td><td>0</td></tr><tr><th>4</th><td>4380-01D</td><td>2013-10-10</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& ID\\_OUVRAGE & DATE & SURVERSE\\\\\n",
       "\t\\hline\n",
       "\t& String & Date & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3260-01D & 2013-10-10 & 0 \\\\\n",
       "\t2 & 4240-01D & 2013-10-10 & 0 \\\\\n",
       "\t3 & 4350-01D & 2013-10-10 & 0 \\\\\n",
       "\t4 & 4380-01D & 2013-10-10 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4×3 DataFrame\n",
       "│ Row │ ID_OUVRAGE │ DATE       │ SURVERSE │\n",
       "│     │ \u001b[90mString\u001b[39m     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m    │\n",
       "├─────┼────────────┼────────────┼──────────┤\n",
       "│ 1   │ 3260-01D   │ 2013-10-10 │ 0        │\n",
       "│ 2   │ 4240-01D   │ 2013-10-10 │ 0        │\n",
       "│ 3   │ 4350-01D   │ 2013-10-10 │ 0        │\n",
       "│ 4   │ 4380-01D   │ 2013-10-10 │ 0        │"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_ouvrages = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"];\n",
    "filter(row -> row.DATE == date_outlier[1] && row.ID_OUVRAGE ∈ important_ouvrages, surverses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visiblement, il y'a pas eu de surverses à cette date là, remplaçons donc cette donnée par 0 afin qu'elle n'induise pas en erreur notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>date</th><th>heure</th><th>McTavish</th><th>Bellevue</th><th>Assomption</th><th>Trudeau</th><th>StHubert</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th><th>Int64⍰</th></tr></thead><tbody><p>1 rows × 7 columns</p><tr><th>1</th><td>2013-10-10</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& date & heure & McTavish & Bellevue & Assomption & Trudeau & StHubert\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰ & Int64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 2013-10-10 & 13 & 0 & 0 & 0 & 0 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "1×7 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ date       │ heure │ McTavish │ Bellevue │ Assomption │ Trudeau │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m   │ \u001b[90mInt64⍰\u001b[39m     │ \u001b[90mInt64⍰\u001b[39m  │\n",
       "├─────┼────────────┼───────┼──────────┼──────────┼────────────┼─────────┤\n",
       "│ 1   │ 2013-10-10 │ 13    │ 0        │ 0        │ 0          │ 0       │"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precipitations[idx_outliers, :McTavish] .= 0;\n",
    "precipitations[idx_outliers, :StHubert] .= 0;\n",
    "precipitations[idx_outliers, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plus grosse donnée abérante a été résolue, regardons le graphique de précipitations à nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations_per_day = by(precipitations, :date,\n",
    "                            McTavish= :McTavish => sum_with_missing,\n",
    "                            Bellevue= :Bellevue => sum_with_missing,\n",
    "                            Assomption= :Assomption => sum_with_missing,\n",
    "                            Trudeau= :Trudeau => sum_with_missing,\n",
    "                            StHubert= :StHubert => sum_with_missing);\n",
    "precipitations_to_plot = melt(precipitations_per_day, :date)\n",
    "plot(precipitations_to_plot, x=:date, y=:value, color=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons combien de données au dessus de 200 on obtient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_outliers = precipitations[!, :McTavish] .>= 200;\n",
    "idx_outliers[isequal.(idx_outliers, missing)] .= false;\n",
    "idx_outliers = convert(Array{Bool, 1}, idx_outliers);\n",
    "\n",
    "date_outlier = precipitations[idx_outliers, :date];\n",
    "precipitations[idx_outliers, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe 7 valeurs au dessus de 200 pour 29,327 entrées. La partie qui suit est assez arbitraire: Nous regardons chacune des 7 dates au dessus de 240 et vérifions s'il y a surverse, si ce n'est pas le cas, nous enlevons les surverses abérantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations[precipitations.date .== Date(2013, 7, 17), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous pouvons voir que le 240 apparait de nul part, et constitue une valeur extrèmes de précipitations. Il n'a pas plu de la journée et d'un coup, 240 mm d'eau s'est déversé sur la station. Bien que cela reste possible, s'il n'y a pas eu de surverses à cette journée là, il est peu probable que cette donnée soit utile à notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(row -> row.DATE == Date(2013, 7, 17) && row.ID_OUVRAGE ∈ important_ouvrages, surverses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme il y a eu surverse, il est fort probable que le haut taux de pluie à cette journée ait eu lieu et ait causé la surverse, nous ne toucherons donc pas à cette donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répétons le processus pour chacun des autres ouvrages, et éliminons les données abérantes qui ne causent pas de surverses. Les étapes sont les mêmes que pour précédemment, nous éviterons donc de les redétailler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations[precipitations.date .== Date(2017, 7, 20), :Bellevue] .= 0;\n",
    "precipitations[precipitations.date .== Date(2017, 7, 20), :Trudeau] .= 0; # VOIR SI GARDER LUI\n",
    "\n",
    "precipitations[precipitations.date .== Date(2013, 6, 24), :Assomption] .= 0;\n",
    "precipitations[precipitations.date .== Date(2014, 8, 5), :Assomption] .= 0;\n",
    "precipitations[precipitations.date .== Date(2015, 6, 10), :Assomption] .= 0;\n",
    "precipitations[precipitations.date .== Date(2018, 7, 26), :Trudeau] .= 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, nous avons éliminé toutes les précipitations hors du commun ne menant pas à des surverses. Celles ci sont probablement dûes à des erreurs de mesure, ou autres, mais dans tous les cas faussent nos résultats. Cette approche reste tout de fois très arbitraire et sera discuté dans la section [5.3. Améliorations possibles](#5.3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.6.\"></a>\n",
    "#### 3.3.6. Traitement des données de précipitations manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données _precipitations.csv_ possédait aussi un nombre considérable de données manquantes. Il fallait alors trouver une solution pour travailler avec ces données en limitant la perte d'informations.\n",
    "\n",
    "Certaines journées étaient entièrement dépourvues de données de précipitations pour une station. L'approche utilisée à donc été d'assigner un taux de précipitation nul à ces journées. Une amélioration possible sera abordé à la section [5.3. Améliorations possibles](#5.3.). \n",
    "\n",
    "Pour les journées possédant des certaines valeurs manquantes mais pas toutes, nous prennons la moyenne du reste des valeurs de la journée pour la station concernée, et l'appliquons à chacune des valeurs manquantes. La plupart du temps, ces valeurs sont de 0 ou proche, la moyenne générale de chaque station n'est donc pas tant affectée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet de trouver la moyenne de précipitation d'une journée contenant des valeurs manquantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_wo_missing (generic function with 1 method)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mean_wo_missing(col)\n",
    "    mean = 0;\n",
    "\n",
    "    for i=1:size(col, 1)\n",
    "        if !isequal(col[i, 1], missing)\n",
    "            mean += col[i, 1]\n",
    "        end\n",
    "    end\n",
    "    mean = mean ÷ size(col, 1)\n",
    "    return mean;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons cette fonction à nos données de précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_by_day = by(precipitations, :date,  \n",
    "                            McTavish = :McTavish=>mean_wo_missing, \n",
    "                            Bellevue = :Bellevue=>mean_wo_missing, \n",
    "                            Assomption = :Assomption=>mean_wo_missing,\n",
    "                            Trudeau = :Trudeau=>mean_wo_missing,\n",
    "                            StHubert = :StHubert=>mean_wo_missing)\n",
    "\n",
    "for i=1:size(precipitations,1)\n",
    "    if isequal(precipitations[i, :McTavish], missing)\n",
    "        precipitations[i,:McTavish] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:McTavish][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Bellevue], missing)\n",
    "        precipitations[i,:Bellevue] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Bellevue][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Assomption], missing)\n",
    "        precipitations[i,:Assomption] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Assomption][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :Trudeau], missing)\n",
    "        precipitations[i,:Trudeau] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:Trudeau][1]\n",
    "    end\n",
    "    if isequal(precipitations[i, :StHubert], missing)\n",
    "        precipitations[i,:StHubert] = filter(row-> row.date == precipitations[i,:date], precipitation_by_day)[!,:StHubert][1]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons maintenant les données de précipitations, après traitement de données abérantes et manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitations_per_day = by(precipitations, :date,\n",
    "                            McTavish= :McTavish => mean,\n",
    "                            Bellevue= :Bellevue => mean,\n",
    "                            Assomption= :Assomption => mean,\n",
    "                            Trudeau= :Trudeau => mean,\n",
    "                            StHubert= :StHubert => mean);\n",
    "precipitations_to_plot = melt(precipitations_per_day, :date)\n",
    "plot(precipitations_to_plot, x=:date, y=:value, color=:variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données semble maintenant près à être utilisé pour la suite du processus de traitement de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.7.\"></a>\n",
    "#### 3.3.7. Traitement des données de surverses manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ce qui concerne les surverses, nous éliminons toutes les entrées qui ne contiennent pas des informations de surverses, ainsi que toutes celles en dehors de la période de Mai à Octobre.\n",
    "\n",
    "En effet, nous ne sommes intéressés que par les données qui vont servir à effectuer une prédiction et la variable de surverse constitue l'élément à prédire par nos modèles dans un contexte d'apprentissage supervisé. Il n'est donc pas possible d'entrainer, dans ce même contexte, un modèle sans cette variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, comme expliqué à la section [2. Analyse exploratoire](#2.), nous ne considèrerons que les surverses dûes à la précipitations dans ce projet. Voici le code qui permet de nettoyer nos données de surverses, déjà employé à la section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverses = CSV.read(\"data/surverses.csv\", missingstring=\"-99999\");\n",
    "surverses = filter(row -> month(row.DATE) > 4, surverses);\n",
    "surverses = filter(row -> month(row.DATE) < 11, surverses);\n",
    "surverses[!,:RAISON] = coalesce.(surverses[:,:RAISON],\"Inconnue\");\n",
    "\n",
    "surverses = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], surverses);\n",
    "select!(surverses, [:NO_OUVRAGE, :DATE, :SURVERSE]);\n",
    "rename!(surverses, :NO_OUVRAGE => :ID_OUVRAGE);\n",
    "surverses = filter(row -> row.ID_OUVRAGE ∈ important_ouvrages, surverses);\n",
    "dropmissing!(surverses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.8.\"></a>\n",
    "#### 3.3.8. Ajout de la somme et du taux maximal de précipitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les précipitations étant séparés en heures, plutôt qu'en journées comme le reste des données, nous devons la convertir en ce dernier afin de pouvoir utiliser ces données.\n",
    "\n",
    "Plusieurs stratégies de conversion s'offrent à nous, pour chaque jour, à chaque station, nous pouvons considérer:\n",
    "- La somme des précipitations de la journée\n",
    "- Le taux maximal de précipitations de la journée\n",
    "- La moyenne de précipitations de la journée\n",
    "- Le mode de précipitations de la journée\n",
    "\n",
    "Nous avons décidé d'utiliser les deux premiers, car notre interprétation d'une surverse nous dit que celle ci est généralement causé par un débit inatendu d'eau sur une courte période plutôt qu'un flux continu. Ce choix est cependant arbitraire et constitue une amélioration possible à notre projet, comme discuté à la section [5.3. Améliorations possibles](#5.3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somme des précipitations quotidiennes par station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_sum = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>sum, \n",
    "            Bellevue = :Bellevue=>sum,\n",
    "            Assomption = :Assomption=>sum, \n",
    "            Trudeau = :Trudeau=>sum, \n",
    "            StHubert = :StHubert=>sum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taux horaire maximal de précipitations dans une journée, par station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max = by(precipitations, :date,  \n",
    "            McTavish = :McTavish=>maximum,\n",
    "            Bellevue = :Bellevue=>maximum, \n",
    "            Assomption = :Assomption=>maximum,\n",
    "            Trudeau = :Trudeau=>maximum,\n",
    "            StHubert = :StHubert=>maximum);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie a été discutée à la section [2. Analyse exploiratoire](#2.), et nous n'afficherons pas de graphes ici afin de ne pas dupliquer l'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.9.\"></a>\n",
    "#### 3.3.9. Ajout du taux maximal de précipitation sur 3 heures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partant de l'hypothèse qu'une surverse est causée par un débit inatendu d'eau dans un trop-plein, sur une durée limitée, il est naturel que le taux maximal sur 1h soit considéré. \n",
    "\n",
    "Une autre possibilité serait le taux sur 3h maximal de précipitations, offrant une plus grosse marge. Encore une fois assez arbitraire comme choix, nous avons estimé que celà pourrait s'ajouter comme variable explicative à notre modèle afin d'aider à la prédiction.\n",
    "\n",
    "Il reste que cette donnée est très corrélée avec la somme et le maximum de précipitations, il pourrait donc causer problèmes pour la régression logistique ou la classification bayésienne naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max3 = by(precipitations, :date,\n",
    "                McTavish = :McTavish=>maximum3,\n",
    "                Bellevue = :Bellevue=>maximum3,\n",
    "                Assomption = :Assomption=>maximum3,\n",
    "                Trudeau = :Trudeau=>maximum3,\n",
    "                StHubert = :StHubert=>maximum3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction maximum3 a été définie à la section [2.1. Analyse exploratoire] lors de la visualisation de cette donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.10.\"></a>\n",
    "#### 3.3.10. Conservation des données pour la station la plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, nous avons supposé que la station la plus proche aurait le plus d'influence sur les surverses d'un ouvrage. Ainsi, nous avons récupéré les localisations géographiques des stations pluviométriques à l'aide d'une recherche internet et bati un DataFrame à partir de celles ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>STATION</th><th>LAT</th><th>LNG</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 3 columns</p><tr><th>1</th><td>McTavish</td><td>45.5047</td><td>-73.5792</td></tr><tr><th>2</th><td>Bellevue</td><td>45.4272</td><td>-73.9292</td></tr><tr><th>3</th><td>Assomption</td><td>45.8094</td><td>-73.4347</td></tr><tr><th>4</th><td>Trudeau</td><td>45.4678</td><td>-73.7417</td></tr><tr><th>5</th><td>StHubert</td><td>45.5175</td><td>-73.4169</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& STATION & LAT & LNG\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & McTavish & 45.5047 & -73.5792 \\\\\n",
       "\t2 & Bellevue & 45.4272 & -73.9292 \\\\\n",
       "\t3 & Assomption & 45.8094 & -73.4347 \\\\\n",
       "\t4 & Trudeau & 45.4678 & -73.7417 \\\\\n",
       "\t5 & StHubert & 45.5175 & -73.4169 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×3 DataFrame\n",
       "│ Row │ STATION    │ LAT     │ LNG      │\n",
       "│     │ \u001b[90mString\u001b[39m     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m  │\n",
       "├─────┼────────────┼─────────┼──────────┤\n",
       "│ 1   │ McTavish   │ 45.5047 │ -73.5792 │\n",
       "│ 2   │ Bellevue   │ 45.4272 │ -73.9292 │\n",
       "│ 3   │ Assomption │ 45.8094 │ -73.4347 │\n",
       "│ 4   │ Trudeau    │ 45.4678 │ -73.7417 │\n",
       "│ 5   │ StHubert   │ 45.5175 │ -73.4169 │"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_df = DataFrame(STATION = String[], LAT = Float64[], LNG = Float64[]);\n",
    "\n",
    "push!(station_df, [\"McTavish\", 45.504742, -73.579167]);\n",
    "push!(station_df, [\"Bellevue\", 45.427222, -73.929167]);\n",
    "push!(station_df, [\"Assomption\", 45.809444, -73.434722]);\n",
    "push!(station_df, [\"Trudeau\", 45.467778, -73.741667]);\n",
    "push!(station_df, [\"StHubert\", 45.5175, -73.416944]);\n",
    "\n",
    "station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous avons comparé les longitudes et latitudes de chaque station avec l'ouvrage en cours, afin de déterminer le plus proche. La fonction suivante a été utilisé pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findDistance (generic function with 1 method)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function findDistance(oLat, oLng, sLat, sLng)\n",
    "    lat = (sLat - oLat) ^ 2;\n",
    "    lng = (sLng - oLng) ^ 2;\n",
    "\n",
    "    return sqrt(lat + lng);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ainsi déterminé la station la plus proche de l'ouvrage en cours, et ajouté à cet ouvrage les données de précipitations de la station (somme, maximum, maximum3). Les détails de l'implémentation ne sont pas ajoutés ici car ils seront couverts dans une autre sous-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche, nous nous sommes rendus compte que la station la plus proche n'était pas le seul facteur qui déterminait le taux de surverse d'un ouvrage. En raison de la nature de la pluie et de sa zone d'effet, un ouvrage peut être touché par la pluie sans que la station la plus proche de celui ci ne le soit. Il peut aussi être plus touché. Observer un autre ouvrage aiderait à obtenir cette information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.11.\"></a>\n",
    "#### 3.3.11. Conservation des données pour toutes les stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À la lumière de la conclusion obtenue à la sous-section précédente, nous avons par la suite décidé de garder les informations de chaque station (somme, maximum, maximum3), afin d'effectuer nos prédictions par ouvrage.\n",
    "\n",
    "Nos modèles n'étaient en revanche pas tout à fait en mesure de correctement déterminer quel station influençait le plus le taux de surverses d'un ouvrage donné, lorsque les informations de précipitations de plusieurs stations se ressemblaient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.12.\"></a>\n",
    "#### 3.3.12. Conservation des données pour les deux stations les plus proches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de résoudre les problèmes encontrés dans les deux sous-sections précédentes, nous avons décidé de garder en mémoire pour chaque couple d'ouvrage / date, les informations des deux ouvrages les plus proches. En effet, ce choix, bien qu'arbitraire encore une fois, est un juste milieu entre nos solutions. Il est aussi plus probable qu'un ouvrage se trouve à l'intersection de deux stations et soit touché par une zone de précipitations qu'il ne le soit de trois ou plus et que la précipitation ne touche qu'une seule de ces stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici l'implémentation de cette section. Tout d'abord, ajoute les colonnes de précipitations à nos surverses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = surverses;\n",
    "\n",
    "train_data[!, :FS_dist] = zeros(size(train_data, 1));\n",
    "train_data[!, :SS_dist] = zeros(size(train_data, 1));\n",
    "train_data[!, :FS_sum] = zeros(size(train_data, 1));\n",
    "train_data[!, :FS_max] = zeros(size(train_data, 1));\n",
    "train_data[!, :FS_max3] = zeros(size(train_data, 1));\n",
    "train_data[!, :SS_sum] = zeros(size(train_data, 1));\n",
    "train_data[!, :SS_max] = zeros(size(train_data, 1));\n",
    "train_data[!, :SS_max3] = zeros(size(train_data, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, on remplit ces colonnes avec les données des deux stations les plus proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i=1:size(train_data, 1)\n",
    "    curr_ouvrage = train_data[i, 1];\n",
    "    ouvrage_data = filter(row -> row.ID_OUVRAGE == curr_ouvrage, ouvrages);\n",
    "    \n",
    "    closest_station = nothing;\n",
    "    closest_distance = 9999;\n",
    "    \n",
    "    second_closest_station = nothing;\n",
    "    second_closest_distance = 9999;\n",
    "    \n",
    "    # Pour chaque station\n",
    "    for j=1:5\n",
    "       current_station = station_df[j, :STATION];\n",
    "       dist = findDistance(ouvrage_data[1, :TP_LAT], ouvrage_data[1, :TP_LNG], station_df[j, :LAT], station_df[j, :LNG]);\n",
    "       \n",
    "        if dist < closest_distance\n",
    "            second_closest_distance = closest_distance;\n",
    "            second_closest_station = closest_station;\n",
    "            closest_distance = dist;\n",
    "            closest_station = current_station;\n",
    "        elseif dist < second_closest_distance\n",
    "            second_closest_distance = dist;\n",
    "            second_closest_station = current_station;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    train_data[i, :FS_dist] = closest_distance;\n",
    "    train_data[i, :SS_dist] = second_closest_distance;\n",
    "    \n",
    "    # Add data for first station\n",
    "    sum_p = pcp_sum[∈([train_data[i, :DATE]]).(pcp_sum.date), Symbol(closest_station)];\n",
    "    train_data[i, :FS_sum] = sum_p[1];\n",
    "    max_p = pcp_max[∈([train_data[i, :DATE]]).(pcp_max.date), Symbol(closest_station)];\n",
    "    train_data[i, :FS_max] = max_p[1];\n",
    "    max3_p = pcp_max3[∈([train_data[i, :DATE]]).(pcp_max3.date), Symbol(closest_station)];\n",
    "    train_data[i, :FS_max3] = max3_p[1];\n",
    "    \n",
    "    # Add data for second station\n",
    "    s_sum_p = pcp_sum[∈([train_data[i, :DATE]]).(pcp_sum.date), Symbol(second_closest_station)];\n",
    "    train_data[i, :SS_sum] = s_sum_p[1];\n",
    "    s_max_p = pcp_max[∈([train_data[i, :DATE]]).(pcp_max.date), Symbol(second_closest_station)];\n",
    "    train_data[i, :SS_max] = s_max_p[1];\n",
    "    s_max3_p = pcp_max3[∈([train_data[i, :DATE]]).(pcp_max3.date), Symbol(second_closest_station)];\n",
    "    train_data[i, :SS_max3] = s_max3_p[1];\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons ainsi les données de précipitations des deux stations les plus proches, par ouvrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>ID_OUVRAGE</th><th>DATE</th><th>SURVERSE</th><th>FS_dist</th><th>SS_dist</th><th>FS_sum</th><th>FS_max</th><th>FS_max3</th></tr><tr><th></th><th>String</th><th>Date</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 11 columns (omitted printing of 3 columns)</p><tr><th>1</th><td>4240-01D</td><td>2017-08-24</td><td>0</td><td>0.149987</td><td>0.168295</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>2</th><td>4240-01D</td><td>2014-06-15</td><td>0</td><td>0.149987</td><td>0.168295</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>3</th><td>4240-01D</td><td>2013-09-16</td><td>0</td><td>0.149987</td><td>0.168295</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>4</th><td>3350-07D</td><td>2017-05-29</td><td>1</td><td>0.0927373</td><td>0.12027</td><td>64.0</td><td>26.0</td><td>53.0</td></tr><tr><th>5</th><td>3350-07D</td><td>2018-06-26</td><td>0</td><td>0.0927373</td><td>0.12027</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& ID\\_OUVRAGE & DATE & SURVERSE & FS\\_dist & SS\\_dist & FS\\_sum & FS\\_max & FS\\_max3 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Date & Int64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 4240-01D & 2017-08-24 & 0 & 0.149987 & 0.168295 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t2 & 4240-01D & 2014-06-15 & 0 & 0.149987 & 0.168295 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t3 & 4240-01D & 2013-09-16 & 0 & 0.149987 & 0.168295 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t4 & 3350-07D & 2017-05-29 & 1 & 0.0927373 & 0.12027 & 64.0 & 26.0 & 53.0 & $\\dots$ \\\\\n",
       "\t5 & 3350-07D & 2018-06-26 & 0 & 0.0927373 & 0.12027 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×11 DataFrame. Omitted printing of 5 columns\n",
       "│ Row │ ID_OUVRAGE │ DATE       │ SURVERSE │ FS_dist   │ SS_dist  │ FS_sum  │\n",
       "│     │ \u001b[90mString\u001b[39m     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼────────────┼────────────┼──────────┼───────────┼──────────┼─────────┤\n",
       "│ 1   │ 4240-01D   │ 2017-08-24 │ 0        │ 0.149987  │ 0.168295 │ 0.0     │\n",
       "│ 2   │ 4240-01D   │ 2014-06-15 │ 0        │ 0.149987  │ 0.168295 │ 0.0     │\n",
       "│ 3   │ 4240-01D   │ 2013-09-16 │ 0        │ 0.149987  │ 0.168295 │ 0.0     │\n",
       "│ 4   │ 3350-07D   │ 2017-05-29 │ 1        │ 0.0927373 │ 0.12027  │ 64.0    │\n",
       "│ 5   │ 3350-07D   │ 2018-06-26 │ 0        │ 0.0927373 │ 0.12027  │ 0.0     │"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_first(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes _dist_ correspondent à la distance entre l'ouvrage et la station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.13.\"></a>\n",
    "#### 3.3.13. Mise à l'échelle des données de la seconde station la plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, une autre considération a été de mettre à l'échelle la contribution de la deuxième station la plus proche d'un ouvrage par rapport à la première. \n",
    "\n",
    "Plusieurs approches ont été utilisées avant d'arriver à l'approche qui nous parraissait la plus approprié: Nous avons utilisé une fonction d'adoucicement afin de ne pas trop pénaliser la seconde station. \n",
    "\n",
    "Voici notre raisonnement: \n",
    "- Si une station est deux fois plus loin de l'ouvrage qu'une autre, sa contribution devrait tourner autour de 80% de la première station, plutot que juste 50%. \n",
    "- Si une station est environ aussi loin de l'ouvrage qu'une autre, sa contribution devrait frôler les 100%.\n",
    "- Si une station est beaucoup plus loin, sa contribution devrait diminuer en conséquence.\n",
    "\n",
    "Nous avons ainsi généré les distances des deux stations les plus proches pour les 5 ouvrages d'intérets, et nous avons manuellement généré une fonction qui nous semble appropriée. Encore une fois, comme cette fonction a été générée manuellement selon nos observations et intuitions, elle constitue une potentielle faiblesse de notre modèle à améliorer, comme discutée à la section [5.3. Améliorations possibles](#5.3.).\n",
    "\n",
    "La fonction en question calcule le ratio de distance de la deuxième station par rapport à la première, puis trouve la racine carrée de ce ratio et le passe en argument à un logarithm naturel, ce résultat est ensuite soustrait à un afin d'obtenir la contribution de la seconde station.\n",
    "\n",
    "Contribution = $ 1 - \\ln(\\sqrt(\\frac{seconde closest distance}{closest distance})) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici l'implémentation de la fonction en question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contribution_second (generic function with 1 method)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function contribution_second(closest_distance, second_closest_distance)\n",
    "    ratio = second_closest_distance / closest_distance;\n",
    "    logratio = log(sqrt(ratio));\n",
    "    return 1 - logratio; \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est finalement appliquée aux données de précipitations de la deuxième station la plus proche avant d'être ajoutée aux données de l'ouvrage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3.14.\"></a>\n",
    "#### 3.3.14. OneHot des dates pour les mois et jours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, un autre traitement général des données a été de considérer les jours et les mois de l'année comme variable explicative de précipitation.\n",
    "\n",
    "Nous avons donc encodé les mois entre 5 et 10 et les jours de 1 à 31 et passé cette information à notre modèle.\n",
    "\n",
    "Ces données ont aussi dûes être standardisées afin de ne pas causer de problèmes d'échelle pour la régression logistique.\n",
    "\n",
    "Par la suite, nous avons retiré cette variable explicative de notre modèle en raison de son faible pouvoir prédictif par rapport aux autres données. Nous ne détaillerons donc pas ici l'implémentation de ces variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4.\"></a>\n",
    "### 3.4. Isolation des ouvrages d'intérêts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4.1.\"></a>\n",
    "#### 3.4.1. Génération des nouveaux fichiers de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu à la section [2.7. Isolation des ouvrages d'intérêts](#2.7.), nous avons décidé de séparer notre jeu de données final en 5 jeux de données, soit un par ouvrage d'intérêt. Le code suivant montre comment nous isolons un ouvrage dans le jeu de donnée obtenu précédemment, et enregistrons le résultat dans un nouveau fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrage_3260 = filter(row -> row.ID_OUVRAGE == \"3260-01D\", train_data);\n",
    "select!(ouvrage_3260, Not(:ID_OUVRAGE));\n",
    "CSV.write(\"data/parsed/tmp-ouvrage.csv\",ouvrage_3260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce processus est ainsi répété pour chacun des quatres autres ouvrages d'intérets. Nous n'enregistrons que les données de précipitations ainsi que la date pour ces ouvrages qui seront utilisées pour la prochaine étape de traitement de données puis de génération de modèles. Les jeux de données finaux générés par cette étape sont présents dans le dossier _data/parsed/_ et inclus dans ce rapport.\n",
    "\n",
    "Ce traitement, ainsi que l'ensemble des traitements employés plus hauts, est bien évidemment appliqué par la suite aux données de tests, afin que ceux ci soient consistents avec les données en entrées aux modèles. Ainsi, notre jeu de test constitué de 283 lignes sera séparé en 5 jeux de test différents, un par ouvrage. Chacun de ces jeux sera donné en entrée au modèle entrainé spécifiquement pour l'ouvrage en question, et les resultats seront assemblés par la suite afin de générer nos prédictions finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4.2.\"></a>\n",
    "#### 3.4.2. Sur-échantillonage de l'ensemble des colonnes d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, nous nous sommes rendu compte que les nouveaux jeux de données étaient très petits en termes d'entrées. En effet, chacun de nos données d'ouvrages d'intérêts ne contenait que 700 à 1100 données comme on peut l'observer ci dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouvrage_4350 = CSV.read(\"data/parsed/ouvrage_4350.csv\");\n",
    "size(ouvrage_4350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela constitue un jeu de données très limité pour entrainer un classificateur en apprentissage machine. Un autre soucis qui a été soulevé à ce niveau est la proportion de données de surverses dans ces données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Nothing</th><th>DataType</th></tr></thead><tbody><p>10 rows × 8 columns</p><tr><th>1</th><td>DATE</td><td></td><td>2013-05-01</td><td></td><td>2018-10-31</td><td>1100</td><td></td><td>Date</td></tr><tr><th>2</th><td>SURVERSE</td><td>0.0381818</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>3</th><td>FS_dist</td><td>0.0248354</td><td>0.0248354</td><td>0.0248354</td><td>0.0248354</td><td></td><td></td><td>Float64</td></tr><tr><th>4</th><td>SS_dist</td><td>0.139269</td><td>0.139269</td><td>0.139269</td><td>0.139269</td><td></td><td></td><td>Float64</td></tr><tr><th>5</th><td>FS_sum</td><td>-0.00633163</td><td>-0.396293</td><td>-0.396293</td><td>9.43361</td><td></td><td></td><td>Float64</td></tr><tr><th>6</th><td>FS_max</td><td>-3.24033e-5</td><td>-0.415792</td><td>-0.415792</td><td>7.80956</td><td></td><td></td><td>Float64</td></tr><tr><th>7</th><td>FS_max3</td><td>-0.00776021</td><td>-0.417149</td><td>-0.417149</td><td>11.6341</td><td></td><td></td><td>Float64</td></tr><tr><th>8</th><td>SS_sum</td><td>-0.00346406</td><td>-0.0493965</td><td>-0.0493965</td><td>1.1989</td><td></td><td></td><td>Float64</td></tr><tr><th>9</th><td>SS_max</td><td>-0.00298854</td><td>-0.0517744</td><td>-0.0517744</td><td>1.07857</td><td></td><td></td><td>Float64</td></tr><tr><th>10</th><td>SS_max3</td><td>-0.00404947</td><td>-0.0514987</td><td>-0.0514987</td><td>1.39248</td><td></td><td></td><td>Float64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Nothing & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & DATE &  & 2013-05-01 &  & 2018-10-31 & 1100 &  & Date \\\\\n",
       "\t2 & SURVERSE & 0.0381818 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t3 & FS\\_dist & 0.0248354 & 0.0248354 & 0.0248354 & 0.0248354 &  &  & Float64 \\\\\n",
       "\t4 & SS\\_dist & 0.139269 & 0.139269 & 0.139269 & 0.139269 &  &  & Float64 \\\\\n",
       "\t5 & FS\\_sum & -0.00633163 & -0.396293 & -0.396293 & 9.43361 &  &  & Float64 \\\\\n",
       "\t6 & FS\\_max & -3.24033e-5 & -0.415792 & -0.415792 & 7.80956 &  &  & Float64 \\\\\n",
       "\t7 & FS\\_max3 & -0.00776021 & -0.417149 & -0.417149 & 11.6341 &  &  & Float64 \\\\\n",
       "\t8 & SS\\_sum & -0.00346406 & -0.0493965 & -0.0493965 & 1.1989 &  &  & Float64 \\\\\n",
       "\t9 & SS\\_max & -0.00298854 & -0.0517744 & -0.0517744 & 1.07857 &  &  & Float64 \\\\\n",
       "\t10 & SS\\_max3 & -0.00404947 & -0.0514987 & -0.0514987 & 1.39248 &  &  & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×8 DataFrame. Omitted printing of 3 columns\n",
       "│ Row │ variable │ mean        │ min        │ median     │ max        │\n",
       "│     │ \u001b[90mSymbol\u001b[39m   │ \u001b[90mUnion…\u001b[39m      │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m     │ \u001b[90mAny\u001b[39m        │\n",
       "├─────┼──────────┼─────────────┼────────────┼────────────┼────────────┤\n",
       "│ 1   │ DATE     │             │ 2013-05-01 │            │ 2018-10-31 │\n",
       "│ 2   │ SURVERSE │ 0.0381818   │ 0          │ 0.0        │ 1          │\n",
       "│ 3   │ FS_dist  │ 0.0248354   │ 0.0248354  │ 0.0248354  │ 0.0248354  │\n",
       "│ 4   │ SS_dist  │ 0.139269    │ 0.139269   │ 0.139269   │ 0.139269   │\n",
       "│ 5   │ FS_sum   │ -0.00633163 │ -0.396293  │ -0.396293  │ 9.43361    │\n",
       "│ 6   │ FS_max   │ -3.24033e-5 │ -0.415792  │ -0.415792  │ 7.80956    │\n",
       "│ 7   │ FS_max3  │ -0.00776021 │ -0.417149  │ -0.417149  │ 11.6341    │\n",
       "│ 8   │ SS_sum   │ -0.00346406 │ -0.0493965 │ -0.0493965 │ 1.1989     │\n",
       "│ 9   │ SS_max   │ -0.00298854 │ -0.0517744 │ -0.0517744 │ 1.07857    │\n",
       "│ 10  │ SS_max3  │ -0.00404947 │ -0.0514987 │ -0.0514987 │ 1.39248    │"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(ouvrage_4350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, seulement 3% des données sont des surverses! Cela est très problématique si l'on cherche à prédire le taux de surverse, nous avons ici des classes très imbalancées!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'égaliser le nombre de surverses dans les données, nous allons sur-échantillonner nos informations en générant aléatoirement des données jusqu'à obtenir un jeu de 10000 données, multipliant par un facteur de 10 le nombre de données de nos modèles. \n",
    "\n",
    "Voici la fonction permettant de générer une nouvelle valeur en y ajouter une variation de $ \\pm$ 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_variation (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_variation(val)\n",
    "    var = rand() * 0.4;\n",
    "    return val * (0.8 + var);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivant génère une nouvelle donnée et l'ajoute au jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_new_entry (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_new_entry(df, entry)\n",
    "    nwDATE = entry.DATE;\n",
    "    nwSURVERSE = entry.SURVERSE;\n",
    "    nwFS_dist = entry.FS_dist;\n",
    "    nwSS_dist = entry.SS_dist;\n",
    "    nwFS_sum = generate_variation(entry.FS_sum);\n",
    "    nwFS_max = generate_variation(entry.FS_max);\n",
    "    nwFS_max3 = generate_variation(entry.FS_max3);\n",
    "    nwSS_sum = generate_variation(entry.SS_sum);\n",
    "    nwSS_max = generate_variation(entry.SS_max);\n",
    "    nwSS_max3 = generate_variation(entry.SS_max3);\n",
    "    \n",
    "    push!(df, [nwDATE, nwSURVERSE, nwFS_dist, nwSS_dist, \n",
    "               nwFS_sum, nwFS_max, nwFS_max3, \n",
    "               nwSS_sum, nwSS_max, nwSS_max3])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons d'abord la répartition des surverses avant d'ajouter nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ouvrage_4350, x=:FS_sum, y=:SS_sum, color=:SURVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, il y'a très peu d'informations de surverses sur ce graphique, des modèles comme la régression linéaire sont très sensibles à un débalancement de classe et pourrait ne pas suivre les données correctement.\n",
    "\n",
    "Ajoutons maintenant les nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_random_sample = 10000 - size(ouvrage_4350, 1);\n",
    "new_entries = similar(ouvrage_4350, nrow(ouvrage_4350))\n",
    "\n",
    "for (i, row) in enumerate(eachrow(ouvrage_4350))\n",
    "    new_entries[i, :] = row[:]\n",
    "end\n",
    "\n",
    "for i=1:n_to_random_sample\n",
    "    should_surverse = rand() > 0.45 ? 1 : 0;\n",
    "    cols = filter(row -> row.SURVERSE == should_surverse, ouvrage_4350);\n",
    "    idx_row = convert(Int64, trunc(rand() * size(cols, 1))) + 1;\n",
    "    \n",
    "    entry = cols[idx_row, :];\n",
    "    generate_new_entry(new_entries, entry);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme permet d'ajouter des données à nos informations de surverses. Le facteur should_surverse est reglé ici à 0.45 qui controlle aléatoirement le nombre de données contenant des surverses à être ajouté. 0.45 correspond donc ici à 55% des 10000 - 1100) nouvelles données contenant des surverses, balançant grandement le nombre de surverses dans le jeu final.\n",
    "\n",
    "Une colonne aléatoire est alors prise dans ouvrage_4350, selon s'il y'a surverse ou pas, et est modifiée de $ \\pm$ 20% afin de diversifier le tout. Cette nouvelle entrée est alors ajoutée à notre jeu final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Nothing</th><th>DataType</th></tr></thead><tbody><p>10 rows × 8 columns</p><tr><th>1</th><td>DATE</td><td></td><td>2013-05-01</td><td></td><td>2018-10-31</td><td>1100</td><td></td><td>Date</td></tr><tr><th>2</th><td>SURVERSE</td><td>0.4987</td><td>0</td><td>0.0</td><td>1</td><td></td><td></td><td>Int64</td></tr><tr><th>3</th><td>FS_dist</td><td>0.0248354</td><td>0.0248354</td><td>0.0248354</td><td>0.0248354</td><td></td><td></td><td>Float64</td></tr><tr><th>4</th><td>SS_dist</td><td>0.139269</td><td>0.139269</td><td>0.139269</td><td>0.139269</td><td></td><td></td><td>Float64</td></tr><tr><th>5</th><td>FS_sum</td><td>1.51042</td><td>-0.475487</td><td>0.678896</td><td>11.2203</td><td></td><td></td><td>Float64</td></tr><tr><th>6</th><td>FS_max</td><td>1.39276</td><td>-0.498947</td><td>0.606317</td><td>9.2999</td><td></td><td></td><td>Float64</td></tr><tr><th>7</th><td>FS_max3</td><td>1.4458</td><td>-0.500544</td><td>0.720914</td><td>13.9477</td><td></td><td></td><td>Float64</td></tr><tr><th>8</th><td>SS_sum</td><td>0.162925</td><td>-0.0592641</td><td>-0.0348566</td><td>1.43759</td><td></td><td></td><td>Float64</td></tr><tr><th>9</th><td>SS_max</td><td>0.139991</td><td>-0.0621283</td><td>-0.0273081</td><td>1.26982</td><td></td><td></td><td>Float64</td></tr><tr><th>10</th><td>SS_max3</td><td>0.150957</td><td>-0.0617911</td><td>-0.0316092</td><td>1.66247</td><td></td><td></td><td>Float64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Nothing & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & DATE &  & 2013-05-01 &  & 2018-10-31 & 1100 &  & Date \\\\\n",
       "\t2 & SURVERSE & 0.4987 & 0 & 0.0 & 1 &  &  & Int64 \\\\\n",
       "\t3 & FS\\_dist & 0.0248354 & 0.0248354 & 0.0248354 & 0.0248354 &  &  & Float64 \\\\\n",
       "\t4 & SS\\_dist & 0.139269 & 0.139269 & 0.139269 & 0.139269 &  &  & Float64 \\\\\n",
       "\t5 & FS\\_sum & 1.51042 & -0.475487 & 0.678896 & 11.2203 &  &  & Float64 \\\\\n",
       "\t6 & FS\\_max & 1.39276 & -0.498947 & 0.606317 & 9.2999 &  &  & Float64 \\\\\n",
       "\t7 & FS\\_max3 & 1.4458 & -0.500544 & 0.720914 & 13.9477 &  &  & Float64 \\\\\n",
       "\t8 & SS\\_sum & 0.162925 & -0.0592641 & -0.0348566 & 1.43759 &  &  & Float64 \\\\\n",
       "\t9 & SS\\_max & 0.139991 & -0.0621283 & -0.0273081 & 1.26982 &  &  & Float64 \\\\\n",
       "\t10 & SS\\_max3 & 0.150957 & -0.0617911 & -0.0316092 & 1.66247 &  &  & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×8 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ variable │ mean      │ min        │ median     │ max        │ nunique │\n",
       "│     │ \u001b[90mSymbol\u001b[39m   │ \u001b[90mUnion…\u001b[39m    │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m     │ \u001b[90mAny\u001b[39m        │ \u001b[90mUnion…\u001b[39m  │\n",
       "├─────┼──────────┼───────────┼────────────┼────────────┼────────────┼─────────┤\n",
       "│ 1   │ DATE     │           │ 2013-05-01 │            │ 2018-10-31 │ 1100    │\n",
       "│ 2   │ SURVERSE │ 0.4987    │ 0          │ 0.0        │ 1          │         │\n",
       "│ 3   │ FS_dist  │ 0.0248354 │ 0.0248354  │ 0.0248354  │ 0.0248354  │         │\n",
       "│ 4   │ SS_dist  │ 0.139269  │ 0.139269   │ 0.139269   │ 0.139269   │         │\n",
       "│ 5   │ FS_sum   │ 1.51042   │ -0.475487  │ 0.678896   │ 11.2203    │         │\n",
       "│ 6   │ FS_max   │ 1.39276   │ -0.498947  │ 0.606317   │ 9.2999     │         │\n",
       "│ 7   │ FS_max3  │ 1.4458    │ -0.500544  │ 0.720914   │ 13.9477    │         │\n",
       "│ 8   │ SS_sum   │ 0.162925  │ -0.0592641 │ -0.0348566 │ 1.43759    │         │\n",
       "│ 9   │ SS_max   │ 0.139991  │ -0.0621283 │ -0.0273081 │ 1.26982    │         │\n",
       "│ 10  │ SS_max3  │ 0.150957  │ -0.0617911 │ -0.0316092 │ 1.66247    │         │"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(new_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons l'observer, 50% des données sont donc des surverses, éliminant ainsi l'imbalancement des classes. Le nombre de nouvelles données, le taux de surverse final ainsi que le taux de variation des données proviennent tous de choix arbitraires qui nous semblent mener aux meilleurs résultats après plusieurs tests, ils constituent toutefois une considération pour la section [5.3. Amélioration possibles](#5.3.).\n",
    "\n",
    "Observons maintenant le nouveau graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(new_entries, x=:FS_sum, y=:SS_sum, color=:SURVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de surverses au milieu jouent maintenant un rôle plus important lors de la création du modèle et permettent une meilleure stabilité à des modèles qui auraient autrement des problèmes de consistence avec le peu de données que nous avions précédemment. Enregistrons finalement ces nouvelles données dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"data/parsed/oversampled/tmp_4350.csv\", new_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5.\"></a>\n",
    "### 3.5. Retour sur le traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour conclure, nous avons ici filtré, rafiné, groupé puis finalement augmenté beaucoup de données ici. Cette étape essentielle à ce projet a su apporter des améliorations aux performances de nos modèles tout au long du projet, jusqu'au dernier jour. Nous comprenons mieux le sens de l'expression: \"L'apprentissage machine est principalement du traitement de données\". Il ne serait pas éronné de dire que cette étape a demandé une des plus grosses sources de temps pour ce travail. Nous sommes toutefois satisfaits du résultat et pensons que cela a beaucoup aidé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.\"></a>\n",
    "## 4. Sélection de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1.\"></a>\n",
    "### 4.1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que les données sont prêtes à être utilisées, il est temps de passer à l'étape de sélection de modèles. En effet, le choix du ou des modèles est crucial dans le domaine de la prédiction car, selon les données reçues, certains modèles vont mener à des meilleures prédictions. Il est donc primordial de bien déterminer le type de modèle qui serait adapté à notre situation afin d'obtenir les meilleures performances et résultats possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2.\"></a>\n",
    "### 4.2. Choix des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles que nous avons considéré pour cet exercice sont:\n",
    "\n",
    "- Les arbres de décision\n",
    "- La forêt aléatoire\n",
    "- La machine à vecteur de support (SVM)\n",
    "- La régression logistique\n",
    "- La classification bayésienne naive\n",
    "- L'ensemble de modèles\n",
    "\n",
    "Chacun de ces modèles sera présenté dans une sous-section de la section [4. Sélection de modèles](#4.), où les avantages et désavantages du modèle seront présentés. Pour les modèles aillant été retenus, nous allons détailler les notions théoriques sur lesquels reposent ces modèles et comment nous les avons appliqué. Pour les modèles non retenus, nous expliqueront pourquoi ils n'ont pas été retenu en faveur des autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.\"></a>\n",
    "### 4.3. Arbres de décision et forêt aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.1.\"></a>\n",
    "#### 4.3.1. Théorie des arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle d'arbres de décision est un algorithme d'apprentissage machine très puissant capable d'effectuer des tâches de classification, mais aussi de régression. Dans notre cas, c'est la partie classification qui nous intéresse ici car nous cherchons à déterminer si une surverse a eu lieu ou pas, selon certaines variables explicatives.\n",
    "\n",
    "Un arbre de décision est modélisé en prenant d'abord les variables explicatives et en calculant les indices de diversité de Gini de chacune. Ce dernier représente la fréquence que l'issu d'un nouvel élément de l'ensemble soit mal classé si la classification des variables explicatives se fait de manière aléatoire. En effet, on calcule l'indice de diversité de Gini de chaque variable explicative et on remplit l'arbre graduellement à partir de celle qui donne la meilleure valeur (les valeurs se situent entre 0 et 1, 0 étant la meilleure) vers velle qui donne la pire.\n",
    "\n",
    "Les avantages des arbres de décision sont:\n",
    "- Facile d'identifier des variables explicatives significatives, modèle à boite ouverte\n",
    "- N'est pas sensible aux échelles, pas besoin de standardiser les données\n",
    "- N'est pas sensible à la multi-colinéarité, pas besoin de la réduire avec du PCA ou autre\n",
    "\n",
    "Les désavantages des arbres de décision sont:\n",
    "- Facilement capable d'adhérer trop étroitement les données d'entrainements\n",
    "\n",
    "Le modèle d'arbres de décision est très facile à implémenter, des manipulations mathématiques ne sont pas nécessaire afin de réaliser la modélisation. Les données n'ont pas à être standardizé comme c'était le cas pour des modèles comme la régression logistique. De plus, ce modèle est connu pour être efficace en terme d'identification des variables explicatives qui sont significatives et pour repérer les relations entre plusieurs variables explicatives. En effet, la structure de l'arbre créée dépend des indices de diversité de Gini de chacune donc le lien entre les variables explicatives y est encapsulé. Cependant, comme tout autre modèle, les arbres de décision ont également des désavantages. Le principal inconvénient d'utiliser ces derniers est qu'il ne permet pas de donner des prédictions précises. C'est la raison pour laquelle il est rare qu'on utilise un arbre de décision directement sur un ensemble de données; on fait recours à un modèle qui se base sur celui-ci, comme c'est le cas pour le modèle de forêt aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.2.\"></a>\n",
    "#### 4.3.2. Théorie des forêts aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle des forêts aléatoires est un algorithme d'apprentissage machine qui repose sur la théorie des arbres de décision détaillée à la sous-section précédente. En effet, il consiste à créer une *forêt*, une multitude d'arbres de décision générés de manière aléatoire, et de les mélanger pour effectuer une prédiction. On emploie le mot aléatoire, car les arbres de décision sont crées en prenant des variables explicatives de manière aléatoire avec un ensemble de données qui sont aléatoires également. Pour remédier au fait que certaines données ne seraient pas représentées par le modèle, on utilise souvent l'ensemble de données qui n'a pas été utilisé lors de la modélisation afin d'évaluer sa précision. La proportion de données qui ont été mal classées (la prédiction n'est pas bonne) est appelée *Out-of-bag Error* en anglais. \n",
    "\n",
    "Les avantages des forêts aléatoires sont:\n",
    "- Une bonne précision de prédiction\n",
    "- Moins suspectible d'*overfit* les données\n",
    "\n",
    "Les désavantages des forêts aléatoires sont:\n",
    "- Une plus grande puissance computationnelle est exigée\n",
    "- Une forêt est plus complexe à interpréter qu'un seul arbre, modèle à boite fermée\n",
    "\n",
    "Le modèle des forêts aléatoires vient résoudre le problème de manque de précision qui était un désavantage pour un arbre de décision. En utilisant les nombreux arbres qui ont été créés et les valeurs de *Out-of-bag Error*, il est possible d'augmenter la précision de prédiction. Et comme il se base sur le modèle des arbres de décision, il conserve l'efficacité lorsqu'on veut identifier les variables explicatives les plus significatives pour un ensemble de données ainsi que les relations existant entre plusieurs variables explicatives. Cependant, le fait que l'entrainement de ce modèle exige une puissance computationnelle élevée dûe aux calculs à réaliser pour chacun des arbres peut être un grand désavantage. Ceci implique également un coût élevé en temps pour l'entrainement. Dernièrement, le fait que plusieurs arbres sont utilisés pour la modélisation rend l'interprétation des résultats relativement plus complexes que si on n'avait qu'un seul arbre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.3.\"></a>\n",
    "#### 4.3.3. Définition des fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions utilisées pour les forêts aléatoires sont définis dans le fichier _random-forest.jl_, inclus dans ce rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"utils/random-forest.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous expliquerons ici les fonctions définies, sans montrer le code afin de ne pas polluer le rapport. Ce code est cependant présent dans le fichier, pour tout lecteur intéressé à l'implémentation spécifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find_best_rf(train_set, validation_set, features, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction prend en entrée le jeu de données d'entrainement, le jeu de données de validation ainsi que les variables d'intérêt. De plus, il prend en paramètre un DataFrame _params_ qui constituent les valeurs possibles pour l'attunement de la forêt. En effet, comme expliqué plus haut, une forêt aléatoire possède plusieurs paramètres en plus des informations d'entrées. Cette variable est donc de la forme suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>name</th><th>min</th><th>max</th><th>step</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>4 rows × 4 columns</p><tr><th>1</th><td>nft</td><td>3</td><td>7</td><td>1</td></tr><tr><th>2</th><td>ntrees</td><td>3</td><td>7</td><td>1</td></tr><tr><th>3</th><td>podata</td><td>75</td><td>90</td><td>5</td></tr><tr><th>4</th><td>maxd</td><td>15</td><td>25</td><td>1</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& name & min & max & step\\\\\n",
       "\t\\hline\n",
       "\t& String & Int64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & nft & 3 & 7 & 1 \\\\\n",
       "\t2 & ntrees & 3 & 7 & 1 \\\\\n",
       "\t3 & podata & 75 & 90 & 5 \\\\\n",
       "\t4 & maxd & 15 & 25 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4×4 DataFrame\n",
       "│ Row │ name   │ min   │ max   │ step  │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │\n",
       "├─────┼────────┼───────┼───────┼───────┤\n",
       "│ 1   │ nft    │ 3     │ 7     │ 1     │\n",
       "│ 2   │ ntrees │ 3     │ 7     │ 1     │\n",
       "│ 3   │ podata │ 75    │ 90    │ 5     │\n",
       "│ 4   │ maxd   │ 15    │ 25    │ 1     │"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = DataFrame(name=String[], min=Int64[], max=Int64[], step=Int64[]);\n",
    "\n",
    "push!(params, [\"nft\", 3, 7, 1]);\n",
    "push!(params, [\"ntrees\", 3, 7, 1]);\n",
    "push!(params, [\"podata\", 75, 90, 5]);\n",
    "push!(params, [\"maxd\", 15, 25, 1]);\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, _nft_ correspond au nombre de variables utilisées par arbre. Une recherche en ligne nous suggère d'utiliser $ \\log_2(N+1)$ variables explicatives, où $N$ est le nombre total de variables explicatives.<sup>[[4]](#6.)</sup>. _ntrees_ est le nombre d'arbres qui composent notre forêt, ajouter des arbres ne fait généralement qu'augmenter la précision jusqu'à un certain seuil, mais ralenti beaucoup l'entrainement. _odata_ est le pourcentage de données d'entrainement donné à chaque arbre, et _maxd_ est la profondeur maximale d'un arbre, afin de limiter le surajustement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction va donc itérer sur les valeurs possibles de chaque paramètres, entrainer une forêt aléatoire et calculer le f1_score de chaque foret. La fonction retourne alors les paramètres qui mènent au meilleur score.\n",
    "\n",
    "En raison du nombre de combinaisons possibles de paramètres, et en faisant plusieurs itérations pour chaque combinaison afin d'obtenir un f1_score représentatif, cette fonction demande un énorme temps de calcul, elle est donc très rarement utilisée dans notre projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get_rf_probas(train_set, validation_set, features, params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va entrainer une forêt aléatoire avec le jeu de donnée en entrées, donc les variables d'intérêts se trouve dans la liste _features_ et les paramètres d'ajustement de la forêt dans _params_.\n",
    "\n",
    "Elle va ensuite nous retourner les probabilités de chaque classe [0, 1] pour chacune des entrées à prédire. Ces probabilités serviront à être combinées à d'autres probabilités dans l'ensemble de modèles, vu plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get_rf_direct(train_set, validation_set, features, params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est similaire à la fonction du dessus, mais ne retourne pas les probabilités de chaque classe. Elle génère plutot des valeurs prédites directement et calcule le f1_score du modèle, qui est retourné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.4.\"></a>\n",
    "#### 4.3.4. Application à l'ensemble de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les détails d'implémentations seront détaillées pour la partie [4.3.5. Application aux ouvrages isolés](#4.3.5.). Nous allons ici couvrir les étapes qui nous ont permis d'atteindre le score actuel, sans trop détailler l'implémentation qui n'est plus d'actualité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors des premières itérations du projet, nous avions commencé par génèrer un arbre de décision à partir de l'ensemble des données. Les résultats obtenus étaient médiocres, mais toutefois plus hauts qu'une prédiction aléatoire (Nous avions environ 0.42 de F1_score lors de nos premières prédictions). \n",
    "\n",
    "Par la suite, nous avions transformé notre arbre de décision en forêt aléatoire et en utilisant à ce moment plus de variables. Les variables explicatives à ce moment étaient la hauteur et position du trop-plein, ainsi que la date. Nous arrivions aux mêmes résultats et au même score, ce qui n'est pas surprenant compte tenu du fait que nous n'utilisions pas les données de précipitations à ce moment là.\n",
    "\n",
    "Nous sommes alors passé par plusieurs itérations avant d'atteindre un premier palier à 0.72 de f1_score avec les forêts aléatoires. Les variables utilisées à ce moment étaient toutes celles du haut, ainsi que la somme des précipitations et le taux maximal de précipitations de la station la plus proche. Nos résultats étaient à ce moment légèrement meilleurs sans normalisation de données qu'avec, mais cela pourrait être dû à de la chance ou aux paramètres modifiés entre-temps pour le modèle. Nous avions aussi essayé de mettre le taux de précipitations à l'échelle selon la distance entre l'ouvrage et la station, sans grand résultats. \n",
    "\n",
    "Peu de tests ont été effectué à cette étape avec les valeurs que nous avions, car la majorité de nos efforts se concentrait à améliorer nos données plutot que nos modèles. Le meilleur résultat obtenu à ce stage était un score de 0.76, avec toutes les données de surverses pour la station la plus proche ainsi que les données de trop-plein et de date (mois, jours). \n",
    "\n",
    "La majorité des soumissions basées sur les arbres de décisions et les forêts aléatoires utilisaient à ce moment là toutes les données, avec des variations sur les variables explicatives et les paramètres d'ajustements de la forêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3.5.\"></a>\n",
    "#### 4.3.5. Application aux ouvrages isolés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, nous avons décidé d'isoler chaque ouvrage d'intérêt et d'entrainer un modèle par ouvrage. La premiere prédiction n'a pas donné de résultats très satisfaisants, avec un f1_score de 0.65. Les prédictions subséquentes après augmentation des données et suréchantillonnages sont beaucoups plus intéressantes. Voici les détails d'implémentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les librairies ainsi que nos fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, Random, MLBase, DecisionTree;\n",
    "include(\"utils/random-forest.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, voici une fonction qui sera utilisée par tous les modèles afin de séparer nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitionTrainTest (generic function with 2 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function partitionTrainTest(data, at = 0.8) # https://discourse.julialang.org/t/simple-tool-for-train-test-split/473/2\n",
    "    n = nrow(data)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    return data[train_idx,:], data[test_idx,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les données de l'ouvrage 3260."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_3260 = CSV.read(\"data/parsed/oversampled/ouvrage_3260.csv\");\n",
    "train_set_3260, val_set_3260 = partitionTrainTest(data_set_3260);\n",
    "val_labels_3260 = val_set_3260[!, :SURVERSE];\n",
    "size(data_set_3260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les variables explicatives d'intérêts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ft = [:FS_sum, :FS_max, :FS_max3, :SS_sum, :SS_max, :SS_max3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainons un arbre de décision pour avoir une idée de la profondeur de l'arbre avec ces données. Nous utilisons la librairie DecisionTree.jl pour les modèles d'arbres de décision et de forêt aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decision Tree\n",
       "Leaves: 244\n",
       "Depth:  27"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_3260 = convert(Matrix{Float64}, train_set_3260[:, names_ft]);\n",
    "train_labels_3260 = convert(Array{Int64}, train_set_3260[!,:SURVERSE]);\n",
    "\n",
    "dt_model_3260 = build_tree(train_labels_3260, train_features_3260);\n",
    "dt_model_3260 = prune_tree(dt_model_3260, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'arbre obtenu a une profondeur d'environ 30, cette donnée va être utile lorsque nous ajusterons la forêt aléatoire. Il faut savoir que cette donnée ne provient que de 80% des données et n'est donc pas forcément idéale pour la forêt finale.\n",
    "\n",
    "Voyons le f1_score de modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589718240237272"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features_3260 = convert(Matrix{Float64}, val_set_3260[:, names_ft]);\n",
    "val_labels_3260 = convert(Array{Int64}, val_set_3260[!,:SURVERSE]);\n",
    "\n",
    "val_pred_dt_3260 = apply_tree(dt_model_3260, val_features_3260);\n",
    "r = roc(val_labels_3260, val_pred_dt_3260);\n",
    "f1score(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient un excellent f1_score de 0.95. De plus, contrairement aux prédictions précédentes, ce score est assez consistent.\n",
    "\n",
    "En effet, avant le sur-échantillonage des données de surverses, le f1_score et la qualité du modèle variait énormément. Cela provenait probablement du fait que nous prenions 80% des données pour entrainement sur un jeu de données ne contenant que 3 à 20% de surverses. La majorité des jeux initiaux ne contenant environ que 7% des surverses, beaucoups d'échantillons ne contenaient que peu ou pas de surverses et ne pouvaient donc pas faire de prédictions notables. \n",
    "\n",
    "Ajouter des données de sorte à avoir une plus grande représentation de surverses permet entre autres d'améliorer les prédictions, mais aussi de rendre le processus plus consistent, peu importe l'échantillon tiré.\n",
    "\n",
    "Avant cet ajout, la prédiction qui nous donnait un f1_score de 0.65 sur Kaggle donnait des f1_score allant de 0.25 à 0.65 en local. Pendant une grande partie du projet, l'ensemble de nos prédictions locales étaient en dessous d'environ 10% par rapport aux résultats sur Kaggle.\n",
    "\n",
    "L'ajout de sur-échantillonnage a inversé ce phénomène, avec des résultats extrémement stables en local, et tournant autour de 90 à 97% pour l'ensemble des ouvrages d'intérêts. En revanche, les résultats étant plus bas sur Kaggle d'environ 10 à 15%, nous pouvons imaginer que nos modèles sont trop ajustés aux données d'entrainements et donc, que les données à prédire pour 2019 suivent une tendance différente. Une proposition d'ajustement est abordé à la section [5.3. Améliorations possibles](#5.3.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la forêt aléatoire, le score est encore plus intéressant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967391304347826"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_params_3260 = [5, 100, 80, 25];\n",
    "get_rf_direct(train_set_3260, val_set_3260, names_ft, estimated_params_3260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ces paramètres estimés, nous obtenons généralement un score plus élevé de 1 à 2%, encore plus consistens dans les prédictions. Idéalement, nous aurions utilisés la fonction _find_best_rf_ afin de trouver les meilleurs paramètres mais des contraintes de temps et d'équipements nous ont empeché de procéder de cette manière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors maintenant passer à la prédiction. Commençons par importer les données de tests, aussi traitées pour être consistentes avec les données d'entrainements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_3260 = CSV.read(\"data/parsed/test_3260.csv\");\n",
    "size(test_set_3260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, il n'y a que 45 dates à estimer pour cet ouvrage, entrainons notre modèle et passons à la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_3260 = get_rf_probas(data_set_3260, test_set_3260, names_ft, estimated_params_3260);\n",
    "test_pred_3260 = test_pred_3260[:, 2]\n",
    "test_pred_3260[test_pred_3260 .>= 0.5] .= 1.0;\n",
    "test_pred_3260[test_pred_3260 .< 0.5] .= 0.0;\n",
    "test_pred_3260 = convert(Array{Int}, trunc.(test_pred_3260));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons ces prédictions aux données de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>DATE</th><th>SURVERSE</th><th>FS_sum</th><th>FS_max</th><th>SS_sum</th><th>SS_max</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10 rows × 6 columns</p><tr><th>1</th><td>2019-05-02</td><td>0</td><td>-0.0635098</td><td>-0.0332178</td><td>-0.0891232</td><td>-0.016819</td></tr><tr><th>2</th><td>2019-05-09</td><td>1</td><td>0.742849</td><td>1.01886</td><td>0.50862</td><td>0.620324</td></tr><tr><th>3</th><td>2019-05-10</td><td>1</td><td>4.53146</td><td>1.68837</td><td>3.48538</td><td>1.14162</td></tr><tr><th>4</th><td>2019-05-15</td><td>0</td><td>-0.370694</td><td>-0.35203</td><td>-0.292356</td><td>-0.306429</td></tr><tr><th>5</th><td>2019-05-20</td><td>0</td><td>0.192477</td><td>0.859456</td><td>0.305387</td><td>0.79409</td></tr><tr><th>6</th><td>2019-05-23</td><td>1</td><td>1.93319</td><td>1.59272</td><td>2.25403</td><td>2.56071</td></tr><tr><th>7</th><td>2019-05-24</td><td>0</td><td>-0.229901</td><td>-0.128861</td><td>-0.124988</td><td>0.0411031</td></tr><tr><th>8</th><td>2019-05-26</td><td>0</td><td>-0.357895</td><td>-0.320149</td><td>-0.208672</td><td>-0.161624</td></tr><tr><th>9</th><td>2019-05-30</td><td>0</td><td>-0.306698</td><td>-0.192624</td><td>-0.172807</td><td>-0.016819</td></tr><tr><th>10</th><td>2019-06-02</td><td>0</td><td>0.794047</td><td>0.381238</td><td>0.448846</td><td>0.127986</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& DATE & SURVERSE & FS\\_sum & FS\\_max & SS\\_sum & SS\\_max\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 2019-05-02 & 0 & -0.0635098 & -0.0332178 & -0.0891232 & -0.016819 \\\\\n",
       "\t2 & 2019-05-09 & 1 & 0.742849 & 1.01886 & 0.50862 & 0.620324 \\\\\n",
       "\t3 & 2019-05-10 & 1 & 4.53146 & 1.68837 & 3.48538 & 1.14162 \\\\\n",
       "\t4 & 2019-05-15 & 0 & -0.370694 & -0.35203 & -0.292356 & -0.306429 \\\\\n",
       "\t5 & 2019-05-20 & 0 & 0.192477 & 0.859456 & 0.305387 & 0.79409 \\\\\n",
       "\t6 & 2019-05-23 & 1 & 1.93319 & 1.59272 & 2.25403 & 2.56071 \\\\\n",
       "\t7 & 2019-05-24 & 0 & -0.229901 & -0.128861 & -0.124988 & 0.0411031 \\\\\n",
       "\t8 & 2019-05-26 & 0 & -0.357895 & -0.320149 & -0.208672 & -0.161624 \\\\\n",
       "\t9 & 2019-05-30 & 0 & -0.306698 & -0.192624 & -0.172807 & -0.016819 \\\\\n",
       "\t10 & 2019-06-02 & 0 & 0.794047 & 0.381238 & 0.448846 & 0.127986 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×6 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ DATE       │ SURVERSE │ FS_sum     │ FS_max     │ SS_sum     │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────┼──────────┼────────────┼────────────┼────────────┤\n",
       "│ 1   │ 2019-05-02 │ 0        │ -0.0635098 │ -0.0332178 │ -0.0891232 │\n",
       "│ 2   │ 2019-05-09 │ 1        │ 0.742849   │ 1.01886    │ 0.50862    │\n",
       "│ 3   │ 2019-05-10 │ 1        │ 4.53146    │ 1.68837    │ 3.48538    │\n",
       "│ 4   │ 2019-05-15 │ 0        │ -0.370694  │ -0.35203   │ -0.292356  │\n",
       "│ 5   │ 2019-05-20 │ 0        │ 0.192477   │ 0.859456   │ 0.305387   │\n",
       "│ 6   │ 2019-05-23 │ 1        │ 1.93319    │ 1.59272    │ 2.25403    │\n",
       "│ 7   │ 2019-05-24 │ 0        │ -0.229901  │ -0.128861  │ -0.124988  │\n",
       "│ 8   │ 2019-05-26 │ 0        │ -0.357895  │ -0.320149  │ -0.208672  │\n",
       "│ 9   │ 2019-05-30 │ 0        │ -0.306698  │ -0.192624  │ -0.172807  │\n",
       "│ 10  │ 2019-06-02 │ 0        │ 0.794047   │ 0.381238   │ 0.448846   │"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_3260 = test_set_3260;\n",
    "pred_3260[!, :SURVERSE] = test_pred_3260;\n",
    "vis_ft = [:DATE, :SURVERSE, :FS_sum, :FS_max, :SS_sum, :SS_max];\n",
    "first(pred_3260[!, vis_ft], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici donc les prédictions pour notre modèle de forêt aléatoire. Les résultats observés nous ont l'air adéquats selon les données de précipitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4.\"></a>\n",
    "### 4.4. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4.1.\"></a>\n",
    "#### 4.4.1. Théorie sur la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de régression logistique sert à modéliser une variable d'intérêt de type Bernoulli *Y* à l'aide d'une fonction logistique. Dans un tel cas, la variable *Y* représente la probabilité de succès d'un évènement, c'est-à-dire qu'elle prend une valeur entre 0 et 1. Le lien entre la variable *Y* et les *p* variables explicatives ne peut alors pas être exprimé selon une relation linéaire. Cependant, on peut considérer une relation linéaire entre la fonction logit et les *p* variables explicatives. Dans la régression logistique, les coefficients de régression permettent de représenter l'effet de chaque variable expliative sur la variation de la cote (rapport entre la probabilité de succès et la probabilité d'échec) et ceux-ci peuvent être estimé à l'aide de la méthode du maximum de la vraisemblance.\n",
    "\n",
    "Tout comme les autres modèles statistiques, la régression logistique possède des points forts et faibles:\n",
    "\n",
    "Les avantages de la régression logistique sont:\n",
    "- La facilité de l'implémentation\n",
    "- Donne souvent des résultats acceptables\n",
    "- Rapide à entrainer\n",
    "\n",
    "Les désavantages de la régression logistique sont:\n",
    "- On doit sélectionner les variables explicatives correctement\n",
    "- Influencé par la multicolinéarité\n",
    "- Influencé par des échelles différentes entre les données\n",
    "\n",
    "D'abord, cette méthode est très facile à implémenter, car il n'y a pas beaucoup d'étapes à exécuter et ce n'est pas exigeant en terme de puissance computationnelle. Aussi, le traitement des variables explicatives avec des manipulations mathématiques complexes ne sont pas nécessaire pour construire ce modèle, tout ce qu'il faut est une standardization afin de rendre le poids de chaque variable explicative équitable. Des désavantages sont également présents. La régression logistique ne peut être performant que si les variables explicatives dont elle utilise sont bien sélectionnées. Des variables qui n'affectent pas du tout la variable de prédiction ou plusieurs varibles qui sont corrélées entre elles peuvent être des bruits pour le modèle et influencer négativement le résultat de la prédiction. De plus, de manière générale, la régression logistique n'est pas un algorithme de classification super performant. D'autres méthodes donnent souvent de meilleurs résultats de prédiction selon le type de problème à résoudre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4.2.\"></a>\n",
    "#### 4.4.2. Définition des fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions utilisées pour la régression logistique sont définis dans le fichier _random-forest.jl_, inclus dans ce rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"utils/reg-log.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous expliquerons ici les fonctions définies, sans montrer le code afin de ne pas polluer le rapport. Ce code est cependant présent dans le fichier, pour tout lecteur intéressé à l'implémentation spécifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find_best_threshold(val_pred, val_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va trouver le palier de décision qui va mener au meilleur f1_score pour les prédictions.\n",
    "\n",
    "En revanche, ceci est très spécialisé par rapport au jeu de donnée entrainé. Ainsi, un modèle entrainé sur un certain échantillon mènera à un palier qui n'est pas forcément applicable pour un autre modèle.\n",
    "\n",
    "Lorsque nous utilisions un jeu de données de 1000 dates par ouvrage, ce palier variait entre 0.15 et 0.7, avec des résultats très différents de f1_score si on prennait arbitrairement 0.5.\n",
    "\n",
    "Toutefois, après avoir ajouté le sur-échantillonnage de données, ce palier était très consistant et tournait autour de 0.5 la plupart du temps. De plus, prendre 0.5 comme palier n'entrainait souvant qu'une diminution minime du score. Ainsi, nous avons décidé de ne plus utiliser cette fonction après avoir implémenté le sur-échantillonnage afin de rester consistant et de ne pas se surajuster aux données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evaluate_threshold(val_pred, val_labels, threshold) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction permet de trouver le f1_score à un palier de données pour des prédictions sachant les vrais valeurs. Elle est utilisée pour évaluer notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4.3.\"></a>\n",
    "#### 4.4.3. Application à l'ensemble des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne détaillerons pas l'application à l'ensemble des données de la régression logistique afin de ne pas rendre ce rapport plus long que nécessaire. En revanche, il est à noter que les résultats sur l'ensemble des données étaient bien en dessous de ceux obtenus en isolant les données. Le f1_score à ce moment là tournait autour de 0.65 sur Kaggle, et 0.55 en local. Nous avons maintenant un f1_score de 0.85 local, très consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4.4.\"></a>\n",
    "#### 4.4.4. Application aux ouvrages isolés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons la régression logistique aux ouvrages isolés. Commençons par importer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using GLM;\n",
    "data_set_4380 = CSV.read(\"data/parsed/oversampled/ouvrage_4380.csv\");\n",
    "train_set_4380, val_set_4380 = partitionTrainTest(data_set_4380);\n",
    "val_labels_4380 = val_set_4380[!, :SURVERSE];\n",
    "size(data_set_4380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous estimons que la répartition des surverses suivent linéairement les données, comme nous le suggère les graphiques de la section [2. Analyse exploratoire](#2.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_form = @formula(SURVERSE ~ FS_sum + FS_max + FS_max3 +\n",
    "                               SS_sum + SS_max + SS_max3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainons un modèle de régression logistique. La librairie la plus proche de ce que nous cherchons afin de ne pas avoir à implémenter ce modèle nous même est GLM. Nous utilisons donc un GLM basée sur une distribution Bernoulli en utilisant la fonction logit comme lien de transformation, comme vu en cours. D'autres options pourraient être considérées, comme nous allons en parler à la section [5.3. Améliorations possibles](#5.3.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9227683049147443"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model_glm_4380 = glm(val_form, train_set_4380, Bernoulli(), LogitLink())\n",
    "val_pred_glm_4380 = GLM.predict(val_model_glm_4380, val_set_4380);\n",
    "\n",
    "evaluate_threshold(val_pred_glm_4380, val_labels_4380, 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score obtenu est très satisfaisant. De plus, celui ci est assez consistant autour de 0.85 à 0.90 à travers les essais et les ouvrages.\n",
    "\n",
    "Entrainons alors le modèle final. On commence par obtenir les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_4380 = CSV.read(\"data/parsed/test_4380.csv\");\n",
    "size(test_set_4380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine et génère nos prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_glm_4380 = glm(val_form, data_set_4380, Bernoulli(), LogitLink());\n",
    "test_pred_glm_4380 = GLM.predict(test_model_glm_4380, test_set_4380);\n",
    "test_pred_4380 = test_pred_glm_4380;\n",
    "test_pred_4380[test_pred_4380 .>= 0.5] .= 1.0;\n",
    "test_pred_4380[test_pred_4380 .< 0.5] .= 0.0;\n",
    "test_pred_4380 = convert(Array{Int}, trunc.(test_pred_4380));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, on ajoute les prédictions aux données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>DATE</th><th>SURVERSE</th><th>FS_sum</th><th>FS_max3</th><th>SS_sum</th><th>SS_max3</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10 rows × 6 columns</p><tr><th>1</th><td>2019-05-02</td><td>0</td><td>-0.0635098</td><td>0.0299474</td><td>-0.035388</td><td>-0.0117939</td></tr><tr><th>2</th><td>2019-05-04</td><td>0</td><td>-0.370694</td><td>-0.376504</td><td>-0.116085</td><td>-0.121026</td></tr><tr><th>3</th><td>2019-05-05</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.116085</td><td>-0.121026</td></tr><tr><th>4</th><td>2019-05-06</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.116085</td><td>-0.121026</td></tr><tr><th>5</th><td>2019-05-08</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.116085</td><td>-0.121026</td></tr><tr><th>6</th><td>2019-05-09</td><td>1</td><td>0.742849</td><td>1.39156</td><td>0.201957</td><td>0.366876</td></tr><tr><th>7</th><td>2019-05-10</td><td>1</td><td>4.53146</td><td>2.73285</td><td>1.38394</td><td>0.796521</td></tr><tr><th>8</th><td>2019-05-17</td><td>0</td><td>-0.332296</td><td>-0.356181</td><td>-0.0923509</td><td>-0.084615</td></tr><tr><th>9</th><td>2019-05-18</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.116085</td><td>-0.121026</td></tr><tr><th>10</th><td>2019-05-28</td><td>0</td><td>-0.357895</td><td>-0.356181</td><td>-0.0686164</td><td>-0.0482044</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& DATE & SURVERSE & FS\\_sum & FS\\_max3 & SS\\_sum & SS\\_max3\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 2019-05-02 & 0 & -0.0635098 & 0.0299474 & -0.035388 & -0.0117939 \\\\\n",
       "\t2 & 2019-05-04 & 0 & -0.370694 & -0.376504 & -0.116085 & -0.121026 \\\\\n",
       "\t3 & 2019-05-05 & 0 & -0.396293 & -0.417149 & -0.116085 & -0.121026 \\\\\n",
       "\t4 & 2019-05-06 & 0 & -0.396293 & -0.417149 & -0.116085 & -0.121026 \\\\\n",
       "\t5 & 2019-05-08 & 0 & -0.396293 & -0.417149 & -0.116085 & -0.121026 \\\\\n",
       "\t6 & 2019-05-09 & 1 & 0.742849 & 1.39156 & 0.201957 & 0.366876 \\\\\n",
       "\t7 & 2019-05-10 & 1 & 4.53146 & 2.73285 & 1.38394 & 0.796521 \\\\\n",
       "\t8 & 2019-05-17 & 0 & -0.332296 & -0.356181 & -0.0923509 & -0.084615 \\\\\n",
       "\t9 & 2019-05-18 & 0 & -0.396293 & -0.417149 & -0.116085 & -0.121026 \\\\\n",
       "\t10 & 2019-05-28 & 0 & -0.357895 & -0.356181 & -0.0686164 & -0.0482044 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×6 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ DATE       │ SURVERSE │ FS_sum     │ FS_max3   │ SS_sum     │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────┼──────────┼────────────┼───────────┼────────────┤\n",
       "│ 1   │ 2019-05-02 │ 0        │ -0.0635098 │ 0.0299474 │ -0.035388  │\n",
       "│ 2   │ 2019-05-04 │ 0        │ -0.370694  │ -0.376504 │ -0.116085  │\n",
       "│ 3   │ 2019-05-05 │ 0        │ -0.396293  │ -0.417149 │ -0.116085  │\n",
       "│ 4   │ 2019-05-06 │ 0        │ -0.396293  │ -0.417149 │ -0.116085  │\n",
       "│ 5   │ 2019-05-08 │ 0        │ -0.396293  │ -0.417149 │ -0.116085  │\n",
       "│ 6   │ 2019-05-09 │ 1        │ 0.742849   │ 1.39156   │ 0.201957   │\n",
       "│ 7   │ 2019-05-10 │ 1        │ 4.53146    │ 2.73285   │ 1.38394    │\n",
       "│ 8   │ 2019-05-17 │ 0        │ -0.332296  │ -0.356181 │ -0.0923509 │\n",
       "│ 9   │ 2019-05-18 │ 0        │ -0.396293  │ -0.417149 │ -0.116085  │\n",
       "│ 10  │ 2019-05-28 │ 0        │ -0.357895  │ -0.356181 │ -0.0686164 │"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_4380 = test_set_4380;\n",
    "pred_4380[!, :SURVERSE] = test_pred_4380;\n",
    "vis_ft = [:DATE, :SURVERSE, :FS_sum, :FS_max3, :SS_sum, :SS_max3];\n",
    "first(pred_4380[!, vis_ft], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats semblent visuellement adéquats et donnent un f1_score local et en ligne assez satisfaisant. Beaucoups de modifications pourraient être apportées pour améliorer les résultats, comme expliqué à la section 5, mais nous allons commencer par améliorer ces résultats avec l'ensemble de modèles, dans une des prochaines sous-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.5.\"></a>\n",
    "### 4.5. Machine à vecteurs de support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons rapidement considéré puis implémenté la machine à vecteurs de support. Cependant, en raison du temps d'entrainement long et des résultats faibles, nous n'avons pas procédé avec ce modèle. Ce temps d'entrainement et ces résultats étaient probablement dûes à nos données qui n'étaient alors pas encore traitées. Il serait possible d'améliorer les résultats à présent mais en raison des contraintes de temps, nous avons décidé de nous concentrer sur d'autres pistes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.\"></a>\n",
    "### 4.6. Classification bayésienne naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette sous-section concernant la classification bayésienne naive ne suit plus le format de ce rapport. Elle a été rédigée une première fois avant le reste de ce rapport et est basée sur un traitement des données qui n'est plus d'actualité. Nous aurions aimé pouvoir mettre cette section à jour avec le reste des informations afin de pouvoir mieux la démontrer mais aussi afin d'obtenir de meilleurs résultats de prédiction, mais des contraintes de temps nous forcent à devoir abandonner la classification bayésienne naive. Nous gardons cependant cette partie dans le rapport, malgré qu'elle ne compile pas, car elle présente le travail effectué sur ce modèle très intéressant et les résultats qu'on a obtenu avec. Les fonctions définies en revanche, qui représente la plus grosse partie de cette sous-section, devraient rester valides même après les éventuels changements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.1.\"></a>\n",
    "#### 4.6.1. Définition des fonctions d'aide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions sont implémentées dans le fichier _naives-bayes.jl_, remis avec le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"utils/naive-bayes.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de comparer 2 vecteurs ensemble et qui retourne le pourcentage de similitude entre nos 2 vecteurs. Cela permet donc d'être certain du format que nous retournons avant de faire une soumission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compareArr(a, b, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de trouver la meilleure distribution qui s'ajuste le mieux pour un ensemble de données. Pour faire cela, nous testons plusieurs distributions différentes sur notre ensemble de données. Pour testé chacune de ces distributions, nous faisons appel à la librairie de Julia \"fit\" qui permet d'ajuster une distribution sur un ensemble de données. Par la suite, nous calculons la log vraisemblance de cette distribution. Nous ne conservons seulement que la distribution ayant la logvraisemblance la plus élevée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fitBestLikelihoodDistribution(data::Array, verbose::Bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de retrouver les meilleures distributions pour chacunes de nos variables explicatives sélectionnées. Pour chacune d'entre elles, nous devons trouver la meilleure distribution sachant la classe final. Dans notre cas, nous trouvons donc, pour chaque variable explicative, la meilleure distribution pour les exemplaires où il n'y a pas de surverse et une pour les exemplaires où il y a surverse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### getBestLikelihoodDistributions(train::DataFrame, variable::Symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une methode permettant d'ajuster notre modèle bayésien sur chacune nos variables à l'aide de l'ensemble d'entraînement. Nous obtenons ainsi pour chacune des variables la distribution sachant la classe la plus approprié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fitModel(train::DataFrame, variables::Array{Symbol} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous définissons une fonction permettant de récuperer une loi a priori en se basant sur un ensemble de données. Puisque notre prédiction est soit surverse ou non, notre loi a priori est donc une loi de Bernoulli où nous pouvons trouver la probabilité de succès en selon l'ensemble de données passé en paramètre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### getPrioris(trainSet::DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.2.\"></a>\n",
    "#### 4.6.2. Prédiction bayesien naïf\n",
    "\n",
    "Ici nous définissons la fonction permettant d'effectuer un prédiction sur les données fournies en paramètres à l'aide des distributions et de la loi a priori aussi fournies en paramètres. Si nous étions capable de trouver une loi a posteriori de façon analytique, nous effecturions la prédiction de la façon suivante:\n",
    "a posteriori = vraisemblance * a priori, ce qui donnerait la formule suivante:\n",
    "\n",
    "$ f_{\\theta | Y}(\\theta) = f_{Y | \\theta}(y) \\dot f_\\theta(\\theta) $\n",
    "\n",
    "Toutefois, puisque nous utilisons une approche numérique, la formule ressemble plutôt à celà: \n",
    "\n",
    "$ p(C|X) = p(C) \\prod_{i=1}^n p(X_i|C)$\n",
    "\n",
    "Les distributions sachant la classe sont données en paramètres et peuvent donc être utilisé ici directement. La loi a priori est aussi fournies en paramètre. Toutefois, si nous voulions utiliser une loi a priori non informative, il sufirait de remplacer la probabilité par 1 comme la ligne commentée ici le suggère. Cela nous donnerait donc une loi non informative Uniforme(0,1). Finalement, pour déterminer la classe de chaque exemplaire, nous prenons la classe ayant obtenu la plus grande probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predictNaiveBayes(data::DataFrame, likelihoodDistrs::Array, prioris::Array, variables::Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.3.\"></a>\n",
    "#### 4.6.3 Sélection du meilleur modèle\n",
    "\n",
    "Dans cette section, nous explorerons deux approches différentes pour sélectionner le meilleur modèle de classification bayésienne naïve, soient une approche basée sur le BIC, ainsi qu'une approche basée sur le F1-score obtenu sur un ensemble de validation.\n",
    "\n",
    "Sélectionner le meilleur modèle de classification bayésienne naïve revient à trouver les variables explicatives devant être incluses et excluses pour que le modèle soit le plus précis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.3.1.\"></a>\n",
    "##### 4.6.3.1. Sélection du meilleur modèle à l'aide du BIC\n",
    "\n",
    "Ici nous tentons de trouver le meilleur modèle à l'aide du BIC comme vu en classe. Nous essayons donc toutes les combinaisions possible de nos variables explicatives et calculons le BIC de notre modèle résultant. Cela devrait donc nous permettre d'éliminer les variables explicatives qui n'amène aucune information pertinente supplémentaire. Toutefois, étant donnée que notre ensemble de données est composé majoritairement de la classe négative, notre modèle le meilleur selon notre cette mesure est un modèle qui prédit uniquement la classe négative. Ainsi, nous avons décidé d'écarter cette approche pour la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIC(distributions::Dict, data::DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### findBestVariablesCombinationBIC(train::DataFrame, likelihoodDistrs::Dict, variables::Array{Symbol})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.3.2.\"></a>\n",
    "##### 4.6.3.2. Sélection du meilleur modèle à l'aide du F1-score sur l'ensemble de validation\n",
    "\n",
    "Puisque l'approche avec le critère du BIC n'était pas prometteuse, nous avons cherché une autre façon de trouver le meilleur modèle possible, soit le modèle prenant en compte seulement les variables explicatives les plus pertinentes pour notre prédiction. Nous avons donc décidez d'obter plutôt pour une approche utilisé dans le domaine de l'apprentissage machine. Cette approche consiste à séparer l'ensemble de données d'entraînement en 2 ensembles, soient un ensemble d'entraînement comprenant 80% des données et un ensemble de validation comprenant 20% des données. Cela permet donc d'entraîner notre modèle sur 80% des données et de vérifier sa performance sur le 20% restant que nous connaissons les réponses. Nous sommes ainsi capable de calculer le f1-score de notre modèle sur l'ensemble de validation. Nous pouvons donc encore une fois tester toutes les combinaisons possibles de nos variables explicative, mais cette fois, plutôt que de comparer le BIC de chacun des modèles résultant, nous pouvons comparer leur f1-score obtenu sur l'ensemble de validation. Ainsi, un modèle obtenant un f1-score plus élevé sur l'ensemble de validation devrait aussi obtenir un f1-score plus élevé sur l'ensemble de test pour notre soumission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### findBestVariablesCombinationF1Validation(train::DataFrame, validation::DataFrame, likelihoodDistrs::Dict, variables::Array{Symbol})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.\"></a>\n",
    "#### 4.6.4. Entraînement et prédiction du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.1.\"></a>\n",
    "##### 4.6.4.1. Définition des fonctions\n",
    "\n",
    "Ici, nous définissons les fonctions permettant d'entrainer notre modèle en utilisant toutes les variables explicatives ou encore en utilisant une de nos méthodes permettant de trouver la meilleure combinaison de variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction permet d'entraîner notre modèle sur un ensemble d'entraînement puis de faire une prédiction sur l'ensemble passé comme étant l'ensemble de test. Pour utiliser notre méthode permettant de trouver le meilleur modèle en utilisant le BIC, il suffit de décommenter la ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trainAndPredictNaiveBayes(train::DataFrame, test::DataFrame, variables::Array{Symbol} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 2 fonctions suivantes permettent d'entrainer notre modèle et de sélectionner le meilleur en utilisant la méthode décrite plus haut du f1-score. Aussi bien que nous avons vu en classe que nous devions trouver la loi a priori de façon indépendante de nos observations, nous n'avions aucune idée de la distribution des classes positives et négatives. Nous avons donc décidé d'essayer tout de même de prendre comme loi a priori la proportion de classes positives et négatives de notre ensemble d'entraînement. Cela a grandement augmenté le résultat de f1-score autant au niveau de l'ensemble de validation qu'au niveau de l'ensemble de test (la soumission) nous avons donc décidé de conserver cette façon d'obtenir la loi a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trainNaiveBayes(train::DataFrame, validation::DataFrame, variables::Array{Symbol} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trainOnValidationAndPredictNaiveBayes(train::DataFrame,validation::DataFrame, test::DataFrame, variables::Array{Symbol} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.2.\"></a>\n",
    "##### 4.6.4.2. Sélection des variables explicatives\n",
    "\n",
    "Ici, nous définissons les variables explicatives pouvant être utilisées pour notre modèle. En effet, il n'est pas possible d'utiliser des variables textuelles. De plus, après avoir effectué notre validation à l'aide de notre méthode qui utilise le f1-score, les variables explicatives qui forment le meilleur modèle sont les suivantes: :TP_LAT, :TP_LNG, :TP_Z, :PCP_MAX3, :S_4. Dans le but d'accélérer le temps de calcul nous remplaçons donc les variables par seulement ces variables, ce qui permet de ne pas recalculer toutes les combinaisons possibles. Toutefois, pour vérifier notre fonction, il suffit d'échanger les 2 lignes ici pour plutôt prendre toutes les variables explicatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ft = [:TP_LAT, :TP_LNG, :TP_Z, :PCP_MAX3, :S_4];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.3.\"></a>\n",
    "##### 4.6.4.3. Validation du modèle\n",
    "\n",
    "Nous pouvons enfin faire la validation de notre modèle. Nous testons d'abord avec tous les variables explicatives puis en sélectionnant seulement les variables les plus pertinentes à l'aide de la méthode discutées plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = trainAndPredictNaiveBayes(X_train, X_val, names_ft);\n",
    "# print(y)\n",
    "# Comparer nos résultats\n",
    "compareArr(y, X_val[:SURVERSE], true);\n",
    "r = roc(X_val[:SURVERSE], y);\n",
    "println(recall(r))\n",
    "println(precision(r))\n",
    "println(f1score(r))\n",
    "\n",
    "no_0_gt = X_val[:SURVERSE] .+ 1\n",
    "no_0_pred = y.+1\n",
    "confusmat(2, no_0_gt, no_0_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = trainOnValidationAndPredictNaiveBayes(X_train, X_val, X_val, names_ft);\n",
    "# print(y)\n",
    "# Comparer nos résultats\n",
    "compareArr(y, X_val[:SURVERSE], true);\n",
    "r = roc(X_val[:SURVERSE], y);\n",
    "println(recall(r))\n",
    "println(precision(r))\n",
    "println(f1score(r))\n",
    "\n",
    "no_0_gt = X_val[:SURVERSE] .+ 1\n",
    "no_0_pred = y.+1\n",
    "confusmat(2, no_0_gt, no_0_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.4.\"></a>\n",
    "##### 4.6.4.4. Prédiction finale\n",
    "\n",
    "Puisque la méthode ayant obtenu le meilleur f1-score sur l'ensemble de validation est celle utilisant seulement les meilleures variables explicative, c'est avec ce modèle que nous effectuons la prédication final sur l'ensemble de test. Sur notre ensemble de validation, nous arrivons a obtenir un f1-score de 0.4077 et sur l'ensemble de test lors de la soumission nous obtenons un score de 0.59. Ce score n'est pas mauvais, mais nous avons toutefois réussi à le battre avec d'autres modèles décrit plus loin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = trainOnValidationAndPredictNaiveBayes(X_train,X_val, X_test, names_ft);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6.4.5.\"></a>\n",
    "##### 4.6.4.5. Génération de la soumission\n",
    "\n",
    "Nous pouvons finalement générer notre soumission finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = X_test[:,:ID_OUVRAGE].*\"_\".*string.(X_test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=test_labels)\n",
    "CSV.write(\"submissions/naive-bayes.csv\",sampleSubmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.7.\"></a>\n",
    "### 4.7. Ensemble de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.7.1.\"></a>\n",
    "#### 4.7.1. Théorie sur l'ensemble des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble de modèles consiste à assembler deux ou plusieurs modèles ensembles afin de générer un nouveau modèle plus performant et robuste. En effet, même si indépendamment plusieurs modèles ne donnent que des résultats médiocres, en les aggrégant il est possible d'obtenir un modèle puissant.\n",
    "\n",
    "Ceci est dû à la théorie des grands nombres: Si nous avons une pièce qui est légèrement biaisé vers les _heads_ (51% _heads_ vs 49% _tails_) va avoir tendance à avoir une majorité de _heads_ après un grand nombre de lancers. Après 1000 jets, la probabilité d'avoir une majorité de _heads_ est de 75%. \n",
    "\n",
    "Ce principe s'applique aussi aux classificateurs. Si l'on crée un système composé de 1000 modèles qui ne sont corrects que 51% du temps, si l'on assemble les résultats, on pourrait alors espérer un résultat de 75% de précision. Ceci n'est totalement vrai que si les erreurs produites par les modèles ne sont pas corrélés, ce qui contraire à la réalité. En revanche, en utilisant des modèles de nature très différentes, on réduit le risque de corrélation au niveau des erreurs et il est alors possible d'obtenir une meilleure prédiction avec l'ensemble de modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.7.2.\"></a>\n",
    "#### 4.7.2. Application aux ouvrages isolées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons finalement l'ensemble de modèles aux ouvrages isolées. Les modèles que nous considèreront sont la régression logistique et la forêt aléatoire. Si le temps nous l'avait permis, nous aurions aussi ajouté la classification bayésienne naive et la machine à vecteurs de support. Après tout, ajouter plusieurs modèles différents profite à l'ensemble de modèles et ce, même si chacun de ces modèles n'est pas idéal en lui même.\n",
    "\n",
    "Commençons par importer un ouvrage d'intérêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_4350 = CSV.read(\"data/parsed/oversampled/ouvrage_4350.csv\");\n",
    "train_set_4350, val_set_4350 = partitionTrainTest(data_set_4350);\n",
    "val_labels_4350 = val_set_4350[!, :SURVERSE];\n",
    "size(data_set_4350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous entrainerons cette fois les deux modèles vus implémentés dans cette section. Commençons par la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9353876739562624"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model_glm_4350 = glm(val_form, train_set_4350, Bernoulli(), LogitLink())\n",
    "val_pred_glm_4350 = GLM.predict(val_model_glm_4350, val_set_4350);\n",
    "\n",
    "evaluate_threshold(val_pred_glm_4350, val_labels_4350, 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec un arbre de décision pour estimer une profondeur maximale adéquate de forêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decision Tree\n",
       "Leaves: 98\n",
       "Depth:  17"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_4350 = convert(Matrix{Float64}, train_set_4350[:, names_ft]);\n",
    "train_labels_4350 = convert(Array{Int64}, train_set_4350[!,:SURVERSE]);\n",
    "\n",
    "dt_model_4350 = build_tree(train_labels_4350, train_features_4350);\n",
    "dt_model_4350 = prune_tree(dt_model_4350, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement de la forêt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9896907216494846"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_params_4350 = [5, 100, 80, 13];\n",
    "get_rf_direct(train_set_4350, val_set_4350, names_ft, estimated_params_4350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est excellent, générons des prédictions de validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_rf_4350 = get_rf_probas(train_set_4350, val_set_4350, names_ft, estimated_params_4350);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En combinant ces deux scores, nous obtenons le score de l'ensemble de modèles. Nous sommes ici parti du principe que les deux modèles pèsent autant dans la prédiction du score. Il serait en revanche possible de faire peser un modèle plus qu'un autre si l'on pense qu'il apporte plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800292255236239"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred_4350 = (val_pred_glm_4350 + val_pred_rf_4350[:, 2]) ./ 2;\n",
    "evaluate_threshold(val_pred_4350, val_labels_4350, 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score se situe entre les deux, mais beaucoups plus proche du score de la forêt aléatoire. Ainsi, la forêt aléatoire vaut naturellement plus que la régression logistique dans notre cas. Ce score correspond au f1 local d'un des ouvrages. Afin d'avoir notre score f1 total, nous prenons la moyenne des 4 scores. En réalité, ce ne serait pas aussi simple que ça car les ouvrages ne sont pas représentés également dans le jeu de test et il faudrait alors les pondérer, mais cela reste une approximation utile pour déterminer comment notre modèle se comporte en comparaison avec des modèles précédents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe aussi une classe _VotingClassifier_ dans l'API de ScikitLearn, mais en raison de problèmes de configuration, nous n'avons pas pu la faire fonctionner dans notre projet. En revanche, la solution implémentée est très satisfaisante et nous nous contenterons de celle ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère ensuite les données de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 9)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_4350 = CSV.read(\"data/parsed/test_4350.csv\");\n",
    "size(test_set_4350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on entraine les modèles finaux et on génère notre prédiction pour le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>DATE</th><th>SURVERSE</th><th>FS_sum</th><th>FS_max3</th><th>SS_sum</th><th>SS_max3</th></tr><tr><th></th><th>Date</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10 rows × 6 columns</p><tr><th>1</th><td>2019-05-03</td><td>0</td><td>0.038885</td><td>0.13156</td><td>0.039479</td><td>0.0507577</td></tr><tr><th>2</th><td>2019-05-04</td><td>0</td><td>-0.370694</td><td>-0.376504</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>3</th><td>2019-05-07</td><td>0</td><td>-0.357895</td><td>-0.356181</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>4</th><td>2019-05-08</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>5</th><td>2019-05-10</td><td>1</td><td>4.53146</td><td>2.73285</td><td>0.588891</td><td>0.338935</td></tr><tr><th>6</th><td>2019-05-11</td><td>0</td><td>-0.370694</td><td>-0.376504</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>7</th><td>2019-05-12</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>8</th><td>2019-05-21</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>9</th><td>2019-05-22</td><td>0</td><td>-0.396293</td><td>-0.417149</td><td>-0.0493965</td><td>-0.0514987</td></tr><tr><th>10</th><td>2019-05-23</td><td>1</td><td>1.93319</td><td>2.48898</td><td>0.380842</td><td>0.524855</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& DATE & SURVERSE & FS\\_sum & FS\\_max3 & SS\\_sum & SS\\_max3\\\\\n",
       "\t\\hline\n",
       "\t& Date & Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 2019-05-03 & 0 & 0.038885 & 0.13156 & 0.039479 & 0.0507577 \\\\\n",
       "\t2 & 2019-05-04 & 0 & -0.370694 & -0.376504 & -0.0493965 & -0.0514987 \\\\\n",
       "\t3 & 2019-05-07 & 0 & -0.357895 & -0.356181 & -0.0493965 & -0.0514987 \\\\\n",
       "\t4 & 2019-05-08 & 0 & -0.396293 & -0.417149 & -0.0493965 & -0.0514987 \\\\\n",
       "\t5 & 2019-05-10 & 1 & 4.53146 & 2.73285 & 0.588891 & 0.338935 \\\\\n",
       "\t6 & 2019-05-11 & 0 & -0.370694 & -0.376504 & -0.0493965 & -0.0514987 \\\\\n",
       "\t7 & 2019-05-12 & 0 & -0.396293 & -0.417149 & -0.0493965 & -0.0514987 \\\\\n",
       "\t8 & 2019-05-21 & 0 & -0.396293 & -0.417149 & -0.0493965 & -0.0514987 \\\\\n",
       "\t9 & 2019-05-22 & 0 & -0.396293 & -0.417149 & -0.0493965 & -0.0514987 \\\\\n",
       "\t10 & 2019-05-23 & 1 & 1.93319 & 2.48898 & 0.380842 & 0.524855 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×6 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ DATE       │ SURVERSE │ FS_sum    │ FS_max3   │ SS_sum     │\n",
       "│     │ \u001b[90mDate\u001b[39m       │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────┼──────────┼───────────┼───────────┼────────────┤\n",
       "│ 1   │ 2019-05-03 │ 0        │ 0.038885  │ 0.13156   │ 0.039479   │\n",
       "│ 2   │ 2019-05-04 │ 0        │ -0.370694 │ -0.376504 │ -0.0493965 │\n",
       "│ 3   │ 2019-05-07 │ 0        │ -0.357895 │ -0.356181 │ -0.0493965 │\n",
       "│ 4   │ 2019-05-08 │ 0        │ -0.396293 │ -0.417149 │ -0.0493965 │\n",
       "│ 5   │ 2019-05-10 │ 1        │ 4.53146   │ 2.73285   │ 0.588891   │\n",
       "│ 6   │ 2019-05-11 │ 0        │ -0.370694 │ -0.376504 │ -0.0493965 │\n",
       "│ 7   │ 2019-05-12 │ 0        │ -0.396293 │ -0.417149 │ -0.0493965 │\n",
       "│ 8   │ 2019-05-21 │ 0        │ -0.396293 │ -0.417149 │ -0.0493965 │\n",
       "│ 9   │ 2019-05-22 │ 0        │ -0.396293 │ -0.417149 │ -0.0493965 │\n",
       "│ 10  │ 2019-05-23 │ 1        │ 1.93319   │ 2.48898   │ 0.380842   │"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_glm_4350 = glm(val_form, data_set_4350, Bernoulli(), LogitLink());\n",
    "test_pred_glm_4350 = GLM.predict(test_model_glm_4350, test_set_4350);\n",
    "test_pred_rf_4350 = get_rf_probas(data_set_4350, test_set_4350, names_ft, estimated_params_4350);\n",
    "test_pred_4350 = (test_pred_glm_4350 + test_pred_rf_4350[:, 2]) ./ 2;\n",
    "test_pred_4350[test_pred_4350 .>= 0.5] .= 1.0;\n",
    "test_pred_4350[test_pred_4350 .< 0.5] .= 0.0;\n",
    "test_pred_4350 = convert(Array{Int}, trunc.(test_pred_4350));\n",
    "\n",
    "pred_4350 = test_set_4350;\n",
    "pred_4350[!, :SURVERSE] = test_pred_4350;\n",
    "vis_ft = [:DATE, :SURVERSE, :FS_sum, :FS_max3, :SS_sum, :SS_max3];\n",
    "first(pred_4350[!, vis_ft], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci conclut l'implémentation de l'ensemble de modèles pour la prédiction de surverses. Ce processus est alors répété pour les autres ouvrages et le tout est aggrégé afin de produire notre jeu de prédiction final qui sera soumis sur Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.\"></a>\n",
    "## 5. Retour et conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2.\"></a>\n",
    "### 5.2. Difficultés rencontrées [TODO: UPDATE TABLE DES MATIÈRES]\n",
    "\n",
    "Lors de cet exercice, nous avons rencontré un certain nombre de difficultés qui nous en ralentit dans la réalisation du projet. Nous allons discuter ici de ces difficultés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.1.\"></a>\n",
    "#### 5.3.1. Interprétation des données\n",
    "\n",
    "Une des plus grande difficultés de ce projet fut de correctement interpréter les données. En effet, une erreur de notre part fut de confondre la signification d'émissaire et de trop-plein. Ne sachant pas initialement ce que ces termes représentaient, une recherche Google Image nous a mené à penser que le trop-plein était une sorte de conteneur dans lequel l'eau s'accumulait et l'émissaire comme étant le tuyeau de déversement vers le fleuve. \n",
    "<br>\n",
    "\n",
    "Cela nous a ainsi mené à effectuer des suppositions qui s'avéraient erronées et ont faussé nos résultats pendant une bonne partie de l'exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.\"></a>\n",
    "### 5.3. Améliorations possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.2.\"></a>\n",
    "#### 5.3.2. Données manquantes de précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.3.\"></a>\n",
    "#### 5.3.3. Remplacement des données abérantes de précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.4.\"></a>\n",
    "#### 5.3.4. Données des journées alentours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Explorer plus de procédés d'aggrégation pour les précipitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Moyenne ou mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Tendance de l'année pour les surverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Réintroduction des données de dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Modification des paramètres de sur-échantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Régler les problèmes d'_overfit_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parler du facteur année etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3.5.\"></a>\n",
    "#### 5.3.5. Explorer d'autres possibilités pour la fonction GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## 6. Références"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Référence style APA ou IEEE\n",
    "+ [1]: http://collections.banq.qc.ca/ark:/52327/bs44911\n",
    "\n",
    "+ [2]: http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse\n",
    "\n",
    "+ [3]: https://climat.meteo.gc.ca/climate_data/hourly_data_f.html?hlyRange=2008-01-08%7C2019-11-12&dlyRange=2002-12-23%7C2019-11-12&mlyRange=%7C&StationID=30165&Prov=QC&urlExtension=_f.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2019&selRowPerPage=25&Line=17&searchMethod=contains&Month=11&Day=12&txtStationName=montreal&timeframe=1&Year=2019\n",
    "\n",
    "+ [4]: https://datascience.stackexchange.com/questions/23666/how-many-features-to-sample-using-random-forests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
